---
title: Latest 15 Papers - April 09, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[How to evaluate control measures for LLM agents? A trajectory from today to superintelligence](http://arxiv.org/abs/2504.05259v1)** | 2025-04-07 |  |
| **[Safety Layers in Aligned Large Language Models: The Key to LLM Security](http://arxiv.org/abs/2408.17003v5)** | 2025-04-07 | <details><summary>Accep...</summary><p>Accepted by ICLR 2025. The code is available at https://github.com/listen0425/Safety-Layers</p></details> |
| **[Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs](http://arxiv.org/abs/2504.04715v1)** | 2025-04-07 |  |
| **[CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data](http://arxiv.org/abs/2503.09334v2)** | 2025-04-05 |  |
| **[AttackLLM: LLM-based Attack Pattern Generation for an Industrial Control System](http://arxiv.org/abs/2504.04187v1)** | 2025-04-05 |  |
| **[Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment](http://arxiv.org/abs/2410.14827v2)** | 2025-04-04 |  |
| **[Les Dissonances: Cross-Tool Harvesting and Polluting in Multi-Tool Empowered LLM Agents](http://arxiv.org/abs/2504.03111v1)** | 2025-04-04 |  |
| **[PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs](http://arxiv.org/abs/2409.14729v2)** | 2025-04-03 |  |
| **[Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation](http://arxiv.org/abs/2504.02458v1)** | 2025-04-03 |  |
| **[Learning to Lie: Reinforcement Learning Attacks Damage Human-AI Teams and Teams of LLMs](http://arxiv.org/abs/2503.21983v2)** | 2025-04-02 | <details><summary>17 pa...</summary><p>17 pages, 9 figures, accepted to ICLR 2025 Workshop on Human-AI Coevolution</p></details> |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[How to evaluate control measures for LLM agents? A trajectory from today to superintelligence](http://arxiv.org/abs/2504.05259v1)** | 2025-04-07 |  |
| **[Safety Layers in Aligned Large Language Models: The Key to LLM Security](http://arxiv.org/abs/2408.17003v5)** | 2025-04-07 | <details><summary>Accep...</summary><p>Accepted by ICLR 2025. The code is available at https://github.com/listen0425/Safety-Layers</p></details> |
| **[Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs](http://arxiv.org/abs/2504.04715v1)** | 2025-04-07 |  |
| **[CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data](http://arxiv.org/abs/2503.09334v2)** | 2025-04-05 |  |
| **[AttackLLM: LLM-based Attack Pattern Generation for an Industrial Control System](http://arxiv.org/abs/2504.04187v1)** | 2025-04-05 |  |
| **[Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment](http://arxiv.org/abs/2410.14827v2)** | 2025-04-04 |  |
| **[Les Dissonances: Cross-Tool Harvesting and Polluting in Multi-Tool Empowered LLM Agents](http://arxiv.org/abs/2504.03111v1)** | 2025-04-04 |  |
| **[PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs](http://arxiv.org/abs/2409.14729v2)** | 2025-04-03 |  |
| **[Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation](http://arxiv.org/abs/2504.02458v1)** | 2025-04-03 |  |
| **[Learning to Lie: Reinforcement Learning Attacks Damage Human-AI Teams and Teams of LLMs](http://arxiv.org/abs/2503.21983v2)** | 2025-04-02 | <details><summary>17 pa...</summary><p>17 pages, 9 figures, accepted to ICLR 2025 Workshop on Human-AI Coevolution</p></details> |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[DiffPatch: Generating Customizable Adversarial Patches using Diffusion Models](http://arxiv.org/abs/2412.01440v3)** | 2025-04-07 |  |
| **[Adversarial Robustness for Deep Learning-based Wildfire Prediction Models](http://arxiv.org/abs/2412.20006v3)** | 2025-04-07 |  |
| **[Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](http://arxiv.org/abs/2504.05050v1)** | 2025-04-07 |  |
| **[A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language Models](http://arxiv.org/abs/2504.04976v1)** | 2025-04-07 | 21 pages, 5 figures |
| **[SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models](http://arxiv.org/abs/2504.04893v1)** | 2025-04-07 | <details><summary>Submi...</summary><p>Submitted to CVPR 2025 Workshop EVAL-FoMo-2</p></details> |
| **[Latent Feature and Attention Dual Erasure Attack against Multi-View Diffusion Models for 3D Assets Protection](http://arxiv.org/abs/2408.11408v2)** | 2025-04-07 | <details><summary>This ...</summary><p>This paper has been accepted by ICME 2025</p></details> |
| **[PiCo: Jailbreaking Multimodal Large Language Models via $\textbf{Pi}$ctorial $\textbf{Co}$de Contextualization](http://arxiv.org/abs/2504.01444v2)** | 2025-04-07 |  |
| **[Safety Layers in Aligned Large Language Models: The Key to LLM Security](http://arxiv.org/abs/2408.17003v5)** | 2025-04-07 | <details><summary>Accep...</summary><p>Accepted by ICLR 2025. The code is available at https://github.com/listen0425/Safety-Layers</p></details> |
| **[Two is Better than One: Efficient Ensemble Defense for Robust and Compact Models](http://arxiv.org/abs/2504.04747v1)** | 2025-04-07 | Accepted to CVPR2025 |
| **[On the Robustness of GUI Grounding Models Against Image Attacks](http://arxiv.org/abs/2504.04716v1)** | 2025-04-07 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[DiffPatch: Generating Customizable Adversarial Patches using Diffusion Models](http://arxiv.org/abs/2412.01440v3)** | 2025-04-07 |  |
| **[Adversarial Robustness for Deep Learning-based Wildfire Prediction Models](http://arxiv.org/abs/2412.20006v3)** | 2025-04-07 |  |
| **[Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](http://arxiv.org/abs/2504.05050v1)** | 2025-04-07 |  |
| **[A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language Models](http://arxiv.org/abs/2504.04976v1)** | 2025-04-07 | 21 pages, 5 figures |
| **[SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models](http://arxiv.org/abs/2504.04893v1)** | 2025-04-07 | <details><summary>Submi...</summary><p>Submitted to CVPR 2025 Workshop EVAL-FoMo-2</p></details> |
| **[Prompting the Unseen: Detecting Hidden Backdoors in Black-Box Models](http://arxiv.org/abs/2411.09540v2)** | 2025-04-07 | <details><summary>This ...</summary><p>This paper has been accepted by IEEE/IFIP DSN 2025</p></details> |
| **[Latent Feature and Attention Dual Erasure Attack against Multi-View Diffusion Models for 3D Assets Protection](http://arxiv.org/abs/2408.11408v2)** | 2025-04-07 | <details><summary>This ...</summary><p>This paper has been accepted by ICME 2025</p></details> |
| **[PiCo: Jailbreaking Multimodal Large Language Models via $\textbf{Pi}$ctorial $\textbf{Co}$de Contextualization](http://arxiv.org/abs/2504.01444v2)** | 2025-04-07 |  |
| **[Safety Layers in Aligned Large Language Models: The Key to LLM Security](http://arxiv.org/abs/2408.17003v5)** | 2025-04-07 | <details><summary>Accep...</summary><p>Accepted by ICLR 2025. The code is available at https://github.com/listen0425/Safety-Layers</p></details> |
| **[Two is Better than One: Efficient Ensemble Defense for Robust and Compact Models](http://arxiv.org/abs/2504.04747v1)** | 2025-04-07 | Accepted to CVPR2025 |

