---
title: Latest 15 Papers - March 14, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data](http://arxiv.org/abs/2503.09334v1)** | 2025-03-12 | <details><summary>The p...</summary><p>The paper is submitted to "The 48th International ACM SIGIR Conference on Research and Development in Information Retrieval" and is currently under review</p></details> |
| **[DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios](http://arxiv.org/abs/2410.23746v3)** | 2025-03-12 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Camera-Ready)</p></details> |
| **[Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States](http://arxiv.org/abs/2503.09066v1)** | 2025-03-12 | 4 figures |
| **[JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing](http://arxiv.org/abs/2503.08990v1)** | 2025-03-12 |  |
| **[Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities](http://arxiv.org/abs/2410.18469v3)** | 2025-03-11 | <details><summary>Accep...</summary><p>Accepted to NAACL 2025 Main (oral)</p></details> |
| **[Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context](http://arxiv.org/abs/2412.16359v2)** | 2025-03-11 | <details><summary>arXiv...</summary><p>arXiv admin note: text overlap with arXiv:2407.14644</p></details> |
| **[Dialogue Injection Attack: Jailbreaking LLMs through Context Manipulation](http://arxiv.org/abs/2503.08195v1)** | 2025-03-11 | 17 pages, 10 figures |
| **[MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents](http://arxiv.org/abs/2412.08014v2)** | 2025-03-11 |  |
| **[Safety Guardrails for LLM-Enabled Robots](http://arxiv.org/abs/2503.07885v1)** | 2025-03-10 |  |
| **[Stepwise Reasoning Error Disruption Attack of LLMs](http://arxiv.org/abs/2412.11934v3)** | 2025-03-10 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data](http://arxiv.org/abs/2503.09334v1)** | 2025-03-12 | <details><summary>The p...</summary><p>The paper is submitted to "The 48th International ACM SIGIR Conference on Research and Development in Information Retrieval" and is currently under review</p></details> |
| **[DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios](http://arxiv.org/abs/2410.23746v3)** | 2025-03-12 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Camera-Ready)</p></details> |
| **[Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States](http://arxiv.org/abs/2503.09066v1)** | 2025-03-12 | 4 figures |
| **[JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing](http://arxiv.org/abs/2503.08990v1)** | 2025-03-12 |  |
| **[Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities](http://arxiv.org/abs/2410.18469v3)** | 2025-03-11 | <details><summary>Accep...</summary><p>Accepted to NAACL 2025 Main (oral)</p></details> |
| **[Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context](http://arxiv.org/abs/2412.16359v2)** | 2025-03-11 | <details><summary>arXiv...</summary><p>arXiv admin note: text overlap with arXiv:2407.14644</p></details> |
| **[Dialogue Injection Attack: Jailbreaking LLMs through Context Manipulation](http://arxiv.org/abs/2503.08195v1)** | 2025-03-11 | 17 pages, 10 figures |
| **[MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents](http://arxiv.org/abs/2412.08014v2)** | 2025-03-11 |  |
| **[Safety Guardrails for LLM-Enabled Robots](http://arxiv.org/abs/2503.07885v1)** | 2025-03-10 |  |
| **[Stepwise Reasoning Error Disruption Attack of LLMs](http://arxiv.org/abs/2412.11934v3)** | 2025-03-10 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Independence Tests for Language Models](http://arxiv.org/abs/2502.12292v2)** | 2025-03-12 |  |
| **[Detecting and Preventing Data Poisoning Attacks on AI Models](http://arxiv.org/abs/2503.09302v1)** | 2025-03-12 | 9 pages, 8 figures |
| **[Prompt Inference Attack on Distributed Large Language Model Inference Frameworks](http://arxiv.org/abs/2503.09291v1)** | 2025-03-12 |  |
| **[Prompt Inversion Attack against Collaborative Inference of Large Language Models](http://arxiv.org/abs/2503.09022v1)** | 2025-03-12 | <details><summary>To ap...</summary><p>To appear at IEEE Symposium on Security and Privacy 2025</p></details> |
| **[Proactive Privacy Amnesia for Large Language Models: Safeguarding PII with Negligible Impact on Model Utility](http://arxiv.org/abs/2502.17591v2)** | 2025-03-11 | <details><summary>ICLR'...</summary><p>ICLR'25 Poster. Project page and code is available at https://ppa-iclr2025.my.canva.site/</p></details> |
| **[MIGA: Mutual Information-Guided Attack on Denoising Models for Semantic Manipulation](http://arxiv.org/abs/2503.06966v2)** | 2025-03-11 |  |
| **[Stochastic Tube-based Model Predictive Control for Cyber-Physical Systems under False Data Injection Attacks with Bounded Probability](http://arxiv.org/abs/2503.07385v2)** | 2025-03-11 | <details><summary>This ...</summary><p>This work has been submitted to the IEEE for possible publication</p></details> |
| **[Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models](http://arxiv.org/abs/2502.11054v4)** | 2025-03-11 |  |
| **[Identifying the Truth of Global Model: A Generic Solution to Defend Against Byzantine and Backdoor Attacks in Federated Learning (full version)](http://arxiv.org/abs/2311.10248v2)** | 2025-03-10 | <details><summary>Accep...</summary><p>Accepted to ACISP 2025. This is the full version</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Independence Tests for Language Models](http://arxiv.org/abs/2502.12292v2)** | 2025-03-12 |  |
| **[Detecting and Preventing Data Poisoning Attacks on AI Models](http://arxiv.org/abs/2503.09302v1)** | 2025-03-12 | 9 pages, 8 figures |
| **[Prompt Inference Attack on Distributed Large Language Model Inference Frameworks](http://arxiv.org/abs/2503.09291v1)** | 2025-03-12 |  |
| **[Prompt Inversion Attack against Collaborative Inference of Large Language Models](http://arxiv.org/abs/2503.09022v1)** | 2025-03-12 | <details><summary>To ap...</summary><p>To appear at IEEE Symposium on Security and Privacy 2025</p></details> |
| **[Proactive Privacy Amnesia for Large Language Models: Safeguarding PII with Negligible Impact on Model Utility](http://arxiv.org/abs/2502.17591v2)** | 2025-03-11 | <details><summary>ICLR'...</summary><p>ICLR'25 Poster. Project page and code is available at https://ppa-iclr2025.my.canva.site/</p></details> |
| **[MIGA: Mutual Information-Guided Attack on Denoising Models for Semantic Manipulation](http://arxiv.org/abs/2503.06966v2)** | 2025-03-11 |  |
| **[Stochastic Tube-based Model Predictive Control for Cyber-Physical Systems under False Data Injection Attacks with Bounded Probability](http://arxiv.org/abs/2503.07385v2)** | 2025-03-11 | <details><summary>This ...</summary><p>This work has been submitted to the IEEE for possible publication</p></details> |
| **[Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models](http://arxiv.org/abs/2502.11054v4)** | 2025-03-11 |  |
| **[Identifying the Truth of Global Model: A Generic Solution to Defend Against Byzantine and Backdoor Attacks in Federated Learning (full version)](http://arxiv.org/abs/2311.10248v2)** | 2025-03-10 | <details><summary>Accep...</summary><p>Accepted to ACISP 2025. This is the full version</p></details> |

