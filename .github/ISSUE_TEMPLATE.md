---
title: Latest 15 Papers - December 15, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection](https://arxiv.org/abs/2512.10449v1)** | 2025-12-11 |  |
| **[How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation](https://arxiv.org/abs/2512.10415v1)** | 2025-12-11 | Under Review |
| **[Attention is All You Need to Defend Against Indirect Prompt Injection Attacks in LLMs](https://arxiv.org/abs/2512.08417v2)** | 2025-12-11 | <details><summary>Accep...</summary><p>Accepted by Network and Distributed System Security (NDSS) Symposium 2026</p></details> |
| **[Verifying LLM Inference to Detect Model Weight Exfiltration](https://arxiv.org/abs/2511.02620v2)** | 2025-12-10 |  |
| **[LLM-PEA: Leveraging Large Language Models Against Phishing Email Attacks](https://arxiv.org/abs/2512.10104v1)** | 2025-12-10 | 7 pages |
| **[FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning](https://arxiv.org/abs/2512.09872v1)** | 2025-12-10 | <details><summary>Accep...</summary><p>Accepted in IEEE HOST 2026</p></details> |
| **[Advancing LLM-Based Security Automation with Customized Group Relative Policy Optimization for Zero-Touch Networks](https://arxiv.org/abs/2512.09485v1)** | 2025-12-10 | <details><summary>Accep...</summary><p>Accepted by IEEE JSAC. This work has been submitted to the IEEE for possible publication</p></details> |
| **[Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs](https://arxiv.org/abs/2512.09403v1)** | 2025-12-10 |  |
| **[ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data](https://arxiv.org/abs/2512.09321v1)** | 2025-12-10 | <details><summary>To ap...</summary><p>To appear in NDSS 2026</p></details> |
| **[Memory Injection Attacks on LLM Agents via Query-Only Interaction](https://arxiv.org/abs/2503.03704v4)** | 2025-12-10 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection](https://arxiv.org/abs/2512.10449v1)** | 2025-12-11 |  |
| **[How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation](https://arxiv.org/abs/2512.10415v1)** | 2025-12-11 | Under Review |
| **[Attention is All You Need to Defend Against Indirect Prompt Injection Attacks in LLMs](https://arxiv.org/abs/2512.08417v2)** | 2025-12-11 | <details><summary>Accep...</summary><p>Accepted by Network and Distributed System Security (NDSS) Symposium 2026</p></details> |
| **[Verifying LLM Inference to Detect Model Weight Exfiltration](https://arxiv.org/abs/2511.02620v2)** | 2025-12-10 |  |
| **[LLM-PEA: Leveraging Large Language Models Against Phishing Email Attacks](https://arxiv.org/abs/2512.10104v1)** | 2025-12-10 | 7 pages |
| **[FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning](https://arxiv.org/abs/2512.09872v1)** | 2025-12-10 | <details><summary>Accep...</summary><p>Accepted in IEEE HOST 2026</p></details> |
| **[Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs](https://arxiv.org/abs/2512.09742v1)** | 2025-12-10 | 70 pages, 47 figures |
| **[Advancing LLM-Based Security Automation with Customized Group Relative Policy Optimization for Zero-Touch Networks](https://arxiv.org/abs/2512.09485v1)** | 2025-12-10 | <details><summary>Accep...</summary><p>Accepted by IEEE JSAC. This work has been submitted to the IEEE for possible publication</p></details> |
| **[Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs](https://arxiv.org/abs/2512.09403v1)** | 2025-12-10 |  |
| **[ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data](https://arxiv.org/abs/2512.09321v1)** | 2025-12-10 | <details><summary>To ap...</summary><p>To appear in NDSS 2026</p></details> |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Better Language Model Inversion by Compactly Representing Next-Token Distributions](https://arxiv.org/abs/2506.17090v3)** | 2025-12-11 |  |
| **[Adaptive Intrusion Detection System Leveraging Dynamic Neural Models with Adversarial Learning for 5G/6G Networks](https://arxiv.org/abs/2512.10637v1)** | 2025-12-11 | <details><summary>6 pag...</summary><p>6 pages,2 figures, 1 Table</p></details> |
| **[Toward Quantitative Modeling of Cybersecurity Risks Due to AI Misuse](https://arxiv.org/abs/2512.08864v2)** | 2025-12-11 |  |
| **[Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense](https://arxiv.org/abs/2412.21051v4)** | 2025-12-11 | <details><summary>7 pag...</summary><p>7 pages; Accepted by IEEE Communications Magazine</p></details> |
| **[When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.16203v3)** | 2025-12-11 |  |
| **[Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization](https://arxiv.org/abs/2411.03752v3)** | 2025-12-11 |  |
| **[Watermarks for Language Models via Probabilistic Automata](https://arxiv.org/abs/2512.10185v1)** | 2025-12-11 |  |
| **[Unveiling the Latent Directions of Reflection in Large Language Models](https://arxiv.org/abs/2508.16989v2)** | 2025-12-11 |  |
| **[Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning](https://arxiv.org/abs/2512.10150v1)** | 2025-12-10 |  |
| **[Verifying LLM Inference to Detect Model Weight Exfiltration](https://arxiv.org/abs/2511.02620v2)** | 2025-12-10 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Better Language Model Inversion by Compactly Representing Next-Token Distributions](https://arxiv.org/abs/2506.17090v3)** | 2025-12-11 |  |
| **[Adaptive Intrusion Detection System Leveraging Dynamic Neural Models with Adversarial Learning for 5G/6G Networks](https://arxiv.org/abs/2512.10637v1)** | 2025-12-11 | <details><summary>6 pag...</summary><p>6 pages,2 figures, 1 Table</p></details> |
| **[Toward Quantitative Modeling of Cybersecurity Risks Due to AI Misuse](https://arxiv.org/abs/2512.08864v2)** | 2025-12-11 |  |
| **[Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense](https://arxiv.org/abs/2412.21051v4)** | 2025-12-11 | <details><summary>7 pag...</summary><p>7 pages; Accepted by IEEE Communications Magazine</p></details> |
| **[When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.16203v3)** | 2025-12-11 |  |
| **[Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization](https://arxiv.org/abs/2411.03752v3)** | 2025-12-11 |  |
| **[Watermarks for Language Models via Probabilistic Automata](https://arxiv.org/abs/2512.10185v1)** | 2025-12-11 |  |
| **[Unveiling the Latent Directions of Reflection in Large Language Models](https://arxiv.org/abs/2508.16989v2)** | 2025-12-11 |  |
| **[Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning](https://arxiv.org/abs/2512.10150v1)** | 2025-12-10 |  |
| **[Verifying LLM Inference to Detect Model Weight Exfiltration](https://arxiv.org/abs/2511.02620v2)** | 2025-12-10 |  |

