---
title: Latest 15 Papers - October 13, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Watch your steps: Dormant Adversarial Behaviors that Activate upon LLM Finetuning](http://arxiv.org/abs/2505.16567v3)** | 2025-10-09 |  |
| **[Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment](http://arxiv.org/abs/2509.22745v2)** | 2025-10-09 | Under review |
| **[Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](http://arxiv.org/abs/2507.11112v2)** | 2025-10-09 |  |
| **[Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations](http://arxiv.org/abs/2510.08120v1)** | 2025-10-09 | <details><summary>12 pa...</summary><p>12 pages, 2 figures, 3 tables</p></details> |
| **[(Token-Level) InfoRMIA: Stronger Membership Inference and Memorization Assessment for LLMs](http://arxiv.org/abs/2510.05582v2)** | 2025-10-09 |  |
| **[Fewer Weights, More Problems: A Practical Attack on LLM Pruning](http://arxiv.org/abs/2510.07985v1)** | 2025-10-09 |  |
| **[Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](http://arxiv.org/abs/2505.13527v2)** | 2025-10-09 |  |
| **[Rethinking Reasoning: A Survey on Reasoning-based Backdoors in LLMs](http://arxiv.org/abs/2510.07697v1)** | 2025-10-09 |  |
| **[LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics](http://arxiv.org/abs/2510.07626v1)** | 2025-10-08 |  |
| **[$\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks](http://arxiv.org/abs/2504.00218v2)** | 2025-10-08 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[The Shape of Adversarial Influence: Characterizing LLM Latent Spaces with Persistent Homology](http://arxiv.org/abs/2505.20435v2)** | 2025-10-09 |  |
| **[Watch your steps: Dormant Adversarial Behaviors that Activate upon LLM Finetuning](http://arxiv.org/abs/2505.16567v3)** | 2025-10-09 |  |
| **[Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment](http://arxiv.org/abs/2509.22745v2)** | 2025-10-09 | Under review |
| **[Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](http://arxiv.org/abs/2507.11112v2)** | 2025-10-09 |  |
| **[Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations](http://arxiv.org/abs/2510.08120v1)** | 2025-10-09 | <details><summary>12 pa...</summary><p>12 pages, 2 figures, 3 tables</p></details> |
| **[(Token-Level) InfoRMIA: Stronger Membership Inference and Memorization Assessment for LLMs](http://arxiv.org/abs/2510.05582v2)** | 2025-10-09 |  |
| **[Fewer Weights, More Problems: A Practical Attack on LLM Pruning](http://arxiv.org/abs/2510.07985v1)** | 2025-10-09 |  |
| **[Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](http://arxiv.org/abs/2505.13527v2)** | 2025-10-09 |  |
| **[Rethinking Reasoning: A Survey on Reasoning-based Backdoors in LLMs](http://arxiv.org/abs/2510.07697v1)** | 2025-10-09 |  |
| **[LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics](http://arxiv.org/abs/2510.07626v1)** | 2025-10-08 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols](http://arxiv.org/abs/2508.13220v2)** | 2025-10-09 | <details><summary>This ...</summary><p>This is a technical report from Lingnan University, Hong Kong. Code is available at https://github.com/AIS2Lab/MCPSecBench</p></details> |
| **[Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks](http://arxiv.org/abs/2506.11113v3)** | 2025-10-09 | <details><summary>Minor...</summary><p>Minor correction: Fixed sign errors in the results table. The update does not affect the main findings or conclusions</p></details> |
| **[Rule Encoding and Compliance in Large Language Models: An Information-Theoretic Analysis](http://arxiv.org/abs/2510.05106v2)** | 2025-10-09 |  |
| **[Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models](http://arxiv.org/abs/2508.21099v2)** | 2025-10-09 |  |
| **[PiCo: Jailbreaking Multimodal Large Language Models via Pictorial Code Contextualization](http://arxiv.org/abs/2504.01444v4)** | 2025-10-09 | <details><summary>Accep...</summary><p>Accepted to IEEE International Conference on Multimedia and Expo (ICME) 2025</p></details> |
| **[Effective and Stealthy One-Shot Jailbreaks on Deployed Mobile Vision-Language Agents](http://arxiv.org/abs/2510.07809v1)** | 2025-10-09 |  |
| **[SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models](http://arxiv.org/abs/2509.03487v2)** | 2025-10-08 |  |
| **[L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)](http://arxiv.org/abs/2510.07363v1)** | 2025-10-08 | <details><summary>This ...</summary><p>This preprint was submitted to IEEE TrustCom 2025. The accepted version will be published under copyright 2025 IEEE</p></details> |
| **[DiffMI: Breaking Face Recognition Privacy via Diffusion-Driven Training-Free Model Inversion](http://arxiv.org/abs/2504.18015v3)** | 2025-10-08 |  |
| **[Benchmarking Gaslighting Negation Attacks Against Multimodal Large Language Models](http://arxiv.org/abs/2501.19017v4)** | 2025-10-08 | <details><summary>Proje...</summary><p>Project website: https://yxg1005.github.io/GaslightingNegationAttacks/</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols](http://arxiv.org/abs/2508.13220v2)** | 2025-10-09 | <details><summary>This ...</summary><p>This is a technical report from Lingnan University, Hong Kong. Code is available at https://github.com/AIS2Lab/MCPSecBench</p></details> |
| **[Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks](http://arxiv.org/abs/2506.11113v3)** | 2025-10-09 | <details><summary>Minor...</summary><p>Minor correction: Fixed sign errors in the results table. The update does not affect the main findings or conclusions</p></details> |
| **[Rule Encoding and Compliance in Large Language Models: An Information-Theoretic Analysis](http://arxiv.org/abs/2510.05106v2)** | 2025-10-09 |  |
| **[Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models](http://arxiv.org/abs/2508.21099v2)** | 2025-10-09 |  |
| **[PiCo: Jailbreaking Multimodal Large Language Models via Pictorial Code Contextualization](http://arxiv.org/abs/2504.01444v4)** | 2025-10-09 | <details><summary>Accep...</summary><p>Accepted to IEEE International Conference on Multimedia and Expo (ICME) 2025</p></details> |
| **[Effective and Stealthy One-Shot Jailbreaks on Deployed Mobile Vision-Language Agents](http://arxiv.org/abs/2510.07809v1)** | 2025-10-09 |  |
| **[SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models](http://arxiv.org/abs/2509.03487v2)** | 2025-10-08 |  |
| **[L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)](http://arxiv.org/abs/2510.07363v1)** | 2025-10-08 | <details><summary>This ...</summary><p>This preprint was submitted to IEEE TrustCom 2025. The accepted version will be published under copyright 2025 IEEE</p></details> |
| **[From Injection to Defense: Constructing Edit-Based Fingerprints for Large Language Models](http://arxiv.org/abs/2509.03122v2)** | 2025-10-08 | preprint |
| **[DiffMI: Breaking Face Recognition Privacy via Diffusion-Driven Training-Free Model Inversion](http://arxiv.org/abs/2504.18015v3)** | 2025-10-08 |  |

