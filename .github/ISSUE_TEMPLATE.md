---
title: Latest 15 Papers - October 17, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks](http://arxiv.org/abs/2504.14556v2)** | 2025-10-15 |  |
| **[Selective Adversarial Attacks on LLM Benchmarks](http://arxiv.org/abs/2510.13570v1)** | 2025-10-15 |  |
| **[In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers](http://arxiv.org/abs/2510.13543v1)** | 2025-10-15 | <details><summary>37 pa...</summary><p>37 pages , 10 figures</p></details> |
| **[LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks](http://arxiv.org/abs/2310.11409v6)** | 2025-10-15 |  |
| **[GRIDAI: Generating and Repairing Intrusion Detection Rules via Collaboration among Multiple LLM-based Agents](http://arxiv.org/abs/2510.13257v1)** | 2025-10-15 |  |
| **[Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs](http://arxiv.org/abs/2510.03567v2)** | 2025-10-15 |  |
| **[When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers](http://arxiv.org/abs/2402.10601v5)** | 2025-10-14 | <details><summary>Publi...</summary><p>Published in Reliable ML from Unreliable Data workshop @ NeurIPS 2025</p></details> |
| **[Attention-Aware GNN-based Input Defense against Multi-Turn LLM Jailbreak](http://arxiv.org/abs/2507.07146v2)** | 2025-10-14 |  |
| **[Robust ML-based Detection of Conventional, LLM-Generated, and Adversarial Phishing Emails Using Advanced Text Preprocessing](http://arxiv.org/abs/2510.11915v1)** | 2025-10-13 |  |
| **[Living Off the LLM: How LLMs Will Change Adversary Tactics](http://arxiv.org/abs/2510.11398v1)** | 2025-10-13 | 6 pages, 0 figures |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks](http://arxiv.org/abs/2504.14556v2)** | 2025-10-15 |  |
| **[Selective Adversarial Attacks on LLM Benchmarks](http://arxiv.org/abs/2510.13570v1)** | 2025-10-15 |  |
| **[In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers](http://arxiv.org/abs/2510.13543v1)** | 2025-10-15 | <details><summary>37 pa...</summary><p>37 pages , 10 figures</p></details> |
| **[LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks](http://arxiv.org/abs/2310.11409v6)** | 2025-10-15 |  |
| **[GRIDAI: Generating and Repairing Intrusion Detection Rules via Collaboration among Multiple LLM-based Agents](http://arxiv.org/abs/2510.13257v1)** | 2025-10-15 |  |
| **[Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs](http://arxiv.org/abs/2510.03567v2)** | 2025-10-15 |  |
| **[When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers](http://arxiv.org/abs/2402.10601v5)** | 2025-10-14 | <details><summary>Publi...</summary><p>Published in Reliable ML from Unreliable Data workshop @ NeurIPS 2025</p></details> |
| **[Attention-Aware GNN-based Input Defense against Multi-Turn LLM Jailbreak](http://arxiv.org/abs/2507.07146v2)** | 2025-10-14 |  |
| **[Robust ML-based Detection of Conventional, LLM-Generated, and Adversarial Phishing Emails Using Advanced Text Preprocessing](http://arxiv.org/abs/2510.11915v1)** | 2025-10-13 |  |
| **[Living Off the LLM: How LLMs Will Change Adversary Tactics](http://arxiv.org/abs/2510.11398v1)** | 2025-10-13 | 6 pages, 0 figures |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Risk-adaptive Activation Steering for Safe Multimodal Large Language Models](http://arxiv.org/abs/2510.13698v1)** | 2025-10-15 |  |
| **[Toward Efficient Inference Attacks: Shadow Model Sharing via Mixture-of-Experts](http://arxiv.org/abs/2510.13451v1)** | 2025-10-15 | <details><summary>To ap...</summary><p>To appear in NeurIPS 2025</p></details> |
| **[Personal Attribute Leakage in Federated Speech Models](http://arxiv.org/abs/2510.13357v1)** | 2025-10-15 | <details><summary>5 pag...</summary><p>5 pages, 4 figures, 2 tables</p></details> |
| **[SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models](http://arxiv.org/abs/2510.05173v3)** | 2025-10-15 | <details><summary>Accep...</summary><p>Accepted by ACM CCS 2025, Code is available at [this https URL](https://github.com/pgqihere/safeguider)</p></details> |
| **[Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models](http://arxiv.org/abs/2510.13237v1)** | 2025-10-15 |  |
| **[L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)](http://arxiv.org/abs/2510.07363v2)** | 2025-10-14 | <details><summary>This ...</summary><p>This preprint was submitted to IEEE TrustCom 2025. The accepted version will be published under copyright 2025 IEEE</p></details> |
| **[SafeMT: Multi-turn Safety for Multimodal Language Models](http://arxiv.org/abs/2510.12133v1)** | 2025-10-14 |  |
| **[WW-FL: Secure and Private Large-Scale Federated Learning](http://arxiv.org/abs/2302.09904v4)** | 2025-10-14 | <details><summary>This ...</summary><p>This is the full and extended version of the work, which will be published in the IACR Transactions on Cryptographic Hardware and Embedded Systems (CHES 2026)</p></details> |
| **[Locket: Robust Feature-Locking Technique for Language Models](http://arxiv.org/abs/2510.12117v1)** | 2025-10-14 | 12 pages, 3 figures |
| **[Countermind: A Multi-Layered Security Architecture for Large Language Models](http://arxiv.org/abs/2510.11837v1)** | 2025-10-13 | <details><summary>33 pa...</summary><p>33 pages, 3 figures, 6 tables. Keywords: LLM security; defense-in-depth; prompt injection; activation steering; multimodal sandbox; threat modeling</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Risk-adaptive Activation Steering for Safe Multimodal Large Language Models](http://arxiv.org/abs/2510.13698v1)** | 2025-10-15 |  |
| **[Toward Efficient Inference Attacks: Shadow Model Sharing via Mixture-of-Experts](http://arxiv.org/abs/2510.13451v1)** | 2025-10-15 | <details><summary>To ap...</summary><p>To appear in NeurIPS 2025</p></details> |
| **[Personal Attribute Leakage in Federated Speech Models](http://arxiv.org/abs/2510.13357v1)** | 2025-10-15 | <details><summary>5 pag...</summary><p>5 pages, 4 figures, 2 tables</p></details> |
| **[SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models](http://arxiv.org/abs/2510.05173v3)** | 2025-10-15 | <details><summary>Accep...</summary><p>Accepted by ACM CCS 2025, Code is available at [this https URL](https://github.com/pgqihere/safeguider)</p></details> |
| **[Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models](http://arxiv.org/abs/2510.13237v1)** | 2025-10-15 |  |
| **[Tokens, the oft-overlooked appetizer: Large language models, the distributional hypothesis, and meaning](http://arxiv.org/abs/2412.10924v7)** | 2025-10-14 |  |
| **[L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)](http://arxiv.org/abs/2510.07363v2)** | 2025-10-14 | <details><summary>This ...</summary><p>This preprint was submitted to IEEE TrustCom 2025. The accepted version will be published under copyright 2025 IEEE</p></details> |
| **[SafeMT: Multi-turn Safety for Multimodal Language Models](http://arxiv.org/abs/2510.12133v1)** | 2025-10-14 |  |
| **[WW-FL: Secure and Private Large-Scale Federated Learning](http://arxiv.org/abs/2302.09904v4)** | 2025-10-14 | <details><summary>This ...</summary><p>This is the full and extended version of the work, which will be published in the IACR Transactions on Cryptographic Hardware and Embedded Systems (CHES 2026)</p></details> |
| **[Locket: Robust Feature-Locking Technique for Language Models](http://arxiv.org/abs/2510.12117v1)** | 2025-10-14 | 12 pages, 3 figures |

