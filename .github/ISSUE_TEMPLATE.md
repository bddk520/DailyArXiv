---
title: Latest 15 Papers - October 02, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization](http://arxiv.org/abs/2509.20230v3)** | 2025-09-30 |  |
| **[QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety](http://arxiv.org/abs/2506.12299v3)** | 2025-09-30 | <details><summary>Accep...</summary><p>Accept to ACLW 2025 (WOAH); fix typo</p></details> |
| **[Dagger Behind Smile: Fool LLMs with a Happy Ending Story](http://arxiv.org/abs/2501.13115v3)** | 2025-09-30 | EMNLP 2025 Findings |
| **[STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents](http://arxiv.org/abs/2509.25624v1)** | 2025-09-30 |  |
| **[Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety](http://arxiv.org/abs/2503.05021v2)** | 2025-09-29 | ACL 2025 Findings |
| **[Watermark under Fire: A Robustness Evaluation of LLM Watermarking](http://arxiv.org/abs/2411.13425v4)** | 2025-09-29 | <details><summary>25 pa...</summary><p>25 pages. Accepted by The 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025)</p></details> |
| **[SemanticShield: LLM-Powered Audits Expose Shilling Attacks in Recommender Systems](http://arxiv.org/abs/2509.24961v1)** | 2025-09-29 |  |
| **[HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment](http://arxiv.org/abs/2509.24384v1)** | 2025-09-29 |  |
| **[AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](http://arxiv.org/abs/2506.08885v3)** | 2025-09-28 |  |
| **[Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning](http://arxiv.org/abs/2509.23558v1)** | 2025-09-28 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization](http://arxiv.org/abs/2509.20230v3)** | 2025-09-30 |  |
| **[QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety](http://arxiv.org/abs/2506.12299v3)** | 2025-09-30 | <details><summary>Accep...</summary><p>Accept to ACLW 2025 (WOAH); fix typo</p></details> |
| **[Dagger Behind Smile: Fool LLMs with a Happy Ending Story](http://arxiv.org/abs/2501.13115v3)** | 2025-09-30 | EMNLP 2025 Findings |
| **[STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents](http://arxiv.org/abs/2509.25624v1)** | 2025-09-30 |  |
| **[Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety](http://arxiv.org/abs/2503.05021v2)** | 2025-09-29 | ACL 2025 Findings |
| **[Watermark under Fire: A Robustness Evaluation of LLM Watermarking](http://arxiv.org/abs/2411.13425v4)** | 2025-09-29 | <details><summary>25 pa...</summary><p>25 pages. Accepted by The 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025)</p></details> |
| **[SemanticShield: LLM-Powered Audits Expose Shilling Attacks in Recommender Systems](http://arxiv.org/abs/2509.24961v1)** | 2025-09-29 |  |
| **[HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment](http://arxiv.org/abs/2509.24384v1)** | 2025-09-29 |  |
| **[AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](http://arxiv.org/abs/2506.08885v3)** | 2025-09-28 |  |
| **[Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning](http://arxiv.org/abs/2509.23558v1)** | 2025-09-28 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models](http://arxiv.org/abs/2509.26584v1)** | 2025-09-30 |  |
| **[STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models](http://arxiv.org/abs/2509.26473v1)** | 2025-09-30 |  |
| **[SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate Jailbreak Attacks in Large Language Models](http://arxiv.org/abs/2509.26345v1)** | 2025-09-30 | 27 pages, 5 figure |
| **[Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions](http://arxiv.org/abs/2501.01872v6)** | 2025-09-30 | <details><summary>Accep...</summary><p>Accepted at EMNLP 2025 (Main)</p></details> |
| **[Detecting Instruction Fine-tuning Attacks on Language Models using Influence Function](http://arxiv.org/abs/2504.09026v2)** | 2025-09-30 |  |
| **[Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](http://arxiv.org/abs/2505.17601v4)** | 2025-09-30 |  |
| **[Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models](http://arxiv.org/abs/2509.21761v2)** | 2025-09-30 |  |
| **[Model Extraction Attacks Revisited](http://arxiv.org/abs/2312.05386v2)** | 2025-09-29 | <details><summary>Accep...</summary><p>Accepted by Proceedings of the 19th ACM Asia Conference on Computer and Communications Security(AsiaCCS 2024)</p></details> |
| **[AutoRAN: Automated Hijacking of Safety Reasoning in Large Reasoning Models](http://arxiv.org/abs/2505.10846v2)** | 2025-09-29 | 10 pages |
| **[Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](http://arxiv.org/abs/2505.14608v2)** | 2025-09-29 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models](http://arxiv.org/abs/2509.26584v1)** | 2025-09-30 |  |
| **[STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models](http://arxiv.org/abs/2509.26473v1)** | 2025-09-30 |  |
| **[SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate Jailbreak Attacks in Large Language Models](http://arxiv.org/abs/2509.26345v1)** | 2025-09-30 | 27 pages, 5 figure |
| **[Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions](http://arxiv.org/abs/2501.01872v6)** | 2025-09-30 | <details><summary>Accep...</summary><p>Accepted at EMNLP 2025 (Main)</p></details> |
| **[Detecting Instruction Fine-tuning Attacks on Language Models using Influence Function](http://arxiv.org/abs/2504.09026v2)** | 2025-09-30 |  |
| **[Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](http://arxiv.org/abs/2505.17601v4)** | 2025-09-30 |  |
| **[Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models](http://arxiv.org/abs/2509.21761v2)** | 2025-09-30 |  |
| **[SMS: Self-supervised Model Seeding for Verification of Machine Unlearning](http://arxiv.org/abs/2509.25613v1)** | 2025-09-30 |  |
| **[Model Extraction Attacks Revisited](http://arxiv.org/abs/2312.05386v2)** | 2025-09-29 | <details><summary>Accep...</summary><p>Accepted by Proceedings of the 19th ACM Asia Conference on Computer and Communications Security(AsiaCCS 2024)</p></details> |
| **[AutoRAN: Automated Hijacking of Safety Reasoning in Large Reasoning Models](http://arxiv.org/abs/2505.10846v2)** | 2025-09-29 | 10 pages |

