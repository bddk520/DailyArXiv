---
title: Latest 15 Papers - June 05, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Unveiling Privacy Risks in LLM Agent Memory](http://arxiv.org/abs/2502.13172v2)** | 2025-06-03 | <details><summary>ACL 2...</summary><p>ACL 2025 (Main Conference)</p></details> |
| **[Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs](http://arxiv.org/abs/2506.02965v1)** | 2025-06-03 | 20 pages, 4 figures, |
| **[Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](http://arxiv.org/abs/2505.02862v2)** | 2025-06-03 |  |
| **[SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage](http://arxiv.org/abs/2412.15289v3)** | 2025-06-03 |  |
| **[Should LLM Safety Be More Than Refusing Harmful Instructions?](http://arxiv.org/abs/2506.02442v1)** | 2025-06-03 | Preprint |
| **[AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](http://arxiv.org/abs/2404.16873v2)** | 2025-06-02 | <details><summary>Accep...</summary><p>Accepted to ICML 2025. Code is available at http://github.com/facebookresearch/advprompter</p></details> |
| **[INVARLLM: LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems Anomaly Detection](http://arxiv.org/abs/2411.10918v2)** | 2025-06-02 |  |
| **[Which Factors Make Code LLMs More Vulnerable to Backdoor Attacks? A Systematic Study](http://arxiv.org/abs/2506.01825v1)** | 2025-06-02 |  |
| **[ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs](http://arxiv.org/abs/2506.01770v1)** | 2025-06-02 |  |
| **[MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments](http://arxiv.org/abs/2506.01616v1)** | 2025-06-02 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Unveiling Privacy Risks in LLM Agent Memory](http://arxiv.org/abs/2502.13172v2)** | 2025-06-03 | <details><summary>ACL 2...</summary><p>ACL 2025 (Main Conference)</p></details> |
| **[Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs](http://arxiv.org/abs/2506.02965v1)** | 2025-06-03 | 20 pages, 4 figures, |
| **[Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](http://arxiv.org/abs/2505.02862v2)** | 2025-06-03 |  |
| **[SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage](http://arxiv.org/abs/2412.15289v3)** | 2025-06-03 |  |
| **[Should LLM Safety Be More Than Refusing Harmful Instructions?](http://arxiv.org/abs/2506.02442v1)** | 2025-06-03 | Preprint |
| **[AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](http://arxiv.org/abs/2404.16873v2)** | 2025-06-02 | <details><summary>Accep...</summary><p>Accepted to ICML 2025. Code is available at http://github.com/facebookresearch/advprompter</p></details> |
| **[INVARLLM: LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems Anomaly Detection](http://arxiv.org/abs/2411.10918v2)** | 2025-06-02 |  |
| **[Which Factors Make Code LLMs More Vulnerable to Backdoor Attacks? A Systematic Study](http://arxiv.org/abs/2506.01825v1)** | 2025-06-02 |  |
| **[ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs](http://arxiv.org/abs/2506.01770v1)** | 2025-06-02 |  |
| **[MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments](http://arxiv.org/abs/2506.01616v1)** | 2025-06-02 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step](http://arxiv.org/abs/2410.03869v2)** | 2025-06-03 | <details><summary>Accep...</summary><p>Accepted by ACL 2025 Findings</p></details> |
| **[On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses](http://arxiv.org/abs/2506.02978v1)** | 2025-06-03 |  |
| **[A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos](http://arxiv.org/abs/2502.15806v2)** | 2025-06-03 |  |
| **[Towards the Worst-case Robustness of Large Language Models](http://arxiv.org/abs/2501.19040v3)** | 2025-06-03 |  |
| **[Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning](http://arxiv.org/abs/2308.16061v2)** | 2025-06-03 |  |
| **[BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage](http://arxiv.org/abs/2506.02479v1)** | 2025-06-03 | <details><summary>24 pa...</summary><p>24 pages, 24 figures, and 7 tables</p></details> |
| **[MISLEADER: Defending against Model Extraction with Ensembles of Distilled Models](http://arxiv.org/abs/2506.02362v1)** | 2025-06-03 |  |
| **[How Breakable Is Privacy: Probing and Resisting Model Inversion Attacks in Collaborative Inference](http://arxiv.org/abs/2501.00824v6)** | 2025-06-03 | <details><summary>15 pa...</summary><p>15 pages, 5 figures, 6 tables. The experimental data have been corrected, and some explanations have been supplemented</p></details> |
| **[Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](http://arxiv.org/abs/2504.05050v4)** | 2025-06-03 |  |
| **[SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings](http://arxiv.org/abs/2502.12562v3)** | 2025-06-02 | <details><summary>Accep...</summary><p>Accepted in ACL 2025 Main Track</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step](http://arxiv.org/abs/2410.03869v2)** | 2025-06-03 | <details><summary>Accep...</summary><p>Accepted by ACL 2025 Findings</p></details> |
| **[On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses](http://arxiv.org/abs/2506.02978v1)** | 2025-06-03 |  |
| **[A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos](http://arxiv.org/abs/2502.15806v2)** | 2025-06-03 |  |
| **[Towards the Worst-case Robustness of Large Language Models](http://arxiv.org/abs/2501.19040v3)** | 2025-06-03 |  |
| **[Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning](http://arxiv.org/abs/2308.16061v2)** | 2025-06-03 |  |
| **[BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage](http://arxiv.org/abs/2506.02479v1)** | 2025-06-03 | <details><summary>24 pa...</summary><p>24 pages, 24 figures, and 7 tables</p></details> |
| **[MISLEADER: Defending against Model Extraction with Ensembles of Distilled Models](http://arxiv.org/abs/2506.02362v1)** | 2025-06-03 |  |
| **[How Breakable Is Privacy: Probing and Resisting Model Inversion Attacks in Collaborative Inference](http://arxiv.org/abs/2501.00824v6)** | 2025-06-03 | <details><summary>15 pa...</summary><p>15 pages, 5 figures, 6 tables. The experimental data have been corrected, and some explanations have been supplemented</p></details> |
| **[Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](http://arxiv.org/abs/2504.05050v4)** | 2025-06-03 |  |
| **[SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings](http://arxiv.org/abs/2502.12562v3)** | 2025-06-02 | <details><summary>Accep...</summary><p>Accepted in ACL 2025 Main Track</p></details> |

