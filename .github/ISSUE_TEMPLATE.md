---
title: Latest 15 Papers - October 01, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[SemanticShield: LLM-Powered Audits Expose Shilling Attacks in Recommender Systems](http://arxiv.org/abs/2509.24961v1)** | 2025-09-29 |  |
| **[HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment](http://arxiv.org/abs/2509.24384v1)** | 2025-09-29 |  |
| **[Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization](http://arxiv.org/abs/2509.20230v2)** | 2025-09-29 |  |
| **[AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](http://arxiv.org/abs/2506.08885v3)** | 2025-09-28 |  |
| **[Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning](http://arxiv.org/abs/2509.23558v1)** | 2025-09-28 |  |
| **[Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems](http://arxiv.org/abs/2509.15213v2)** | 2025-09-27 |  |
| **[NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning](http://arxiv.org/abs/2509.23252v1)** | 2025-09-27 | preprint version |
| **[Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data](http://arxiv.org/abs/2509.23041v1)** | 2025-09-27 | <details><summary>NeurI...</summary><p>NeurIPS 2025 Spotlight. Source code: https://github.com/liangzid/VirusInfectionAttack</p></details> |
| **[LLM Watermark Evasion via Bias Inversion](http://arxiv.org/abs/2509.23019v1)** | 2025-09-27 |  |
| **[When Speculation Spills Secrets: Side Channels via Speculative Decoding In LLMs](http://arxiv.org/abs/2411.01076v3)** | 2025-09-26 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[SemanticShield: LLM-Powered Audits Expose Shilling Attacks in Recommender Systems](http://arxiv.org/abs/2509.24961v1)** | 2025-09-29 |  |
| **[HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment](http://arxiv.org/abs/2509.24384v1)** | 2025-09-29 |  |
| **[Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization](http://arxiv.org/abs/2509.20230v2)** | 2025-09-29 |  |
| **[AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](http://arxiv.org/abs/2506.08885v3)** | 2025-09-28 |  |
| **[Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning](http://arxiv.org/abs/2509.23558v1)** | 2025-09-28 |  |
| **[Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems](http://arxiv.org/abs/2509.15213v2)** | 2025-09-27 |  |
| **[NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning](http://arxiv.org/abs/2509.23252v1)** | 2025-09-27 | preprint version |
| **[Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data](http://arxiv.org/abs/2509.23041v1)** | 2025-09-27 | <details><summary>NeurI...</summary><p>NeurIPS 2025 Spotlight. Source code: https://github.com/liangzid/VirusInfectionAttack</p></details> |
| **[LLM Watermark Evasion via Bias Inversion](http://arxiv.org/abs/2509.23019v1)** | 2025-09-27 |  |
| **[When Speculation Spills Secrets: Side Channels via Speculative Decoding In LLMs](http://arxiv.org/abs/2411.01076v3)** | 2025-09-26 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](http://arxiv.org/abs/2505.14608v2)** | 2025-09-29 |  |
| **[Score-based Membership Inference on Diffusion Models](http://arxiv.org/abs/2509.25003v1)** | 2025-09-29 |  |
| **[PRIVMARK: Private Large Language Models Watermarking with MPC](http://arxiv.org/abs/2509.24624v1)** | 2025-09-29 | <details><summary>8 pag...</summary><p>8 pages, 4 figures, under peer-review</p></details> |
| **[TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent](http://arxiv.org/abs/2505.20118v3)** | 2025-09-29 | <details><summary>9 pag...</summary><p>9 pages, 5 figures To be presented in the Conference on Empirical Methods in Natural Language Processing, 2025</p></details> |
| **[TokenSwap: Backdoor Attack on the Compositional Understanding of Large Vision-Language Models](http://arxiv.org/abs/2509.24566v1)** | 2025-09-29 |  |
| **[Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models](http://arxiv.org/abs/2509.19994v2)** | 2025-09-29 |  |
| **[Guided Uncertainty Learning Using a Post-Hoc Evidential Meta-Model](http://arxiv.org/abs/2509.24492v1)** | 2025-09-29 |  |
| **[DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models](http://arxiv.org/abs/2509.24296v1)** | 2025-09-29 |  |
| **[AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models](http://arxiv.org/abs/2509.24269v1)** | 2025-09-29 |  |
| **[BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models](http://arxiv.org/abs/2505.16670v3)** | 2025-09-29 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](http://arxiv.org/abs/2505.14608v2)** | 2025-09-29 |  |
| **[Score-based Membership Inference on Diffusion Models](http://arxiv.org/abs/2509.25003v1)** | 2025-09-29 |  |
| **[PRIVMARK: Private Large Language Models Watermarking with MPC](http://arxiv.org/abs/2509.24624v1)** | 2025-09-29 | <details><summary>8 pag...</summary><p>8 pages, 4 figures, under peer-review</p></details> |
| **[TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent](http://arxiv.org/abs/2505.20118v3)** | 2025-09-29 | <details><summary>9 pag...</summary><p>9 pages, 5 figures To be presented in the Conference on Empirical Methods in Natural Language Processing, 2025</p></details> |
| **[TokenSwap: Backdoor Attack on the Compositional Understanding of Large Vision-Language Models](http://arxiv.org/abs/2509.24566v1)** | 2025-09-29 |  |
| **[Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models](http://arxiv.org/abs/2509.19994v2)** | 2025-09-29 |  |
| **[Guided Uncertainty Learning Using a Post-Hoc Evidential Meta-Model](http://arxiv.org/abs/2509.24492v1)** | 2025-09-29 |  |
| **[DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models](http://arxiv.org/abs/2509.24296v1)** | 2025-09-29 |  |
| **[AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models](http://arxiv.org/abs/2509.24269v1)** | 2025-09-29 |  |
| **[BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models](http://arxiv.org/abs/2505.16670v3)** | 2025-09-29 |  |

