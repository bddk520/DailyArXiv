---
title: Latest 15 Papers - May 30, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing](http://arxiv.org/abs/2505.22298v1)** | 2025-05-28 | ACL 2025 Findings |
| **[Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs](http://arxiv.org/abs/2502.19041v2)** | 2025-05-28 | <details><summary>16 pa...</summary><p>16 pages, 12 figures, ACL 2025 findings</p></details> |
| **[MedSentry: Understanding and Mitigating Safety Risks in Medical LLM Multi-Agent Systems](http://arxiv.org/abs/2505.20824v1)** | 2025-05-27 |  |
| **[Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond](http://arxiv.org/abs/2502.05374v4)** | 2025-05-27 | <details><summary>Accep...</summary><p>Accepted by ICML 2025</p></details> |
| **[Capability-Based Scaling Laws for LLM Red-Teaming](http://arxiv.org/abs/2505.20162v1)** | 2025-05-26 |  |
| **[PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks](http://arxiv.org/abs/2505.13862v3)** | 2025-05-26 |  |
| **[Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings](http://arxiv.org/abs/2412.13879v4)** | 2025-05-26 | <details><summary>22 pa...</summary><p>22 pages, 8 figures, 11 tables</p></details> |
| **[JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs](http://arxiv.org/abs/2402.05668v3)** | 2025-05-26 | <details><summary>Corre...</summary><p>Correct typos and update new experiment results. Accepted in ACL 2025. 25 pages, 12 figures</p></details> |
| **[Firewalls to Secure Dynamic LLM Agentic Networks](http://arxiv.org/abs/2502.01822v5)** | 2025-05-26 |  |
| **[What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs](http://arxiv.org/abs/2505.19773v1)** | 2025-05-26 | Accepted by ACL 2025 |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing](http://arxiv.org/abs/2505.22298v1)** | 2025-05-28 | ACL 2025 Findings |
| **[Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs](http://arxiv.org/abs/2502.19041v2)** | 2025-05-28 | <details><summary>16 pa...</summary><p>16 pages, 12 figures, ACL 2025 findings</p></details> |
| **[MedSentry: Understanding and Mitigating Safety Risks in Medical LLM Multi-Agent Systems](http://arxiv.org/abs/2505.20824v1)** | 2025-05-27 |  |
| **[Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond](http://arxiv.org/abs/2502.05374v4)** | 2025-05-27 | <details><summary>Accep...</summary><p>Accepted by ICML 2025</p></details> |
| **[Capability-Based Scaling Laws for LLM Red-Teaming](http://arxiv.org/abs/2505.20162v1)** | 2025-05-26 |  |
| **[PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks](http://arxiv.org/abs/2505.13862v3)** | 2025-05-26 |  |
| **[Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings](http://arxiv.org/abs/2412.13879v4)** | 2025-05-26 | <details><summary>22 pa...</summary><p>22 pages, 8 figures, 11 tables</p></details> |
| **[JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs](http://arxiv.org/abs/2402.05668v3)** | 2025-05-26 | <details><summary>Corre...</summary><p>Correct typos and update new experiment results. Accepted in ACL 2025. 25 pages, 12 figures</p></details> |
| **[Firewalls to Secure Dynamic LLM Agentic Networks](http://arxiv.org/abs/2502.01822v5)** | 2025-05-26 |  |
| **[What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs](http://arxiv.org/abs/2505.19773v1)** | 2025-05-26 | Accepted by ACL 2025 |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Understanding Adversarial Training with Energy-based Models](http://arxiv.org/abs/2505.22486v1)** | 2025-05-28 | <details><summary>Under...</summary><p>Under review for TPAMI</p></details> |
| **[VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models](http://arxiv.org/abs/2505.19684v2)** | 2025-05-28 |  |
| **[Privacy-preserving Prompt Personalization in Federated Learning for Multimodal Large Language Models](http://arxiv.org/abs/2505.22447v1)** | 2025-05-28 | Under Review |
| **[Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](http://arxiv.org/abs/2505.22271v1)** | 2025-05-28 | Under Review |
| **[PriRoAgg: Achieving Robust Model Aggregation with Minimum Privacy Leakage for Federated Learning](http://arxiv.org/abs/2407.08954v2)** | 2025-05-28 | <details><summary>Accep...</summary><p>Accepted to IEEE Transactions on Information Forensics and Security (TIFS)</p></details> |
| **[Understanding Model Ensemble in Transferable Adversarial Attack](http://arxiv.org/abs/2410.06851v3)** | 2025-05-28 | <details><summary>Accep...</summary><p>Accepted by ICML 2025</p></details> |
| **[Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](http://arxiv.org/abs/2406.14023v3)** | 2025-05-28 | <details><summary>Accep...</summary><p>Accepted to ACL 2025 Findings</p></details> |
| **[REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space](http://arxiv.org/abs/2406.09325v4)** | 2025-05-28 | <details><summary>ACL 2...</summary><p>ACL 2025 Findings, 24 pages, 4 figures</p></details> |
| **[Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](http://arxiv.org/abs/2505.17601v2)** | 2025-05-28 |  |
| **[Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack](http://arxiv.org/abs/2505.21967v1)** | 2025-05-28 | Preprint |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Understanding Adversarial Training with Energy-based Models](http://arxiv.org/abs/2505.22486v1)** | 2025-05-28 | <details><summary>Under...</summary><p>Under review for TPAMI</p></details> |
| **[VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models](http://arxiv.org/abs/2505.19684v2)** | 2025-05-28 |  |
| **[Privacy-preserving Prompt Personalization in Federated Learning for Multimodal Large Language Models](http://arxiv.org/abs/2505.22447v1)** | 2025-05-28 | Under Review |
| **[Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](http://arxiv.org/abs/2505.22271v1)** | 2025-05-28 | Under Review |
| **[PriRoAgg: Achieving Robust Model Aggregation with Minimum Privacy Leakage for Federated Learning](http://arxiv.org/abs/2407.08954v2)** | 2025-05-28 | <details><summary>Accep...</summary><p>Accepted to IEEE Transactions on Information Forensics and Security (TIFS)</p></details> |
| **[Understanding Model Ensemble in Transferable Adversarial Attack](http://arxiv.org/abs/2410.06851v3)** | 2025-05-28 | <details><summary>Accep...</summary><p>Accepted by ICML 2025</p></details> |
| **[Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](http://arxiv.org/abs/2406.14023v3)** | 2025-05-28 | <details><summary>Accep...</summary><p>Accepted to ACL 2025 Findings</p></details> |
| **[REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space](http://arxiv.org/abs/2406.09325v4)** | 2025-05-28 | <details><summary>ACL 2...</summary><p>ACL 2025 Findings, 24 pages, 4 figures</p></details> |
| **[Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](http://arxiv.org/abs/2505.17601v2)** | 2025-05-28 |  |
| **[Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack](http://arxiv.org/abs/2505.21967v1)** | 2025-05-28 | Preprint |

