---
title: Latest 15 Papers - February 26, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs](http://arxiv.org/abs/2409.14866v4)** | 2025-02-24 |  |
| **[RapidPen: Fully Automated IP-to-Shell Penetration Testing with LLM-based Agents](http://arxiv.org/abs/2502.16730v1)** | 2025-02-23 |  |
| **[Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond](http://arxiv.org/abs/2502.05374v2)** | 2025-02-23 |  |
| **[On Calibration of LLM-based Guard Models for Reliable Content Moderation](http://arxiv.org/abs/2410.10414v2)** | 2025-02-23 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025</p></details> |
| **[A generative approach to LLM harmfulness detection with special red flag tokens](http://arxiv.org/abs/2502.16366v1)** | 2025-02-22 | 13 pages, 6 figures |
| **[Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors](http://arxiv.org/abs/2410.19230v2)** | 2025-02-22 | 29 pages |
| **[Merger-as-a-Stealer: Stealing Targeted PII from Aligned LLMs with Model Merging](http://arxiv.org/abs/2502.16094v1)** | 2025-02-22 | 17 pages, 3 figures |
| **[CVE-LLM : Ontology-Assisted Automatic Vulnerability Evaluation Using Large Language Models](http://arxiv.org/abs/2502.15932v1)** | 2025-02-21 | <details><summary>arXiv...</summary><p>arXiv admin note: substantial text overlap with arXiv:2407.14640</p></details> |
| **[IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector](http://arxiv.org/abs/2502.15902v1)** | 2025-02-21 |  |
| **[Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders](http://arxiv.org/abs/2502.15576v1)** | 2025-02-21 | <details><summary>Pre-p...</summary><p>Pre-print. 20 pages, 5 figures</p></details> |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](http://arxiv.org/abs/2502.17424v1)** | 2025-02-24 | 10 pages, 9 figures |
| **[PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs](http://arxiv.org/abs/2409.14866v4)** | 2025-02-24 |  |
| **[RapidPen: Fully Automated IP-to-Shell Penetration Testing with LLM-based Agents](http://arxiv.org/abs/2502.16730v1)** | 2025-02-23 |  |
| **[Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond](http://arxiv.org/abs/2502.05374v2)** | 2025-02-23 |  |
| **[On Calibration of LLM-based Guard Models for Reliable Content Moderation](http://arxiv.org/abs/2410.10414v2)** | 2025-02-23 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025</p></details> |
| **[A generative approach to LLM harmfulness detection with special red flag tokens](http://arxiv.org/abs/2502.16366v1)** | 2025-02-22 | 13 pages, 6 figures |
| **[Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors](http://arxiv.org/abs/2410.19230v2)** | 2025-02-22 | 29 pages |
| **[Merger-as-a-Stealer: Stealing Targeted PII from Aligned LLMs with Model Merging](http://arxiv.org/abs/2502.16094v1)** | 2025-02-22 | 17 pages, 3 figures |
| **[CVE-LLM : Ontology-Assisted Automatic Vulnerability Evaluation Using Large Language Models](http://arxiv.org/abs/2502.15932v1)** | 2025-02-21 | <details><summary>arXiv...</summary><p>arXiv admin note: substantial text overlap with arXiv:2407.14640</p></details> |
| **[IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector](http://arxiv.org/abs/2502.15902v1)** | 2025-02-21 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence](http://arxiv.org/abs/2502.17420v1)** | 2025-02-24 |  |
| **[REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective](http://arxiv.org/abs/2502.17254v1)** | 2025-02-24 | <details><summary>30 pa...</summary><p>30 pages, 6 figures, 15 tables</p></details> |
| **[Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Images](http://arxiv.org/abs/2502.16593v1)** | 2025-02-23 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025</p></details> |
| **[SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model](http://arxiv.org/abs/2501.18636v2)** | 2025-02-23 |  |
| **[On Calibration of LLM-based Guard Models for Reliable Content Moderation](http://arxiv.org/abs/2410.10414v2)** | 2025-02-23 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025</p></details> |
| **[Intrinsic Model Weaknesses: How Priming Attacks Unveil Vulnerabilities in Large Language Models](http://arxiv.org/abs/2502.16491v1)** | 2025-02-23 |  |
| **[Unified Prompt Attack Against Text-to-Image Generation Models](http://arxiv.org/abs/2502.16423v1)** | 2025-02-23 | <details><summary>Accep...</summary><p>Accepted by IEEE T-PAMI 2025</p></details> |
| **[A Framework for Evaluating Vision-Language Model Safety: Building Trust in AI for Public Sector Applications](http://arxiv.org/abs/2502.16361v1)** | 2025-02-22 | <details><summary>AAAI ...</summary><p>AAAI 2025 Workshop on AI for Social Impact: Bridging Innovations in Finance, Social Media, and Crime Prevention</p></details> |
| **[Your Diffusion Model is Secretly a Certifiably Robust Classifier](http://arxiv.org/abs/2402.02316v4)** | 2025-02-22 | <details><summary>Accep...</summary><p>Accepted by NeurIPS 2024. Also named as "Diffusion Models are Certifiably Robust Classifiers"</p></details> |
| **[PersGuard: Preventing Malicious Personalization via Backdoor Attacks on Pre-trained Text-to-Image Diffusion Models](http://arxiv.org/abs/2502.16167v1)** | 2025-02-22 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence](http://arxiv.org/abs/2502.17420v1)** | 2025-02-24 |  |
| **[REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective](http://arxiv.org/abs/2502.17254v1)** | 2025-02-24 | <details><summary>30 pa...</summary><p>30 pages, 6 figures, 15 tables</p></details> |
| **[Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Images](http://arxiv.org/abs/2502.16593v1)** | 2025-02-23 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025</p></details> |
| **[SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model](http://arxiv.org/abs/2501.18636v2)** | 2025-02-23 |  |
| **[On Calibration of LLM-based Guard Models for Reliable Content Moderation](http://arxiv.org/abs/2410.10414v2)** | 2025-02-23 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025</p></details> |
| **[Intrinsic Model Weaknesses: How Priming Attacks Unveil Vulnerabilities in Large Language Models](http://arxiv.org/abs/2502.16491v1)** | 2025-02-23 |  |
| **[Unified Prompt Attack Against Text-to-Image Generation Models](http://arxiv.org/abs/2502.16423v1)** | 2025-02-23 | <details><summary>Accep...</summary><p>Accepted by IEEE T-PAMI 2025</p></details> |
| **[A Framework for Evaluating Vision-Language Model Safety: Building Trust in AI for Public Sector Applications](http://arxiv.org/abs/2502.16361v1)** | 2025-02-22 | <details><summary>AAAI ...</summary><p>AAAI 2025 Workshop on AI for Social Impact: Bridging Innovations in Finance, Social Media, and Crime Prevention</p></details> |
| **[Beyond Trusting Trust: Multi-Model Validation for Robust Code Generation](http://arxiv.org/abs/2502.16279v1)** | 2025-02-22 | 3 pages, 2 figures |
| **[Your Diffusion Model is Secretly a Certifiably Robust Classifier](http://arxiv.org/abs/2402.02316v4)** | 2025-02-22 | <details><summary>Accep...</summary><p>Accepted by NeurIPS 2024. Also named as "Diffusion Models are Certifiably Robust Classifiers"</p></details> |

