---
title: Latest 15 Papers - February 11, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning](http://arxiv.org/abs/2410.07163v3)** | 2025-02-07 |  |
| **[GenBFA: An Evolutionary Optimization Approach to Bit-Flip Attacks on LLMs](http://arxiv.org/abs/2411.13757v2)** | 2025-02-07 |  |
| **[From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection](http://arxiv.org/abs/2412.10198v2)** | 2025-02-07 |  |
| **[Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions](http://arxiv.org/abs/2502.04322v1)** | 2025-02-06 |  |
| **[Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach Penetration-Testing Active Directory Networks](http://arxiv.org/abs/2502.04227v1)** | 2025-02-06 |  |
| **["Short-length" Adversarial Training Helps LLMs Defend "Long-length" Jailbreak Attacks: Theoretical and Empirical Evidence](http://arxiv.org/abs/2502.04204v1)** | 2025-02-06 |  |
| **[OverThink: Slowdown Attacks on Reasoning LLMs](http://arxiv.org/abs/2502.02542v2)** | 2025-02-05 |  |
| **[SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner](http://arxiv.org/abs/2406.05498v3)** | 2025-02-05 | <details><summary>Accep...</summary><p>Accepted by USENIX Security Symposium 2025. Please cite the conference version of this paper, i.e., "Xunguang Wang, Daoyuan Wu, Zhenlan Ji, Zongjie Li, Pingchuan Ma, Shuai Wang, Yingjiu Li, Yang Liu, Ning Liu, and Juergen Rahmel. SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner. In Proc. USENIX Security, 2025."</p></details> |
| **[Lost in Overlap: Exploring Logit-based Watermark Collision in LLMs](http://arxiv.org/abs/2403.10020v3)** | 2025-02-05 | <details><summary>Long ...</summary><p>Long Paper, 9 pages, accepted at NAACL 2025 Findings</p></details> |
| **[Certifying LLM Safety against Adversarial Prompting](http://arxiv.org/abs/2309.02705v4)** | 2025-02-04 | <details><summary>Accep...</summary><p>Accepted at COLM 2024: https://openreview.net/forum?id=9Ik05cycLq</p></details> |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning](http://arxiv.org/abs/2410.07163v3)** | 2025-02-07 |  |
| **[GenBFA: An Evolutionary Optimization Approach to Bit-Flip Attacks on LLMs](http://arxiv.org/abs/2411.13757v2)** | 2025-02-07 |  |
| **[From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection](http://arxiv.org/abs/2412.10198v2)** | 2025-02-07 |  |
| **[Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions](http://arxiv.org/abs/2502.04322v1)** | 2025-02-06 |  |
| **[Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach Penetration-Testing Active Directory Networks](http://arxiv.org/abs/2502.04227v1)** | 2025-02-06 |  |
| **["Short-length" Adversarial Training Helps LLMs Defend "Long-length" Jailbreak Attacks: Theoretical and Empirical Evidence](http://arxiv.org/abs/2502.04204v1)** | 2025-02-06 |  |
| **[OverThink: Slowdown Attacks on Reasoning LLMs](http://arxiv.org/abs/2502.02542v2)** | 2025-02-05 |  |
| **[SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner](http://arxiv.org/abs/2406.05498v3)** | 2025-02-05 | <details><summary>Accep...</summary><p>Accepted by USENIX Security Symposium 2025. Please cite the conference version of this paper, i.e., "Xunguang Wang, Daoyuan Wu, Zhenlan Ji, Zongjie Li, Pingchuan Ma, Shuai Wang, Yingjiu Li, Yang Liu, Ning Liu, and Juergen Rahmel. SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner. In Proc. USENIX Security, 2025."</p></details> |
| **[Lost in Overlap: Exploring Logit-based Watermark Collision in LLMs](http://arxiv.org/abs/2403.10020v3)** | 2025-02-05 | <details><summary>Long ...</summary><p>Long Paper, 9 pages, accepted at NAACL 2025 Findings</p></details> |
| **[Certifying LLM Safety against Adversarial Prompting](http://arxiv.org/abs/2309.02705v4)** | 2025-02-04 | <details><summary>Accep...</summary><p>Accepted at COLM 2024: https://openreview.net/forum?id=9Ik05cycLq</p></details> |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Do Unlearning Methods Remove Information from Language Model Weights?](http://arxiv.org/abs/2410.08827v3)** | 2025-02-07 |  |
| **[Enhancing SQL Injection Detection and Prevention Using Generative Models](http://arxiv.org/abs/2502.04786v1)** | 2025-02-07 | <details><summary>13 pa...</summary><p>13 pages, 22 Figures, 1 Table</p></details> |
| **[DMPA: Model Poisoning Attacks on Decentralized Federated Learning for Model Differences](http://arxiv.org/abs/2502.04771v1)** | 2025-02-07 | 8 pages, 3 figures |
| **[Enhancing Phishing Email Identification with Large Language Models](http://arxiv.org/abs/2502.04759v1)** | 2025-02-07 | 9 pages, 5 figures |
| **[Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models](http://arxiv.org/abs/2410.02298v4)** | 2025-02-07 | <details><summary>Accep...</summary><p>Accepted by ICLR2025. url: https://openreview.net/forum?id=s20W12XTF8</p></details> |
| **[Membership Inference Attacks Against Vision-Language Models](http://arxiv.org/abs/2501.18624v2)** | 2025-02-07 | <details><summary>Accep...</summary><p>Accepted by USENIX'25; 22 pages, 28 figures;</p></details> |
| **[Confidence Elicitation: A New Attack Vector for Large Language Models](http://arxiv.org/abs/2502.04643v1)** | 2025-02-07 | <details><summary>Publi...</summary><p>Published in ICLR 2025. The code is publicly available at https://github.com/Aniloid2/Confidence_Elicitation_Attacks</p></details> |
| **[Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Carrier Articles](http://arxiv.org/abs/2408.11182v2)** | 2025-02-07 |  |
| **[The Gradient Puppeteer: Adversarial Domination in Gradient Leakage Attacks through Model Poisoning](http://arxiv.org/abs/2502.04106v1)** | 2025-02-06 |  |
| **[DiffZOO: A Purely Query-Based Black-Box Attack for Red-teaming Text-to-Image Generative Model via Zeroth Order Optimization](http://arxiv.org/abs/2408.11071v2)** | 2025-02-06 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Do Unlearning Methods Remove Information from Language Model Weights?](http://arxiv.org/abs/2410.08827v3)** | 2025-02-07 |  |
| **[Enhancing SQL Injection Detection and Prevention Using Generative Models](http://arxiv.org/abs/2502.04786v1)** | 2025-02-07 | <details><summary>13 pa...</summary><p>13 pages, 22 Figures, 1 Table</p></details> |
| **[DMPA: Model Poisoning Attacks on Decentralized Federated Learning for Model Differences](http://arxiv.org/abs/2502.04771v1)** | 2025-02-07 | 8 pages, 3 figures |
| **[Enhancing Phishing Email Identification with Large Language Models](http://arxiv.org/abs/2502.04759v1)** | 2025-02-07 | 9 pages, 5 figures |
| **[Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models](http://arxiv.org/abs/2410.02298v4)** | 2025-02-07 | <details><summary>Accep...</summary><p>Accepted by ICLR2025. url: https://openreview.net/forum?id=s20W12XTF8</p></details> |
| **[Membership Inference Attacks Against Vision-Language Models](http://arxiv.org/abs/2501.18624v2)** | 2025-02-07 | <details><summary>Accep...</summary><p>Accepted by USENIX'25; 22 pages, 28 figures;</p></details> |
| **[Confidence Elicitation: A New Attack Vector for Large Language Models](http://arxiv.org/abs/2502.04643v1)** | 2025-02-07 | <details><summary>Publi...</summary><p>Published in ICLR 2025. The code is publicly available at https://github.com/Aniloid2/Confidence_Elicitation_Attacks</p></details> |
| **[Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Carrier Articles](http://arxiv.org/abs/2408.11182v2)** | 2025-02-07 |  |
| **[The Gradient Puppeteer: Adversarial Domination in Gradient Leakage Attacks through Model Poisoning](http://arxiv.org/abs/2502.04106v1)** | 2025-02-06 |  |
| **[DiffZOO: A Purely Query-Based Black-Box Attack for Red-teaming Text-to-Image Generative Model via Zeroth Order Optimization](http://arxiv.org/abs/2408.11071v2)** | 2025-02-06 |  |

