---
title: Latest 15 Papers - October 28, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots](http://arxiv.org/abs/2510.21459v1)** | 2025-10-24 | <details><summary>to be...</summary><p>to be published in: The 3rd International Conference on Foundation and Large Language Models (FLLM2025), IEEE, 2025</p></details> |
| **[Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs](http://arxiv.org/abs/2502.14828v2)** | 2025-10-24 |  |
| **[FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract Security](http://arxiv.org/abs/2510.21401v1)** | 2025-10-24 |  |
| **[LLM-Powered Detection of Price Manipulation in DeFi](http://arxiv.org/abs/2510.21272v1)** | 2025-10-24 |  |
| **[Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data](http://arxiv.org/abs/2509.23041v2)** | 2025-10-24 | <details><summary>Camer...</summary><p>Camera Ready of NeurIPS 2025 Spotlight. Source code: https://github.com/liangzid/VirusInfectionAttack</p></details> |
| **[The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning](http://arxiv.org/abs/2510.21190v1)** | 2025-10-24 | under review |
| **[RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards](http://arxiv.org/abs/2506.07736v3)** | 2025-10-24 |  |
| **[NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge](http://arxiv.org/abs/2510.21144v1)** | 2025-10-24 |  |
| **[DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents](http://arxiv.org/abs/2506.12104v2)** | 2025-10-24 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2025</p></details> |
| **[A Reinforcement Learning Framework for Robust and Secure LLM Watermarking](http://arxiv.org/abs/2510.21053v1)** | 2025-10-23 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots](http://arxiv.org/abs/2510.21459v1)** | 2025-10-24 | <details><summary>to be...</summary><p>to be published in: The 3rd International Conference on Foundation and Large Language Models (FLLM2025), IEEE, 2025</p></details> |
| **[Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs](http://arxiv.org/abs/2502.14828v2)** | 2025-10-24 |  |
| **[FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract Security](http://arxiv.org/abs/2510.21401v1)** | 2025-10-24 |  |
| **[LLM-Powered Detection of Price Manipulation in DeFi](http://arxiv.org/abs/2510.21272v1)** | 2025-10-24 |  |
| **[Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data](http://arxiv.org/abs/2509.23041v2)** | 2025-10-24 | <details><summary>Camer...</summary><p>Camera Ready of NeurIPS 2025 Spotlight. Source code: https://github.com/liangzid/VirusInfectionAttack</p></details> |
| **[The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning](http://arxiv.org/abs/2510.21190v1)** | 2025-10-24 | under review |
| **[RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards](http://arxiv.org/abs/2506.07736v3)** | 2025-10-24 |  |
| **[NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge](http://arxiv.org/abs/2510.21144v1)** | 2025-10-24 |  |
| **[DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents](http://arxiv.org/abs/2506.12104v2)** | 2025-10-24 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2025</p></details> |
| **[A Reinforcement Learning Framework for Robust and Secure LLM Watermarking](http://arxiv.org/abs/2510.21053v1)** | 2025-10-23 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails](http://arxiv.org/abs/2510.21285v1)** | 2025-10-24 | <details><summary>First...</summary><p>First two authors contributed equally. The main text is 10 pages, with an appendix of 19 pages. The paper contains 18 figures and 16 tables</p></details> |
| **[How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models](http://arxiv.org/abs/2501.01741v2)** | 2025-10-24 |  |
| **[Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency](http://arxiv.org/abs/2510.21189v1)** | 2025-10-24 | <details><summary>Accep...</summary><p>Accepted in NeurIPS 2025</p></details> |
| **[Quantifying CBRN Risk in Frontier Models](http://arxiv.org/abs/2510.21133v1)** | 2025-10-24 |  |
| **[Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations](http://arxiv.org/abs/2505.13763v2)** | 2025-10-24 |  |
| **[BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation](http://arxiv.org/abs/2510.20792v1)** | 2025-10-23 |  |
| **[From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks](http://arxiv.org/abs/2502.05325v3)** | 2025-10-23 |  |
| **[Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models](http://arxiv.org/abs/2510.20468v1)** | 2025-10-23 | NeurIPS 2025 |
| **[GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models](http://arxiv.org/abs/2510.17621v2)** | 2025-10-23 | <details><summary>This ...</summary><p>This work has been submitted to the IEEE for possible publication</p></details> |
| **[Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations](http://arxiv.org/abs/2510.20223v1)** | 2025-10-23 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails](http://arxiv.org/abs/2510.21285v1)** | 2025-10-24 | <details><summary>First...</summary><p>First two authors contributed equally. The main text is 10 pages, with an appendix of 19 pages. The paper contains 18 figures and 16 tables</p></details> |
| **[How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models](http://arxiv.org/abs/2501.01741v2)** | 2025-10-24 |  |
| **[Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency](http://arxiv.org/abs/2510.21189v1)** | 2025-10-24 | <details><summary>Accep...</summary><p>Accepted in NeurIPS 2025</p></details> |
| **[Quantifying CBRN Risk in Frontier Models](http://arxiv.org/abs/2510.21133v1)** | 2025-10-24 |  |
| **[Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations](http://arxiv.org/abs/2505.13763v2)** | 2025-10-24 |  |
| **[BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation](http://arxiv.org/abs/2510.20792v1)** | 2025-10-23 |  |
| **[From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks](http://arxiv.org/abs/2502.05325v3)** | 2025-10-23 |  |
| **[Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models](http://arxiv.org/abs/2510.20468v1)** | 2025-10-23 | NeurIPS 2025 |
| **[GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models](http://arxiv.org/abs/2510.17621v2)** | 2025-10-23 | <details><summary>This ...</summary><p>This work has been submitted to the IEEE for possible publication</p></details> |
| **[Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations](http://arxiv.org/abs/2510.20223v1)** | 2025-10-23 |  |

