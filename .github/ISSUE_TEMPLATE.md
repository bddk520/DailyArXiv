---
title: Latest 15 Papers - June 06, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[TracLLM: A Generic Framework for Attributing Long Context LLMs](http://arxiv.org/abs/2506.04202v1)** | 2025-06-04 | <details><summary>To ap...</summary><p>To appear in USENIX Security Symposium 2025. The code and data are at: https://github.com/Wang-Yanting/TracLLM</p></details> |
| **[Client-Side Zero-Shot LLM Inference for Comprehensive In-Browser URL Analysis](http://arxiv.org/abs/2506.03656v1)** | 2025-06-04 | 46 pages , 5 figures |
| **[Should LLM Safety Be More Than Refusing Harmful Instructions?](http://arxiv.org/abs/2506.02442v2)** | 2025-06-04 | Preprint |
| **[PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs](http://arxiv.org/abs/2506.02965v2)** | 2025-06-04 | 20 pages, 4 figures |
| **[Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks](http://arxiv.org/abs/2405.20099v2)** | 2025-06-04 |  |
| **[Unveiling Privacy Risks in LLM Agent Memory](http://arxiv.org/abs/2502.13172v2)** | 2025-06-03 | <details><summary>ACL 2...</summary><p>ACL 2025 (Main Conference)</p></details> |
| **[Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](http://arxiv.org/abs/2505.02862v2)** | 2025-06-03 |  |
| **[SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage](http://arxiv.org/abs/2412.15289v3)** | 2025-06-03 |  |
| **[AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](http://arxiv.org/abs/2404.16873v2)** | 2025-06-02 | <details><summary>Accep...</summary><p>Accepted to ICML 2025. Code is available at http://github.com/facebookresearch/advprompter</p></details> |
| **[INVARLLM: LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems Anomaly Detection](http://arxiv.org/abs/2411.10918v2)** | 2025-06-02 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[TracLLM: A Generic Framework for Attributing Long Context LLMs](http://arxiv.org/abs/2506.04202v1)** | 2025-06-04 | <details><summary>To ap...</summary><p>To appear in USENIX Security Symposium 2025. The code and data are at: https://github.com/Wang-Yanting/TracLLM</p></details> |
| **[Client-Side Zero-Shot LLM Inference for Comprehensive In-Browser URL Analysis](http://arxiv.org/abs/2506.03656v1)** | 2025-06-04 | 46 pages , 5 figures |
| **[Should LLM Safety Be More Than Refusing Harmful Instructions?](http://arxiv.org/abs/2506.02442v2)** | 2025-06-04 | Preprint |
| **[PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs](http://arxiv.org/abs/2506.02965v2)** | 2025-06-04 | 20 pages, 4 figures |
| **[Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks](http://arxiv.org/abs/2405.20099v2)** | 2025-06-04 |  |
| **[Unveiling Privacy Risks in LLM Agent Memory](http://arxiv.org/abs/2502.13172v2)** | 2025-06-03 | <details><summary>ACL 2...</summary><p>ACL 2025 (Main Conference)</p></details> |
| **[Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](http://arxiv.org/abs/2505.02862v2)** | 2025-06-03 |  |
| **[SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage](http://arxiv.org/abs/2412.15289v3)** | 2025-06-03 |  |
| **[AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](http://arxiv.org/abs/2404.16873v2)** | 2025-06-02 | <details><summary>Accep...</summary><p>Accepted to ICML 2025. Code is available at http://github.com/facebookresearch/advprompter</p></details> |
| **[INVARLLM: LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems Anomaly Detection](http://arxiv.org/abs/2411.10918v2)** | 2025-06-02 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models](http://arxiv.org/abs/2506.03933v1)** | 2025-06-04 |  |
| **[Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets](http://arxiv.org/abs/2506.03870v1)** | 2025-06-04 |  |
| **[Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks](http://arxiv.org/abs/2506.03627v1)** | 2025-06-04 | 13pages |
| **[Across Programming Language Silos: A Study on Cross-Lingual Retrieval-augmented Code Generation](http://arxiv.org/abs/2506.03535v1)** | 2025-06-04 |  |
| **[Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks](http://arxiv.org/abs/2501.09328v3)** | 2025-06-04 |  |
| **[Adversarial Attacks on Robotic Vision Language Action Models](http://arxiv.org/abs/2506.03350v1)** | 2025-06-03 |  |
| **[Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region](http://arxiv.org/abs/2502.13946v2)** | 2025-06-03 | ACL 2025 Main |
| **[Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step](http://arxiv.org/abs/2410.03869v2)** | 2025-06-03 | <details><summary>Accep...</summary><p>Accepted by ACL 2025 Findings</p></details> |
| **[BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF](http://arxiv.org/abs/2506.03234v1)** | 2025-06-03 |  |
| **[On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses](http://arxiv.org/abs/2506.02978v1)** | 2025-06-03 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models](http://arxiv.org/abs/2506.03933v1)** | 2025-06-04 |  |
| **[Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets](http://arxiv.org/abs/2506.03870v1)** | 2025-06-04 |  |
| **[Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks](http://arxiv.org/abs/2506.03627v1)** | 2025-06-04 | 13pages |
| **[Across Programming Language Silos: A Study on Cross-Lingual Retrieval-augmented Code Generation](http://arxiv.org/abs/2506.03535v1)** | 2025-06-04 |  |
| **[Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks](http://arxiv.org/abs/2501.09328v3)** | 2025-06-04 |  |
| **[Adversarial Attacks on Robotic Vision Language Action Models](http://arxiv.org/abs/2506.03350v1)** | 2025-06-03 |  |
| **[Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region](http://arxiv.org/abs/2502.13946v2)** | 2025-06-03 | ACL 2025 Main |
| **[Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step](http://arxiv.org/abs/2410.03869v2)** | 2025-06-03 | <details><summary>Accep...</summary><p>Accepted by ACL 2025 Findings</p></details> |
| **[BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF](http://arxiv.org/abs/2506.03234v1)** | 2025-06-03 |  |
| **[On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses](http://arxiv.org/abs/2506.02978v1)** | 2025-06-03 |  |

