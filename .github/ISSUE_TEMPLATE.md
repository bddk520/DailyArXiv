---
title: Latest 15 Papers - February 09, 2026
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy](https://arxiv.org/abs/2602.05877v1)** | 2026-02-05 |  |
| **[Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates](https://arxiv.org/abs/2602.04653v2)** | 2026-02-05 |  |
| **[Clouding the Mirror: Stealthy Prompt Injection Attacks Targeting LLM-based Phishing Detection](https://arxiv.org/abs/2602.05484v1)** | 2026-02-05 |  |
| **[Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs](https://arxiv.org/abs/2602.05444v1)** | 2026-02-05 |  |
| **[How Catastrophic is Your LLM? Certifying Risk in Conversation](https://arxiv.org/abs/2510.03969v3)** | 2026-02-05 | <details><summary>Accep...</summary><p>Accepted by ICLR 2026</p></details> |
| **[STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068v3)** | 2026-02-05 | <details><summary>Add r...</summary><p>Add results on other models and datasets</p></details> |
| **[OverThink: Slowdown Attacks on Reasoning LLMs](https://arxiv.org/abs/2502.02542v4)** | 2026-02-04 |  |
| **[Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases](https://arxiv.org/abs/2602.04739v1)** | 2026-02-04 | under peer-review |
| **[GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering](https://arxiv.org/abs/2512.06655v2)** | 2026-02-04 |  |
| **[PriMod4AI: Lifecycle-Aware Privacy Threat Modeling for AI Systems using LLM](https://arxiv.org/abs/2602.04927v1)** | 2026-02-04 | <details><summary>Accep...</summary><p>Accepted at the NDSS LAST-X Workshop 2026</p></details> |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy](https://arxiv.org/abs/2602.05877v1)** | 2026-02-05 |  |
| **[Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates](https://arxiv.org/abs/2602.04653v2)** | 2026-02-05 |  |
| **[Clouding the Mirror: Stealthy Prompt Injection Attacks Targeting LLM-based Phishing Detection](https://arxiv.org/abs/2602.05484v1)** | 2026-02-05 |  |
| **[Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs](https://arxiv.org/abs/2602.05444v1)** | 2026-02-05 |  |
| **[How Catastrophic is Your LLM? Certifying Risk in Conversation](https://arxiv.org/abs/2510.03969v3)** | 2026-02-05 | <details><summary>Accep...</summary><p>Accepted by ICLR 2026</p></details> |
| **[STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068v3)** | 2026-02-05 | <details><summary>Add r...</summary><p>Add results on other models and datasets</p></details> |
| **[OverThink: Slowdown Attacks on Reasoning LLMs](https://arxiv.org/abs/2502.02542v4)** | 2026-02-04 |  |
| **[Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases](https://arxiv.org/abs/2602.04739v1)** | 2026-02-04 | under peer-review |
| **[GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering](https://arxiv.org/abs/2512.06655v2)** | 2026-02-04 |  |
| **[PriMod4AI: Lifecycle-Aware Privacy Threat Modeling for AI Systems using LLM](https://arxiv.org/abs/2602.04927v1)** | 2026-02-04 | <details><summary>Accep...</summary><p>Accepted at the NDSS LAST-X Workshop 2026</p></details> |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Verification of the Implicit World Model in a Generative Model via Adversarial Sequences](https://arxiv.org/abs/2602.05903v1)** | 2026-02-05 | <details><summary>Accep...</summary><p>Accepted at ICLR 2026. Code, datasets, and models are available at https://github.com/szegedai/world-model-verification</p></details> |
| **[Alignment-Aware Model Adaptation via Feedback-Guided Optimization](https://arxiv.org/abs/2602.02258v2)** | 2026-02-05 |  |
| **[BadTemplate: A Training-Free Backdoor Attack via Chat Template Against Large Language Models](https://arxiv.org/abs/2602.05401v1)** | 2026-02-05 | <details><summary>This ...</summary><p>This paper includes biased content that may be disturbing or offensive to certain readers</p></details> |
| **[Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models](https://arxiv.org/abs/2510.08859v2)** | 2026-02-05 |  |
| **[Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models](https://arxiv.org/abs/2601.13948v3)** | 2026-02-05 | <details><summary>Accep...</summary><p>Accepted by ICASSP2026. Demo/code: https://paniquex.github.io/Stream-Voice-Anon/</p></details> |
| **[From Sands to Mansions: Towards Automated Cyberattack Emulation with Classical Planning and Large Language Models](https://arxiv.org/abs/2407.16928v4)** | 2026-02-05 | <details><summary>This ...</summary><p>This is the author-accepted version of a paper accepted at the Applied Cryptography and Network Security (ACNS 2026)</p></details> |
| **[Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?](https://arxiv.org/abs/2602.05023v1)** | 2026-02-04 | <details><summary>Accep...</summary><p>Accepted by ICLR 2026</p></details> |
| **[Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases](https://arxiv.org/abs/2602.04739v1)** | 2026-02-04 | under peer-review |
| **[RASA: Routing-Aware Safety Alignment for Mixture-of-Experts Models](https://arxiv.org/abs/2602.04448v1)** | 2026-02-04 | 9 pages |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Verification of the Implicit World Model in a Generative Model via Adversarial Sequences](https://arxiv.org/abs/2602.05903v1)** | 2026-02-05 | <details><summary>Accep...</summary><p>Accepted at ICLR 2026. Code, datasets, and models are available at https://github.com/szegedai/world-model-verification</p></details> |
| **[Alignment-Aware Model Adaptation via Feedback-Guided Optimization](https://arxiv.org/abs/2602.02258v2)** | 2026-02-05 |  |
| **[The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces](https://arxiv.org/abs/2512.13821v2)** | 2026-02-05 | 13 Pages, A Preprint |
| **[BadTemplate: A Training-Free Backdoor Attack via Chat Template Against Large Language Models](https://arxiv.org/abs/2602.05401v1)** | 2026-02-05 | <details><summary>This ...</summary><p>This paper includes biased content that may be disturbing or offensive to certain readers</p></details> |
| **[Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models](https://arxiv.org/abs/2510.08859v2)** | 2026-02-05 |  |
| **[Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models](https://arxiv.org/abs/2601.13948v3)** | 2026-02-05 | <details><summary>Accep...</summary><p>Accepted by ICASSP2026. Demo/code: https://paniquex.github.io/Stream-Voice-Anon/</p></details> |
| **[From Sands to Mansions: Towards Automated Cyberattack Emulation with Classical Planning and Large Language Models](https://arxiv.org/abs/2407.16928v4)** | 2026-02-05 | <details><summary>This ...</summary><p>This is the author-accepted version of a paper accepted at the Applied Cryptography and Network Security (ACNS 2026)</p></details> |
| **[Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions](https://arxiv.org/abs/2602.05234v1)** | 2026-02-05 | <details><summary>55 pa...</summary><p>55 pages, 25 figures; accepted for ICLR 2026</p></details> |
| **[Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?](https://arxiv.org/abs/2602.05023v1)** | 2026-02-04 | <details><summary>Accep...</summary><p>Accepted by ICLR 2026</p></details> |

