---
title: Latest 15 Papers - December 04, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation](https://arxiv.org/abs/2405.13068v3)** | 2025-12-02 |  |
| **[COGNITION: From Evaluation to Defense against Multimodal LLM CAPTCHA Solvers](https://arxiv.org/abs/2512.02318v1)** | 2025-12-02 |  |
| **[TradeTrap: Are LLM-based Trading Agents Truly Reliable and Faithful?](https://arxiv.org/abs/2512.02261v1)** | 2025-12-01 |  |
| **[Latent Debate: A Surrogate Framework for Interpreting LLM Thinking](https://arxiv.org/abs/2512.01909v1)** | 2025-12-01 | Preprint |
| **[A Wolf in Sheep's Clothing: Bypassing Commercial LLM Guardrails via Harmless Prompt Weaving and Adaptive Tree Search](https://arxiv.org/abs/2512.01353v1)** | 2025-12-01 |  |
| **[Securing Large Language Models (LLMs) from Prompt Injection Attacks](https://arxiv.org/abs/2512.01326v1)** | 2025-12-01 | <details><summary>10 pa...</summary><p>10 pages, 1 figure, 1 table</p></details> |
| **[SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](https://arxiv.org/abs/2510.04398v2)** | 2025-11-30 | <details><summary>Accep...</summary><p>Accepted at NeurIPS 2025. Code is available at https://github.com/Buyun-Liang/SECA</p></details> |
| **[Enhancing Jailbreak Attacks on LLMs via Persona Prompts](https://arxiv.org/abs/2507.22171v2)** | 2025-11-30 | <details><summary>Works...</summary><p>Workshop on LLM Persona Modeling at NeurIPS 2025</p></details> |
| **[Efficient LLM-Jailbreaking via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015v3)** | 2025-11-30 |  |
| **[On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks](https://arxiv.org/abs/2507.06489v2)** | 2025-11-28 | <details><summary>Publi...</summary><p>Published in NeurIPS 2025</p></details> |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation](https://arxiv.org/abs/2405.13068v3)** | 2025-12-02 |  |
| **[COGNITION: From Evaluation to Defense against Multimodal LLM CAPTCHA Solvers](https://arxiv.org/abs/2512.02318v1)** | 2025-12-02 |  |
| **[TradeTrap: Are LLM-based Trading Agents Truly Reliable and Faithful?](https://arxiv.org/abs/2512.02261v1)** | 2025-12-01 |  |
| **[Latent Debate: A Surrogate Framework for Interpreting LLM Thinking](https://arxiv.org/abs/2512.01909v1)** | 2025-12-01 | Preprint |
| **[A Wolf in Sheep's Clothing: Bypassing Commercial LLM Guardrails via Harmless Prompt Weaving and Adaptive Tree Search](https://arxiv.org/abs/2512.01353v1)** | 2025-12-01 |  |
| **[Securing Large Language Models (LLMs) from Prompt Injection Attacks](https://arxiv.org/abs/2512.01326v1)** | 2025-12-01 | <details><summary>10 pa...</summary><p>10 pages, 1 figure, 1 table</p></details> |
| **[SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](https://arxiv.org/abs/2510.04398v2)** | 2025-11-30 | <details><summary>Accep...</summary><p>Accepted at NeurIPS 2025. Code is available at https://github.com/Buyun-Liang/SECA</p></details> |
| **[Enhancing Jailbreak Attacks on LLMs via Persona Prompts](https://arxiv.org/abs/2507.22171v2)** | 2025-11-30 | <details><summary>Works...</summary><p>Workshop on LLM Persona Modeling at NeurIPS 2025</p></details> |
| **[Efficient LLM-Jailbreaking via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015v3)** | 2025-11-30 |  |
| **[On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks](https://arxiv.org/abs/2507.06489v2)** | 2025-11-28 | <details><summary>Publi...</summary><p>Published in NeurIPS 2025</p></details> |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Invasive Context Engineering to Control Large Language Models](https://arxiv.org/abs/2512.03001v1)** | 2025-12-02 | 4 pages |
| **[OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295v2)** | 2025-12-02 | WACV2026 Accepted |
| **[Defense That Attacks: How Robust Models Become Better Attackers](https://arxiv.org/abs/2512.02830v1)** | 2025-12-02 |  |
| **[Membership Inference Attack against Large Language Model-based Recommendation Systems: A New Distillation-based Paradigm](https://arxiv.org/abs/2511.14763v2)** | 2025-12-02 |  |
| **[A review of mechanistic and data-driven models of terrorism and radicalization](https://arxiv.org/abs/1903.08485v3)** | 2025-12-02 | 80 pages, 17 figures |
| **[Rank Matters: Understanding and Defending Model Inversion Attacks via Low-Rank Feature Filtering](https://arxiv.org/abs/2410.05814v4)** | 2025-12-02 | KDD 2026 Accept |
| **[Reasoning Up the Instruction Ladder for Controllable Language Models](https://arxiv.org/abs/2511.04694v3)** | 2025-12-01 |  |
| **[Adversarial Confusion Attack: Disrupting Multimodal Large Language Models](https://arxiv.org/abs/2511.20494v3)** | 2025-12-01 |  |
| **[Do Vision-Language Models Leak What They Learn? Adaptive Token-Weighted Model Inversion Attacks](https://arxiv.org/abs/2508.04097v2)** | 2025-12-01 | Under review |
| **[Securing Large Language Models (LLMs) from Prompt Injection Attacks](https://arxiv.org/abs/2512.01326v1)** | 2025-12-01 | <details><summary>10 pa...</summary><p>10 pages, 1 figure, 1 table</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Invasive Context Engineering to Control Large Language Models](https://arxiv.org/abs/2512.03001v1)** | 2025-12-02 | 4 pages |
| **[OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295v2)** | 2025-12-02 | WACV2026 Accepted |
| **[Defense That Attacks: How Robust Models Become Better Attackers](https://arxiv.org/abs/2512.02830v1)** | 2025-12-02 |  |
| **[Membership Inference Attack against Large Language Model-based Recommendation Systems: A New Distillation-based Paradigm](https://arxiv.org/abs/2511.14763v2)** | 2025-12-02 |  |
| **[A review of mechanistic and data-driven models of terrorism and radicalization](https://arxiv.org/abs/1903.08485v3)** | 2025-12-02 | 80 pages, 17 figures |
| **[Rank Matters: Understanding and Defending Model Inversion Attacks via Low-Rank Feature Filtering](https://arxiv.org/abs/2410.05814v4)** | 2025-12-02 | KDD 2026 Accept |
| **[Reasoning Up the Instruction Ladder for Controllable Language Models](https://arxiv.org/abs/2511.04694v3)** | 2025-12-01 |  |
| **[Adversarial Confusion Attack: Disrupting Multimodal Large Language Models](https://arxiv.org/abs/2511.20494v3)** | 2025-12-01 |  |
| **[Do Vision-Language Models Leak What They Learn? Adaptive Token-Weighted Model Inversion Attacks](https://arxiv.org/abs/2508.04097v2)** | 2025-12-01 | Under review |
| **[Securing Large Language Models (LLMs) from Prompt Injection Attacks](https://arxiv.org/abs/2512.01326v1)** | 2025-12-01 | <details><summary>10 pa...</summary><p>10 pages, 1 figure, 1 table</p></details> |

