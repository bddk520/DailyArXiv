---
title: Latest 15 Papers - February 10, 2026
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering](https://arxiv.org/abs/2602.06911v1)** | 2026-02-06 | 28 pages, 13 figures |
| **[Plato's Form: Toward Backdoor Defense-as-a-Service for LLMs with Prototype Representations](https://arxiv.org/abs/2602.06887v1)** | 2026-02-06 |  |
| **[Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks](https://arxiv.org/abs/2507.02735v3)** | 2026-02-06 |  |
| **[Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs](https://arxiv.org/abs/2602.05444v2)** | 2026-02-06 |  |
| **[TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking](https://arxiv.org/abs/2602.06440v1)** | 2026-02-06 |  |
| **[VENOMREC: Cross-Modal Interactive Poisoning for Targeted Promotion in Multimodal LLM Recommender Systems](https://arxiv.org/abs/2602.06409v1)** | 2026-02-06 |  |
| **[MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539v2)** | 2026-02-06 |  |
| **[MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs](https://arxiv.org/abs/2602.06268v1)** | 2026-02-06 | 13 pages, 7 figures |
| **[Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy](https://arxiv.org/abs/2602.05877v1)** | 2026-02-05 |  |
| **[Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates](https://arxiv.org/abs/2602.04653v2)** | 2026-02-05 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering](https://arxiv.org/abs/2602.06911v1)** | 2026-02-06 | 28 pages, 13 figures |
| **[Plato's Form: Toward Backdoor Defense-as-a-Service for LLMs with Prototype Representations](https://arxiv.org/abs/2602.06887v1)** | 2026-02-06 |  |
| **[Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks](https://arxiv.org/abs/2507.02735v3)** | 2026-02-06 |  |
| **[Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs](https://arxiv.org/abs/2602.05444v2)** | 2026-02-06 |  |
| **[TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking](https://arxiv.org/abs/2602.06440v1)** | 2026-02-06 |  |
| **[VENOMREC: Cross-Modal Interactive Poisoning for Targeted Promotion in Multimodal LLM Recommender Systems](https://arxiv.org/abs/2602.06409v1)** | 2026-02-06 |  |
| **[MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539v2)** | 2026-02-06 |  |
| **[MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs](https://arxiv.org/abs/2602.06268v1)** | 2026-02-06 | 13 pages, 7 figures |
| **[Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy](https://arxiv.org/abs/2602.05877v1)** | 2026-02-05 |  |
| **[Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates](https://arxiv.org/abs/2602.04653v2)** | 2026-02-05 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Next-generation cyberattack detection with large language models: anomaly analysis across heterogeneous logs](https://arxiv.org/abs/2602.06777v1)** | 2026-02-06 |  |
| **[AEGIS: Adversarial Target-Guided Retention-Data-Free Robust Concept Erasure from Diffusion Models](https://arxiv.org/abs/2602.06771v1)** | 2026-02-06 | 30 pages,12 figures |
| **[Adversarial generalization of unfolding (model-based) networks](https://arxiv.org/abs/2509.15370v4)** | 2026-02-06 | <details><summary>Accep...</summary><p>Accepted at NeurIPS2025</p></details> |
| **[Temperature Scaling Attack Disrupting Model Confidence in Federated Learning](https://arxiv.org/abs/2602.06638v1)** | 2026-02-06 | 20 pages, 20 figures |
| **[Malicious Agent Skills in the Wild: A Large-Scale Security Empirical Study](https://arxiv.org/abs/2602.06547v1)** | 2026-02-06 |  |
| **[Private and interpretable clinical prediction with quantum-inspired tensor train models](https://arxiv.org/abs/2602.06110v1)** | 2026-02-05 | <details><summary>21 pa...</summary><p>21 pages, 5 figures, 9 tables. The code for the experiments is publicly available at https://github.com/joserapa98/tns4loris</p></details> |
| **[Verification of the Implicit World Model in a Generative Model via Adversarial Sequences](https://arxiv.org/abs/2602.05903v1)** | 2026-02-05 | <details><summary>Accep...</summary><p>Accepted at ICLR 2026. Code, datasets, and models are available at https://github.com/szegedai/world-model-verification</p></details> |
| **[Alignment-Aware Model Adaptation via Feedback-Guided Optimization](https://arxiv.org/abs/2602.02258v2)** | 2026-02-05 |  |
| **[BadTemplate: A Training-Free Backdoor Attack via Chat Template Against Large Language Models](https://arxiv.org/abs/2602.05401v1)** | 2026-02-05 | <details><summary>This ...</summary><p>This paper includes biased content that may be disturbing or offensive to certain readers</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Next-generation cyberattack detection with large language models: anomaly analysis across heterogeneous logs](https://arxiv.org/abs/2602.06777v1)** | 2026-02-06 |  |
| **[AEGIS: Adversarial Target-Guided Retention-Data-Free Robust Concept Erasure from Diffusion Models](https://arxiv.org/abs/2602.06771v1)** | 2026-02-06 | 30 pages,12 figures |
| **[Adversarial generalization of unfolding (model-based) networks](https://arxiv.org/abs/2509.15370v4)** | 2026-02-06 | <details><summary>Accep...</summary><p>Accepted at NeurIPS2025</p></details> |
| **[Temperature Scaling Attack Disrupting Model Confidence in Federated Learning](https://arxiv.org/abs/2602.06638v1)** | 2026-02-06 | 20 pages, 20 figures |
| **[Malicious Agent Skills in the Wild: A Large-Scale Security Empirical Study](https://arxiv.org/abs/2602.06547v1)** | 2026-02-06 |  |
| **[Private and interpretable clinical prediction with quantum-inspired tensor train models](https://arxiv.org/abs/2602.06110v1)** | 2026-02-05 | <details><summary>21 pa...</summary><p>21 pages, 5 figures, 9 tables. The code for the experiments is publicly available at https://github.com/joserapa98/tns4loris</p></details> |
| **[Verification of the Implicit World Model in a Generative Model via Adversarial Sequences](https://arxiv.org/abs/2602.05903v1)** | 2026-02-05 | <details><summary>Accep...</summary><p>Accepted at ICLR 2026. Code, datasets, and models are available at https://github.com/szegedai/world-model-verification</p></details> |
| **[Alignment-Aware Model Adaptation via Feedback-Guided Optimization](https://arxiv.org/abs/2602.02258v2)** | 2026-02-05 |  |
| **[The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces](https://arxiv.org/abs/2512.13821v2)** | 2026-02-05 | 13 Pages, A Preprint |

