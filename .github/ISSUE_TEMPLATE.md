---
title: Latest 15 Papers - November 25, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning](https://arxiv.org/abs/2503.17987v3)** | 2025-11-21 | <details><summary>Noted...</summary><p>Noted that This paper includes model-generated content that may contain offensive or distressing material</p></details> |
| **[SoK: Honeypots & LLMs, More Than the Sum of Their Parts?](https://arxiv.org/abs/2510.25939v3)** | 2025-11-20 | <details><summary>Syste...</summary><p>Systemization of Knowledge</p></details> |
| **["To Survive, I Must Defect": Jailbreaking LLMs via the Game-Theory Scenarios](https://arxiv.org/abs/2511.16278v1)** | 2025-11-20 | 20 pages |
| **[PSM: Prompt Sensitivity Minimization via LLM-Guided Black-Box Optimization](https://arxiv.org/abs/2511.16209v1)** | 2025-11-20 |  |
| **[Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs](https://arxiv.org/abs/2511.05919v2)** | 2025-11-20 |  |
| **[As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192v2)** | 2025-11-20 |  |
| **[AutoBackdoor: Automating Backdoor Attacks via LLM Agents](https://arxiv.org/abs/2511.16709v1)** | 2025-11-20 | 23 pages |
| **[A Closer Look at Adversarial Suffix Learning for Jailbreaking LLMs: Augmented Adversarial Trigger Learning](https://arxiv.org/abs/2503.12339v4)** | 2025-11-19 | <details><summary>the A...</summary><p>the Association for Computational Linguistics: NAACL 2025</p></details> |
| **[Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks](https://arxiv.org/abs/2511.15203v1)** | 2025-11-19 |  |
| **[PromptCOS: Towards Content-only System Prompt Copyright Auditing for LLMs](https://arxiv.org/abs/2509.03117v2)** | 2025-11-19 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning](https://arxiv.org/abs/2503.17987v3)** | 2025-11-21 | <details><summary>Noted...</summary><p>Noted that This paper includes model-generated content that may contain offensive or distressing material</p></details> |
| **[SoK: Honeypots & LLMs, More Than the Sum of Their Parts?](https://arxiv.org/abs/2510.25939v3)** | 2025-11-20 | <details><summary>Syste...</summary><p>Systemization of Knowledge</p></details> |
| **["To Survive, I Must Defect": Jailbreaking LLMs via the Game-Theory Scenarios](https://arxiv.org/abs/2511.16278v1)** | 2025-11-20 | 20 pages |
| **[PSM: Prompt Sensitivity Minimization via LLM-Guided Black-Box Optimization](https://arxiv.org/abs/2511.16209v1)** | 2025-11-20 |  |
| **[Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs](https://arxiv.org/abs/2511.05919v2)** | 2025-11-20 |  |
| **[As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192v2)** | 2025-11-20 |  |
| **[AutoBackdoor: Automating Backdoor Attacks via LLM Agents](https://arxiv.org/abs/2511.16709v1)** | 2025-11-20 | 23 pages |
| **[A Closer Look at Adversarial Suffix Learning for Jailbreaking LLMs: Augmented Adversarial Trigger Learning](https://arxiv.org/abs/2503.12339v4)** | 2025-11-19 | <details><summary>the A...</summary><p>the Association for Computational Linguistics: NAACL 2025</p></details> |
| **[Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks](https://arxiv.org/abs/2511.15203v1)** | 2025-11-19 |  |
| **[PromptCOS: Towards Content-only System Prompt Copyright Auditing for LLMs](https://arxiv.org/abs/2509.03117v2)** | 2025-11-19 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models](https://arxiv.org/abs/2511.02894v3)** | 2025-11-21 |  |
| **[Steering in the Shadows: Causal Amplification for Activation Space Attacks in Large Language Models](https://arxiv.org/abs/2511.17194v1)** | 2025-11-21 | <details><summary>31 pa...</summary><p>31 pages, 5 figures, 9 tables</p></details> |
| **[Model Inversion Attack Against Deep Hashing](https://arxiv.org/abs/2511.12233v2)** | 2025-11-21 |  |
| **[Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models](https://arxiv.org/abs/2507.05248v2)** | 2025-11-21 | <details><summary>20 pa...</summary><p>20 pages, 10 figures. Code and data available at https://github.com/Dtc7w3PQ/Response-Attack</p></details> |
| **[T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model](https://arxiv.org/abs/2510.22300v2)** | 2025-11-21 | AAAI 2026 |
| **[Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning](https://arxiv.org/abs/2503.17987v3)** | 2025-11-21 | <details><summary>Noted...</summary><p>Noted that This paper includes model-generated content that may contain offensive or distressing material</p></details> |
| **[MultiPriv: Benchmarking Individual-Level Privacy Reasoning in Vision-Language Models](https://arxiv.org/abs/2511.16940v1)** | 2025-11-21 |  |
| **[PEPPER: Perception-Guided Perturbation for Robust Backdoor Defense in Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.16830v1)** | 2025-11-20 |  |
| **[Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense](https://arxiv.org/abs/2511.16483v1)** | 2025-11-20 | <details><summary>Accep...</summary><p>Accepted in the AAAI-26 Workshop on Artificial Intelligence for Cyber Security (AICS)</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models](https://arxiv.org/abs/2511.02894v3)** | 2025-11-21 |  |
| **[Steering in the Shadows: Causal Amplification for Activation Space Attacks in Large Language Models](https://arxiv.org/abs/2511.17194v1)** | 2025-11-21 | <details><summary>31 pa...</summary><p>31 pages, 5 figures, 9 tables</p></details> |
| **[Model Inversion Attack Against Deep Hashing](https://arxiv.org/abs/2511.12233v2)** | 2025-11-21 |  |
| **[Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models](https://arxiv.org/abs/2507.05248v2)** | 2025-11-21 | <details><summary>20 pa...</summary><p>20 pages, 10 figures. Code and data available at https://github.com/Dtc7w3PQ/Response-Attack</p></details> |
| **[T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model](https://arxiv.org/abs/2510.22300v2)** | 2025-11-21 | AAAI 2026 |
| **[Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning](https://arxiv.org/abs/2503.17987v3)** | 2025-11-21 | <details><summary>Noted...</summary><p>Noted that This paper includes model-generated content that may contain offensive or distressing material</p></details> |
| **[MultiPriv: Benchmarking Individual-Level Privacy Reasoning in Vision-Language Models](https://arxiv.org/abs/2511.16940v1)** | 2025-11-21 |  |
| **[PEPPER: Perception-Guided Perturbation for Robust Backdoor Defense in Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.16830v1)** | 2025-11-20 |  |
| **[Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense](https://arxiv.org/abs/2511.16483v1)** | 2025-11-20 | <details><summary>Accep...</summary><p>Accepted in the AAAI-26 Workshop on Artificial Intelligence for Cyber Security (AICS)</p></details> |

