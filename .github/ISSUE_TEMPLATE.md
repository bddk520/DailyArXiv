---
title: Latest 15 Papers - May 28, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks](http://arxiv.org/abs/2505.13862v3)** | 2025-05-26 |  |
| **[Firewalls to Secure Dynamic LLM Agentic Networks](http://arxiv.org/abs/2502.01822v5)** | 2025-05-26 |  |
| **[What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs](http://arxiv.org/abs/2505.19773v1)** | 2025-05-26 | Accepted by ACL 2025 |
| **[Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](http://arxiv.org/abs/2505.15337v2)** | 2025-05-26 |  |
| **[Robo-Troj: Attacking LLM-based Task Planners](http://arxiv.org/abs/2504.17070v2)** | 2025-05-26 |  |
| **[One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs](http://arxiv.org/abs/2503.04856v2)** | 2025-05-26 |  |
| **[An Embarrassingly Simple Defense Against LLM Abliteration Attacks](http://arxiv.org/abs/2505.19056v1)** | 2025-05-25 | preprint |
| **[PII-Scope: A Comprehensive Study on Training Data PII Extraction Attacks in LLMs](http://arxiv.org/abs/2410.06704v2)** | 2025-05-25 | <details><summary>Addit...</summary><p>Additional results with Pythia6.9B; Additional results with Phone number PII;</p></details> |
| **[LLMs know their vulnerabilities: Uncover Safety Gaps through Natural Distribution Shifts](http://arxiv.org/abs/2410.10700v2)** | 2025-05-25 | <details><summary>ACL 2...</summary><p>ACL 2025 main conference. Code is available at https://github.com/AI45Lab/ActorAttack</p></details> |
| **[Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking](http://arxiv.org/abs/2504.05652v2)** | 2025-05-24 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks](http://arxiv.org/abs/2505.13862v3)** | 2025-05-26 |  |
| **[Firewalls to Secure Dynamic LLM Agentic Networks](http://arxiv.org/abs/2502.01822v5)** | 2025-05-26 |  |
| **[What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs](http://arxiv.org/abs/2505.19773v1)** | 2025-05-26 | Accepted by ACL 2025 |
| **[Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](http://arxiv.org/abs/2505.15337v2)** | 2025-05-26 |  |
| **[Robo-Troj: Attacking LLM-based Task Planners](http://arxiv.org/abs/2504.17070v2)** | 2025-05-26 |  |
| **[One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs](http://arxiv.org/abs/2503.04856v2)** | 2025-05-26 |  |
| **[An Embarrassingly Simple Defense Against LLM Abliteration Attacks](http://arxiv.org/abs/2505.19056v1)** | 2025-05-25 | preprint |
| **[PII-Scope: A Comprehensive Study on Training Data PII Extraction Attacks in LLMs](http://arxiv.org/abs/2410.06704v2)** | 2025-05-25 | <details><summary>Addit...</summary><p>Additional results with Pythia6.9B; Additional results with Phone number PII;</p></details> |
| **[LLMs know their vulnerabilities: Uncover Safety Gaps through Natural Distribution Shifts](http://arxiv.org/abs/2410.10700v2)** | 2025-05-25 | <details><summary>ACL 2...</summary><p>ACL 2025 main conference. Code is available at https://github.com/AI45Lab/ActorAttack</p></details> |
| **[Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking](http://arxiv.org/abs/2504.05652v2)** | 2025-05-24 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent](http://arxiv.org/abs/2505.20118v1)** | 2025-05-26 | 8 pages, 5 figures |
| **[Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](http://arxiv.org/abs/2504.05050v3)** | 2025-05-26 |  |
| **[Attention! You Vision Language Model Could Be Maliciously Manipulated](http://arxiv.org/abs/2505.19911v1)** | 2025-05-26 |  |
| **[CPA-RAG:Covert Poisoning Attacks on Retrieval-Augmented Generation in Large Language Models](http://arxiv.org/abs/2505.19864v1)** | 2025-05-26 |  |
| **[Jailbreak-AudioBench: In-Depth Evaluation and Analysis of Jailbreak Threats for Large Audio Language Models](http://arxiv.org/abs/2501.13772v2)** | 2025-05-26 |  |
| **[QueryAttack: Jailbreaking Aligned Large Language Models Using Structured Non-natural Query Language](http://arxiv.org/abs/2502.09723v3)** | 2025-05-26 | <details><summary>To ap...</summary><p>To appear in ACL 2025</p></details> |
| **[VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models](http://arxiv.org/abs/2505.19684v1)** | 2025-05-26 |  |
| **[Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](http://arxiv.org/abs/2505.15337v2)** | 2025-05-26 |  |
| **[Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models](http://arxiv.org/abs/2412.11041v3)** | 2025-05-26 | <details><summary>16 pa...</summary><p>16 pages, 14 figures. Camera-ready for ACL2025 findings</p></details> |
| **[JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models](http://arxiv.org/abs/2505.19610v1)** | 2025-05-26 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent](http://arxiv.org/abs/2505.20118v1)** | 2025-05-26 | 8 pages, 5 figures |
| **[Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](http://arxiv.org/abs/2504.05050v3)** | 2025-05-26 |  |
| **[Attention! You Vision Language Model Could Be Maliciously Manipulated](http://arxiv.org/abs/2505.19911v1)** | 2025-05-26 |  |
| **[CPA-RAG:Covert Poisoning Attacks on Retrieval-Augmented Generation in Large Language Models](http://arxiv.org/abs/2505.19864v1)** | 2025-05-26 |  |
| **[Jailbreak-AudioBench: In-Depth Evaluation and Analysis of Jailbreak Threats for Large Audio Language Models](http://arxiv.org/abs/2501.13772v2)** | 2025-05-26 |  |
| **[QueryAttack: Jailbreaking Aligned Large Language Models Using Structured Non-natural Query Language](http://arxiv.org/abs/2502.09723v3)** | 2025-05-26 | <details><summary>To ap...</summary><p>To appear in ACL 2025</p></details> |
| **[VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models](http://arxiv.org/abs/2505.19684v1)** | 2025-05-26 |  |
| **[Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](http://arxiv.org/abs/2505.15337v2)** | 2025-05-26 |  |
| **[Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models](http://arxiv.org/abs/2412.11041v3)** | 2025-05-26 | <details><summary>16 pa...</summary><p>16 pages, 14 figures. Camera-ready for ACL2025 findings</p></details> |
| **[JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models](http://arxiv.org/abs/2505.19610v1)** | 2025-05-26 |  |

