---
title: Latest 15 Papers - March 24, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Robust LLM safeguarding via refusal feature adversarial training](http://arxiv.org/abs/2409.20089v2)** | 2025-03-20 |  |
| **[Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection](http://arxiv.org/abs/2503.15552v1)** | 2025-03-18 |  |
| **[Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack](http://arxiv.org/abs/2503.15551v1)** | 2025-03-18 |  |
| **[A Framework to Assess Multilingual Vulnerabilities of LLMs](http://arxiv.org/abs/2503.13081v1)** | 2025-03-17 |  |
| **[TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning](http://arxiv.org/abs/2404.19597v3)** | 2025-03-17 | work in progress |
| **[Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via Benign Relearning](http://arxiv.org/abs/2406.13356v4)** | 2025-03-17 | <details><summary>ICLR ...</summary><p>ICLR 2025, 32 pages, 8 figures, 9 tables</p></details> |
| **[Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents](http://arxiv.org/abs/2503.15547v1)** | 2025-03-17 |  |
| **[When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers](http://arxiv.org/abs/2402.10601v3)** | 2025-03-16 |  |
| **[JailGuard: A Universal Detection Framework for LLM Prompt-based Attacks](http://arxiv.org/abs/2312.10766v4)** | 2025-03-15 | 40 pages, 12 figures |
| **[The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions](http://arxiv.org/abs/2406.12480v2)** | 2025-03-12 | ICLR 2025 Spotlight |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Robust LLM safeguarding via refusal feature adversarial training](http://arxiv.org/abs/2409.20089v2)** | 2025-03-20 |  |
| **[Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection](http://arxiv.org/abs/2503.15552v1)** | 2025-03-18 |  |
| **[Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack](http://arxiv.org/abs/2503.15551v1)** | 2025-03-18 |  |
| **[A Framework to Assess Multilingual Vulnerabilities of LLMs](http://arxiv.org/abs/2503.13081v1)** | 2025-03-17 |  |
| **[TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning](http://arxiv.org/abs/2404.19597v3)** | 2025-03-17 | work in progress |
| **[Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via Benign Relearning](http://arxiv.org/abs/2406.13356v4)** | 2025-03-17 | <details><summary>ICLR ...</summary><p>ICLR 2025, 32 pages, 8 figures, 9 tables</p></details> |
| **[Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents](http://arxiv.org/abs/2503.15547v1)** | 2025-03-17 |  |
| **[When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers](http://arxiv.org/abs/2402.10601v3)** | 2025-03-16 |  |
| **[Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models via Watermarking](http://arxiv.org/abs/2503.04636v2)** | 2025-03-15 | <details><summary>Accep...</summary><p>Accepted by the ICLR 2025 Workshop on GenAI Watermarking</p></details> |
| **[JailGuard: A Universal Detection Framework for LLM Prompt-based Attacks](http://arxiv.org/abs/2312.10766v4)** | 2025-03-15 | 40 pages, 12 figures |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[From Head to Tail: Efficient Black-box Model Inversion Attack via Long-tailed Learning](http://arxiv.org/abs/2503.16266v1)** | 2025-03-20 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025</p></details> |
| **["Moralized" Multi-Step Jailbreak Prompts: Black-Box Testing of Guardrails in Large Language Models for Verbal Attacks](http://arxiv.org/abs/2411.16730v4)** | 2025-03-20 | <details><summary>This ...</summary><p>This paper has been submitted to Nature Machine Intelligence and OpenReview preprints. It has 7 pages of text, 3 figures, and 3 tables</p></details> |
| **[Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning](http://arxiv.org/abs/2412.20392v2)** | 2025-03-20 |  |
| **[BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models](http://arxiv.org/abs/2503.16023v1)** | 2025-03-20 | <details><summary>This ...</summary><p>This paper is accepted by CVPR 2025</p></details> |
| **[Differentially Private Steering for Large Language Model Alignment](http://arxiv.org/abs/2501.18532v2)** | 2025-03-20 | <details><summary>ICLR ...</summary><p>ICLR 2025 Camera Ready; Code: https://github.com/UKPLab/iclr2025-psa</p></details> |
| **[SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders](http://arxiv.org/abs/2503.14530v2)** | 2025-03-20 | <details><summary>More ...</summary><p>More comparative experiments are needed</p></details> |
| **[Undesirable Memorization in Large Language Models: A Survey](http://arxiv.org/abs/2410.02650v2)** | 2025-03-19 |  |
| **[Safety at Scale: A Comprehensive Survey of Large Model Safety](http://arxiv.org/abs/2502.05206v3)** | 2025-03-19 | <details><summary>47 pa...</summary><p>47 pages, 3 figures, 11 tables; GitHub: https://github.com/xingjunm/Awesome-Large-Model-Safety</p></details> |
| **[Test-Time Backdoor Detection for Object Detection Models](http://arxiv.org/abs/2503.15293v1)** | 2025-03-19 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[From Head to Tail: Efficient Black-box Model Inversion Attack via Long-tailed Learning](http://arxiv.org/abs/2503.16266v1)** | 2025-03-20 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025</p></details> |
| **["Moralized" Multi-Step Jailbreak Prompts: Black-Box Testing of Guardrails in Large Language Models for Verbal Attacks](http://arxiv.org/abs/2411.16730v4)** | 2025-03-20 | <details><summary>This ...</summary><p>This paper has been submitted to Nature Machine Intelligence and OpenReview preprints. It has 7 pages of text, 3 figures, and 3 tables</p></details> |
| **[Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning](http://arxiv.org/abs/2412.20392v2)** | 2025-03-20 |  |
| **[BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models](http://arxiv.org/abs/2503.16023v1)** | 2025-03-20 | <details><summary>This ...</summary><p>This paper is accepted by CVPR 2025</p></details> |
| **[Differentially Private Steering for Large Language Model Alignment](http://arxiv.org/abs/2501.18532v2)** | 2025-03-20 | <details><summary>ICLR ...</summary><p>ICLR 2025 Camera Ready; Code: https://github.com/UKPLab/iclr2025-psa</p></details> |
| **[SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders](http://arxiv.org/abs/2503.14530v2)** | 2025-03-20 | <details><summary>More ...</summary><p>More comparative experiments are needed</p></details> |
| **[Undesirable Memorization in Large Language Models: A Survey](http://arxiv.org/abs/2410.02650v2)** | 2025-03-19 |  |
| **[Safety at Scale: A Comprehensive Survey of Large Model Safety](http://arxiv.org/abs/2502.05206v3)** | 2025-03-19 | <details><summary>47 pa...</summary><p>47 pages, 3 figures, 11 tables; GitHub: https://github.com/xingjunm/Awesome-Large-Model-Safety</p></details> |
| **[Test-Time Backdoor Detection for Object Detection Models](http://arxiv.org/abs/2503.15293v1)** | 2025-03-19 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025</p></details> |

