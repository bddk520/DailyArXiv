---
title: Latest 15 Papers - April 23, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[aiXamine: LLM Safety and Security Simplified](http://arxiv.org/abs/2504.14985v1)** | 2025-04-21 |  |
| **[MCGMark: An Encodable and Robust Online Watermark for Tracing LLM-Generated Malicious Code](http://arxiv.org/abs/2408.01354v2)** | 2025-04-21 |  |
| **[Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents](http://arxiv.org/abs/2503.15547v2)** | 2025-04-21 |  |
| **[LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks](http://arxiv.org/abs/2504.14556v1)** | 2025-04-20 | 8 pages, 7 figures, |
| **[Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning](http://arxiv.org/abs/2503.17987v2)** | 2025-04-19 | <details><summary>This ...</summary><p>This paper includes model-generated content that may contain offensive or distressing material</p></details> |
| **[Detecting Malicious Source Code in PyPI Packages with LLMs: Does RAG Come in Handy?](http://arxiv.org/abs/2504.13769v1)** | 2025-04-18 | <details><summary>The p...</summary><p>The paper has been peer-reviewed and accepted for publication to the 29th International Conference on Evaluation and Assessment in Software Engineering (EASE 2025)</p></details> |
| **[AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents](http://arxiv.org/abs/2410.09024v3)** | 2025-04-18 | <details><summary>Accep...</summary><p>Accepted at ICLR 2025</p></details> |
| **[DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification](http://arxiv.org/abs/2504.13562v1)** | 2025-04-18 |  |
| **[Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](http://arxiv.org/abs/2404.02151v4)** | 2025-04-17 | <details><summary>Accep...</summary><p>Accepted at ICLR 2025. Updates in the v3: GPT-4o and Claude 3.5 Sonnet results, improved writing. Updates in the v2: more models (Llama3, Phi-3, Nemotron-4-340B), jailbreak artifacts for all attacks are available, evaluation with different judges (Llama-3-70B and Llama Guard 2), more experiments (convergence plots, ablation on the suffix length for random search), examples of jailbroken generation</p></details> |
| **[Does Refusal Training in LLMs Generalize to the Past Tense?](http://arxiv.org/abs/2407.11969v4)** | 2025-04-17 | <details><summary>Accep...</summary><p>Accepted at ICLR 2025. Updates in v2 and v3: added GPT-4o, Claude 3.5 Sonnet, o1-mini, and o1-preview results. Code and jailbreak artifacts: https://github.com/tml-epfl/llm-past-tense</p></details> |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[aiXamine: LLM Safety and Security Simplified](http://arxiv.org/abs/2504.14985v1)** | 2025-04-21 |  |
| **[MCGMark: An Encodable and Robust Online Watermark for Tracing LLM-Generated Malicious Code](http://arxiv.org/abs/2408.01354v2)** | 2025-04-21 |  |
| **[Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents](http://arxiv.org/abs/2503.15547v2)** | 2025-04-21 |  |
| **[LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks](http://arxiv.org/abs/2504.14556v1)** | 2025-04-20 | 8 pages, 7 figures, |
| **[Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning](http://arxiv.org/abs/2503.17987v2)** | 2025-04-19 | <details><summary>This ...</summary><p>This paper includes model-generated content that may contain offensive or distressing material</p></details> |
| **[Detecting Malicious Source Code in PyPI Packages with LLMs: Does RAG Come in Handy?](http://arxiv.org/abs/2504.13769v1)** | 2025-04-18 | <details><summary>The p...</summary><p>The paper has been peer-reviewed and accepted for publication to the 29th International Conference on Evaluation and Assessment in Software Engineering (EASE 2025)</p></details> |
| **[AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents](http://arxiv.org/abs/2410.09024v3)** | 2025-04-18 | <details><summary>Accep...</summary><p>Accepted at ICLR 2025</p></details> |
| **[DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification](http://arxiv.org/abs/2504.13562v1)** | 2025-04-18 |  |
| **[Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](http://arxiv.org/abs/2404.02151v4)** | 2025-04-17 | <details><summary>Accep...</summary><p>Accepted at ICLR 2025. Updates in the v3: GPT-4o and Claude 3.5 Sonnet results, improved writing. Updates in the v2: more models (Llama3, Phi-3, Nemotron-4-340B), jailbreak artifacts for all attacks are available, evaluation with different judges (Llama-3-70B and Llama Guard 2), more experiments (convergence plots, ablation on the suffix length for random search), examples of jailbroken generation</p></details> |
| **[Does Refusal Training in LLMs Generalize to the Past Tense?](http://arxiv.org/abs/2407.11969v4)** | 2025-04-17 | <details><summary>Accep...</summary><p>Accepted at ICLR 2025. Updates in v2 and v3: added GPT-4o, Claude 3.5 Sonnet, o1-mini, and o1-preview results. Code and jailbreak artifacts: https://github.com/tml-epfl/llm-past-tense</p></details> |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[ASIDE: Architectural Separation of Instructions and Data in Language Models](http://arxiv.org/abs/2503.10566v2)** | 2025-04-21 | <details><summary>ICLR ...</summary><p>ICLR 2025 Workshop on Building Trust in Language Models and Applications</p></details> |
| **[HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](http://arxiv.org/abs/2502.14744v3)** | 2025-04-21 |  |
| **[Scalable Discrete Event Simulation Tool for Large-Scale Cyber-Physical Energy Systems: Advancing System Efficiency and Scalability](http://arxiv.org/abs/2504.15198v1)** | 2025-04-21 |  |
| **[Gaussian Shading++: Rethinking the Realistic Deployment Challenge of Performance-Lossless Image Watermark for Diffusion Models](http://arxiv.org/abs/2504.15026v1)** | 2025-04-21 | 18 pages, 8 figures |
| **[Transferable Adversarial Attacks on SAM and Its Downstream Models](http://arxiv.org/abs/2410.20197v3)** | 2025-04-21 | update fig 1 |
| **[Risks of Practicing Large Language Models in Smart Grid: Threat Modeling and Validation](http://arxiv.org/abs/2405.06237v3)** | 2025-04-21 |  |
| **[BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models](http://arxiv.org/abs/2504.13775v2)** | 2025-04-21 | 16 pages, 6 figures |
| **[Detecting Training Data of Large Language Models via Expectation Maximization](http://arxiv.org/abs/2410.07582v2)** | 2025-04-21 | 15 pages |
| **[Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models](http://arxiv.org/abs/2504.14798v1)** | 2025-04-21 |  |
| **[The last Dance : Robust backdoor attack via diffusion models and bayesian approach](http://arxiv.org/abs/2402.05967v7)** | 2025-04-20 | <details><summary>Prepr...</summary><p>Preprint (Last update, will never be modified again( correction of a sketch)): audio backdoor attack on Hugging Face's Transformer pre-trained models. This attack incorporates state-of-the-art Bayesian techniques, a modified Fokker-Planck equation (via Yang-Mills), and a diffusion model approach</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[ASIDE: Architectural Separation of Instructions and Data in Language Models](http://arxiv.org/abs/2503.10566v2)** | 2025-04-21 | <details><summary>ICLR ...</summary><p>ICLR 2025 Workshop on Building Trust in Language Models and Applications</p></details> |
| **[HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](http://arxiv.org/abs/2502.14744v3)** | 2025-04-21 |  |
| **[Scalable Discrete Event Simulation Tool for Large-Scale Cyber-Physical Energy Systems: Advancing System Efficiency and Scalability](http://arxiv.org/abs/2504.15198v1)** | 2025-04-21 |  |
| **[Gaussian Shading++: Rethinking the Realistic Deployment Challenge of Performance-Lossless Image Watermark for Diffusion Models](http://arxiv.org/abs/2504.15026v1)** | 2025-04-21 | 18 pages, 8 figures |
| **[Transferable Adversarial Attacks on SAM and Its Downstream Models](http://arxiv.org/abs/2410.20197v3)** | 2025-04-21 | update fig 1 |
| **[Risks of Practicing Large Language Models in Smart Grid: Threat Modeling and Validation](http://arxiv.org/abs/2405.06237v3)** | 2025-04-21 |  |
| **[BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models](http://arxiv.org/abs/2504.13775v2)** | 2025-04-21 | 16 pages, 6 figures |
| **[Detecting Training Data of Large Language Models via Expectation Maximization](http://arxiv.org/abs/2410.07582v2)** | 2025-04-21 | 15 pages |
| **[Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models](http://arxiv.org/abs/2504.14798v1)** | 2025-04-21 |  |
| **[The last Dance : Robust backdoor attack via diffusion models and bayesian approach](http://arxiv.org/abs/2402.05967v7)** | 2025-04-20 | <details><summary>Prepr...</summary><p>Preprint (Last update, will never be modified again( correction of a sketch)): audio backdoor attack on Hugging Face's Transformer pre-trained models. This attack incorporates state-of-the-art Bayesian techniques, a modified Fokker-Planck equation (via Yang-Mills), and a diffusion model approach</p></details> |

