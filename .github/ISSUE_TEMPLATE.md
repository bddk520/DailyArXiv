---
title: Latest 15 Papers - February 05, 2026
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation](https://arxiv.org/abs/2512.10415v2)** | 2026-02-03 | <details><summary>This ...</summary><p>This manuscript has been withdrawn by the authors because the methodology and results have been superseded by a more rigorous framework (SPACI and AST-ASIP). The corrected and expanded findings are now available in arXiv:2601.21360. Please cite the new manuscript instead</p></details> |
| **[OverThink: Slowdown Attacks on Reasoning LLMs](https://arxiv.org/abs/2502.02542v3)** | 2026-02-03 |  |
| **[Proactive defense against LLM Jailbreak](https://arxiv.org/abs/2510.05052v2)** | 2026-02-02 |  |
| **[STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents](https://arxiv.org/abs/2509.25624v2)** | 2026-02-02 |  |
| **[RACA: Representation-Aware Coverage Criteria for LLM Safety Testing](https://arxiv.org/abs/2602.02280v1)** | 2026-02-02 |  |
| **[Code-Mixed Phonetic Perturbations for Red-Teaming LLMs](https://arxiv.org/abs/2505.14226v4)** | 2026-02-02 |  |
| **[Expected Harm: Rethinking Safety Evaluation of (Mis)Aligned LLMs](https://arxiv.org/abs/2602.01600v1)** | 2026-02-02 |  |
| **[Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment](https://arxiv.org/abs/2602.01587v1)** | 2026-02-02 | 10 pages |
| **[MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539v1)** | 2026-02-02 |  |
| **[MAS-Shield: A Defense Framework for Secure and Efficient LLM MAS](https://arxiv.org/abs/2511.22924v2)** | 2026-02-02 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation](https://arxiv.org/abs/2512.10415v2)** | 2026-02-03 | <details><summary>This ...</summary><p>This manuscript has been withdrawn by the authors because the methodology and results have been superseded by a more rigorous framework (SPACI and AST-ASIP). The corrected and expanded findings are now available in arXiv:2601.21360. Please cite the new manuscript instead</p></details> |
| **[OverThink: Slowdown Attacks on Reasoning LLMs](https://arxiv.org/abs/2502.02542v3)** | 2026-02-03 |  |
| **[The Trigger in the Haystack: Extracting and Reconstructing LLM Backdoor Triggers](https://arxiv.org/abs/2602.03085v1)** | 2026-02-03 |  |
| **[Proactive defense against LLM Jailbreak](https://arxiv.org/abs/2510.05052v2)** | 2026-02-02 |  |
| **[STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents](https://arxiv.org/abs/2509.25624v2)** | 2026-02-02 |  |
| **[RACA: Representation-Aware Coverage Criteria for LLM Safety Testing](https://arxiv.org/abs/2602.02280v1)** | 2026-02-02 |  |
| **[Code-Mixed Phonetic Perturbations for Red-Teaming LLMs](https://arxiv.org/abs/2505.14226v4)** | 2026-02-02 |  |
| **[Expected Harm: Rethinking Safety Evaluation of (Mis)Aligned LLMs](https://arxiv.org/abs/2602.01600v1)** | 2026-02-02 |  |
| **[Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment](https://arxiv.org/abs/2602.01587v1)** | 2026-02-02 | 10 pages |
| **[MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539v1)** | 2026-02-02 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models](https://arxiv.org/abs/2501.18533v3)** | 2026-02-03 |  |
| **[Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility](https://arxiv.org/abs/2602.03402v1)** | 2026-02-03 |  |
| **[A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models](https://arxiv.org/abs/2509.23286v2)** | 2026-02-03 | <details><summary>Accep...</summary><p>Accepted at ICLR 2026. Code and models are available at https://ai-isl.github.io/A2D</p></details> |
| **[Beyond Suffixes: Token Position in GCG Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2602.03265v1)** | 2026-02-03 | 12 pages, 10 figures |
| **[Monotonicity as an Architectural Bias for Robust Language Models](https://arxiv.org/abs/2602.02686v1)** | 2026-02-02 | 12 pages, 1 figure |
| **[Benchmarking Large Language Models for Zero-shot and Few-shot Phishing URL Detection](https://arxiv.org/abs/2602.02641v1)** | 2026-02-02 | <details><summary>9 pag...</summary><p>9 pages, accepted at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025 LAW Workshop)</p></details> |
| **[AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models](https://arxiv.org/abs/2505.14103v3)** | 2026-02-02 | <details><summary>Accep...</summary><p>Accepted by IEEE Transactions on Dependable and Secure Computing (TDSC)</p></details> |
| **[MalCVE: Malware Detection and CVE Association Using Large Language Models](https://arxiv.org/abs/2510.15567v2)** | 2026-02-02 |  |
| **[Learning Better Certified Models from Empirically-Robust Teachers](https://arxiv.org/abs/2602.02626v1)** | 2026-02-02 |  |
| **[Alignment-Aware Model Adaptation via Feedback-Guided Optimization](https://arxiv.org/abs/2602.02258v1)** | 2026-02-02 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models](https://arxiv.org/abs/2501.18533v3)** | 2026-02-03 |  |
| **[Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility](https://arxiv.org/abs/2602.03402v1)** | 2026-02-03 |  |
| **[A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models](https://arxiv.org/abs/2509.23286v2)** | 2026-02-03 | <details><summary>Accep...</summary><p>Accepted at ICLR 2026. Code and models are available at https://ai-isl.github.io/A2D</p></details> |
| **[Beyond Suffixes: Token Position in GCG Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2602.03265v1)** | 2026-02-03 | 12 pages, 10 figures |
| **[Monotonicity as an Architectural Bias for Robust Language Models](https://arxiv.org/abs/2602.02686v1)** | 2026-02-02 | 12 pages, 1 figure |
| **[Benchmarking Large Language Models for Zero-shot and Few-shot Phishing URL Detection](https://arxiv.org/abs/2602.02641v1)** | 2026-02-02 | <details><summary>9 pag...</summary><p>9 pages, accepted at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025 LAW Workshop)</p></details> |
| **[AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models](https://arxiv.org/abs/2505.14103v3)** | 2026-02-02 | <details><summary>Accep...</summary><p>Accepted by IEEE Transactions on Dependable and Secure Computing (TDSC)</p></details> |
| **[MalCVE: Malware Detection and CVE Association Using Large Language Models](https://arxiv.org/abs/2510.15567v2)** | 2026-02-02 |  |
| **[Learning Better Certified Models from Empirically-Robust Teachers](https://arxiv.org/abs/2602.02626v1)** | 2026-02-02 |  |
| **[Alignment-Aware Model Adaptation via Feedback-Guided Optimization](https://arxiv.org/abs/2602.02258v1)** | 2026-02-02 |  |

