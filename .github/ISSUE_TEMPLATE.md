---
title: Latest 15 Papers - March 11, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How to Fix It)](http://arxiv.org/abs/2406.17975v3)** | 2025-03-07 | <details><summary>IEEE ...</summary><p>IEEE Conference on Secure and Trustworthy Machine Learning (SaTML 2025)</p></details> |
| **[Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection via Backdoor Attacks](http://arxiv.org/abs/2503.05445v1)** | 2025-03-07 |  |
| **[DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios](http://arxiv.org/abs/2410.23746v2)** | 2025-03-07 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Camera-Ready)</p></details> |
| **[A Practical Memory Injection Attack against LLM Agents](http://arxiv.org/abs/2503.03704v2)** | 2025-03-07 |  |
| **[Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety](http://arxiv.org/abs/2503.05021v1)** | 2025-03-06 |  |
| **[Get my drift? Catching LLM Task Drift with Activation Deltas](http://arxiv.org/abs/2406.00799v6)** | 2025-03-06 | SaTML 2025 |
| **[Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges](http://arxiv.org/abs/2503.04474v1)** | 2025-03-06 | <details><summary>Accep...</summary><p>Accepted to the ICBINB Workshop at ICLR'25</p></details> |
| **[Exploring the Multilingual NLG Evaluation Abilities of LLM-Based Evaluators](http://arxiv.org/abs/2503.04360v1)** | 2025-03-06 |  |
| **[Malware Detection at the Edge with Lightweight LLMs: A Performance Evaluation](http://arxiv.org/abs/2503.04302v1)** | 2025-03-06 |  |
| **[One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs](http://arxiv.org/abs/2503.04856v1)** | 2025-03-06 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How to Fix It)](http://arxiv.org/abs/2406.17975v3)** | 2025-03-07 | <details><summary>IEEE ...</summary><p>IEEE Conference on Secure and Trustworthy Machine Learning (SaTML 2025)</p></details> |
| **[Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection via Backdoor Attacks](http://arxiv.org/abs/2503.05445v1)** | 2025-03-07 |  |
| **[DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios](http://arxiv.org/abs/2410.23746v2)** | 2025-03-07 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Camera-Ready)</p></details> |
| **[A Practical Memory Injection Attack against LLM Agents](http://arxiv.org/abs/2503.03704v2)** | 2025-03-07 |  |
| **[Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety](http://arxiv.org/abs/2503.05021v1)** | 2025-03-06 |  |
| **[Get my drift? Catching LLM Task Drift with Activation Deltas](http://arxiv.org/abs/2406.00799v6)** | 2025-03-06 | SaTML 2025 |
| **[Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models via Watermarking](http://arxiv.org/abs/2503.04636v1)** | 2025-03-06 | <details><summary>Accep...</summary><p>Accepted by the 1st Workshop on GenAI Watermarking, collocated with ICLR 2025</p></details> |
| **[Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges](http://arxiv.org/abs/2503.04474v1)** | 2025-03-06 | <details><summary>Accep...</summary><p>Accepted to the ICBINB Workshop at ICLR'25</p></details> |
| **[Exploring the Multilingual NLG Evaluation Abilities of LLM-Based Evaluators](http://arxiv.org/abs/2503.04360v1)** | 2025-03-06 |  |
| **[Malware Detection at the Edge with Lightweight LLMs: A Performance Evaluation](http://arxiv.org/abs/2503.04302v1)** | 2025-03-06 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset](http://arxiv.org/abs/2411.03554v3)** | 2025-03-07 |  |
| **[Membership Inference Attacks Cannot Prove that a Model Was Trained On Your Data](http://arxiv.org/abs/2409.19798v2)** | 2025-03-07 | <details><summary>posit...</summary><p>position paper at IEEE SaTML 2025</p></details> |
| **[Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection via Backdoor Attacks](http://arxiv.org/abs/2503.05445v1)** | 2025-03-07 |  |
| **[Double Backdoored: Converting Code Large Language Model Backdoors to Traditional Malware via Adversarial Instruction Tuning Attacks](http://arxiv.org/abs/2404.18567v2)** | 2025-03-07 |  |
| **[Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring](http://arxiv.org/abs/2410.21083v2)** | 2025-03-06 | <details><summary>Accep...</summary><p>Accepted by NAACL 2025</p></details> |
| **[UniNet: A Unified Multi-granular Traffic Modeling Framework for Network Security](http://arxiv.org/abs/2503.04174v1)** | 2025-03-06 | <details><summary>21 pa...</summary><p>21 pages, 6 figures,15 tables</p></details> |
| **[How Breakable Is Privacy: Probing and Resisting Model Inversion Attacks in Collaborative Inference](http://arxiv.org/abs/2501.00824v3)** | 2025-03-06 | <details><summary>14 pa...</summary><p>14 pages, 5 figures, 7 tables. The experiment is still being supplemented. V3 modified the writing and supplemented the experiment</p></details> |
| **[Task-Agnostic Attacks Against Vision Foundation Models](http://arxiv.org/abs/2503.03842v1)** | 2025-03-05 |  |
| **[LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on Large Language Models](http://arxiv.org/abs/2407.16205v5)** | 2025-03-05 |  |
| **[Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models](http://arxiv.org/abs/2503.01742v2)** | 2025-03-05 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset](http://arxiv.org/abs/2411.03554v3)** | 2025-03-07 |  |
| **[Membership Inference Attacks Cannot Prove that a Model Was Trained On Your Data](http://arxiv.org/abs/2409.19798v2)** | 2025-03-07 | <details><summary>posit...</summary><p>position paper at IEEE SaTML 2025</p></details> |
| **[Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection via Backdoor Attacks](http://arxiv.org/abs/2503.05445v1)** | 2025-03-07 |  |
| **[Double Backdoored: Converting Code Large Language Model Backdoors to Traditional Malware via Adversarial Instruction Tuning Attacks](http://arxiv.org/abs/2404.18567v2)** | 2025-03-07 |  |
| **[Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models via Watermarking](http://arxiv.org/abs/2503.04636v1)** | 2025-03-06 | <details><summary>Accep...</summary><p>Accepted by the 1st Workshop on GenAI Watermarking, collocated with ICLR 2025</p></details> |
| **[Activation Space Interventions Can Be Transferred Between Large Language Models](http://arxiv.org/abs/2503.04429v1)** | 2025-03-06 | 68 pages |
| **[Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring](http://arxiv.org/abs/2410.21083v2)** | 2025-03-06 | <details><summary>Accep...</summary><p>Accepted by NAACL 2025</p></details> |
| **[UniNet: A Unified Multi-granular Traffic Modeling Framework for Network Security](http://arxiv.org/abs/2503.04174v1)** | 2025-03-06 | <details><summary>21 pa...</summary><p>21 pages, 6 figures,15 tables</p></details> |
| **[How Breakable Is Privacy: Probing and Resisting Model Inversion Attacks in Collaborative Inference](http://arxiv.org/abs/2501.00824v3)** | 2025-03-06 | <details><summary>14 pa...</summary><p>14 pages, 5 figures, 7 tables. The experiment is still being supplemented. V3 modified the writing and supplemented the experiment</p></details> |
| **[Task-Agnostic Attacks Against Vision Foundation Models](http://arxiv.org/abs/2503.03842v1)** | 2025-03-05 |  |

