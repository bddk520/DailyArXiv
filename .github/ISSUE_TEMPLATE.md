---
title: Latest 15 Papers - October 22, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs](http://arxiv.org/abs/2508.00161v2)** | 2025-10-20 |  |
| **[OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph Anomaly Detection and LLMs](http://arxiv.org/abs/2510.15188v2)** | 2025-10-20 | <details><summary>This ...</summary><p>This is the authors' extended version of the paper accepted for publication at the ACM SIGSAC Conference on Computer and Communications Security (CCS 2025). The final published version is available at https://doi.org/10.1145/3719027.3765219</p></details> |
| **[Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](http://arxiv.org/abs/2510.17021v1)** | 2025-10-19 |  |
| **[Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs](http://arxiv.org/abs/2510.17000v1)** | 2025-10-19 | <details><summary>NeurI...</summary><p>NeurIPS 2025 (spotlight)</p></details> |
| **[Black-box Optimization of LLM Outputs by Asking for Directions](http://arxiv.org/abs/2510.16794v1)** | 2025-10-19 |  |
| **[DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge](http://arxiv.org/abs/2510.16716v1)** | 2025-10-19 |  |
| **[Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs](http://arxiv.org/abs/2510.02833v3)** | 2025-10-19 | <details><summary>Publi...</summary><p>Published as a conference paper at Neurips 2025</p></details> |
| **[Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning](http://arxiv.org/abs/2410.07163v4)** | 2025-10-17 | <details><summary>Accep...</summary><p>Accepted by NeurIPS 2025</p></details> |
| **[PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features](http://arxiv.org/abs/2510.14005v2)** | 2025-10-17 | <details><summary>The c...</summary><p>The code is available at https://github.com/weizou52/PIShield</p></details> |
| **[Active Honeypot Guardrail System: Probing and Confirming Multi-Turn LLM Jailbreaks](http://arxiv.org/abs/2510.15017v1)** | 2025-10-16 | 6pages, 2 figures |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs](http://arxiv.org/abs/2508.00161v2)** | 2025-10-20 |  |
| **[OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph Anomaly Detection and LLMs](http://arxiv.org/abs/2510.15188v2)** | 2025-10-20 | <details><summary>This ...</summary><p>This is the authors' extended version of the paper accepted for publication at the ACM SIGSAC Conference on Computer and Communications Security (CCS 2025). The final published version is available at https://doi.org/10.1145/3719027.3765219</p></details> |
| **[Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](http://arxiv.org/abs/2510.17021v1)** | 2025-10-19 |  |
| **[Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs](http://arxiv.org/abs/2510.17000v1)** | 2025-10-19 | <details><summary>NeurI...</summary><p>NeurIPS 2025 (spotlight)</p></details> |
| **[Black-box Optimization of LLM Outputs by Asking for Directions](http://arxiv.org/abs/2510.16794v1)** | 2025-10-19 |  |
| **[DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge](http://arxiv.org/abs/2510.16716v1)** | 2025-10-19 |  |
| **[Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs](http://arxiv.org/abs/2510.02833v3)** | 2025-10-19 | <details><summary>Publi...</summary><p>Published as a conference paper at Neurips 2025</p></details> |
| **[Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning](http://arxiv.org/abs/2410.07163v4)** | 2025-10-17 | <details><summary>Accep...</summary><p>Accepted by NeurIPS 2025</p></details> |
| **[PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features](http://arxiv.org/abs/2510.14005v2)** | 2025-10-17 | <details><summary>The c...</summary><p>The code is available at https://github.com/weizou52/PIShield</p></details> |
| **[Active Honeypot Guardrail System: Probing and Confirming Multi-Turn LLM Jailbreaks](http://arxiv.org/abs/2510.15017v1)** | 2025-10-16 | 6pages, 2 figures |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](http://arxiv.org/abs/2510.17759v1)** | 2025-10-20 | 18 pages, 7 Figures, |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](http://arxiv.org/abs/2510.17759v1)** | 2025-10-20 | 18 pages, 7 Figures, |
| **[GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models](http://arxiv.org/abs/2510.17621v1)** | 2025-10-20 |  |
| **[Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models](http://arxiv.org/abs/2510.15430v2)** | 2025-10-20 | <details><summary>Withd...</summary><p>Withdrawn due to an accidental duplicate submission. This paper (arXiv:2510.15430) was unintentionally submitted as a new entry instead of a new version of our previous work (arXiv:2508.09201)</p></details> |
| **[Semantic Representation Attack against Aligned Large Language Models](http://arxiv.org/abs/2509.19360v2)** | 2025-10-20 |  |
| **[Membership Inference Attack Should Move On to Distributional Statistics for Distilled Generative Models](http://arxiv.org/abs/2502.02970v4)** | 2025-10-20 |  |
| **[ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning](http://arxiv.org/abs/2507.11500v2)** | 2025-10-20 |  |
| **[Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models](http://arxiv.org/abs/2510.17098v1)** | 2025-10-20 |  |
| **[System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection](http://arxiv.org/abs/2505.06493v3)** | 2025-10-19 |  |
| **[DP-Fusion: Token-Level Differentially Private Inference for Large Language Models](http://arxiv.org/abs/2507.04531v2)** | 2025-10-19 | <details><summary>Our c...</summary><p>Our code and data are publicly available here: https://github.com/MBZUAI-Trustworthy-ML/DP-Fusion-DPI</p></details> |

