---
title: Latest 15 Papers - February 13, 2026
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Vulnerabilities in Partial TEE-Shielded LLM Inference with Precomputed Noise](https://arxiv.org/abs/2602.11088v1)** | 2026-02-11 |  |
| **[LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks](https://arxiv.org/abs/2310.11409v7)** | 2026-02-11 |  |
| **[When Speculation Spills Secrets: Side Channels via Speculative Decoding In LLMs](https://arxiv.org/abs/2411.01076v4)** | 2026-02-11 |  |
| **[The Landscape of Prompt Injection Threats in LLM Agents: From Taxonomy to Analysis](https://arxiv.org/abs/2602.10453v1)** | 2026-02-11 |  |
| **[Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard](https://arxiv.org/abs/2511.10222v3)** | 2026-02-10 |  |
| **[CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment](https://arxiv.org/abs/2602.08023v2)** | 2026-02-10 |  |
| **[Stop Testing Attacks, Start Diagnosing Defenses: The Four-Checkpoint Framework Reveals Where LLM Safety Breaks](https://arxiv.org/abs/2602.09629v1)** | 2026-02-10 | 17 pages, pre-print |
| **[The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097v2)** | 2026-02-10 | <details><summary>Accep...</summary><p>Accepted by ICLR 2026</p></details> |
| **[Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs](https://arxiv.org/abs/2501.16534v4)** | 2026-02-09 | <details><summary>Accep...</summary><p>Accepted to the 4th IEEE Conference on Secure and Trustworthy Machine Learning (SaTML'26)</p></details> |
| **[SearchAttack: Red-Teaming LLMs against Knowledge-to-Action Threats under Online Web Search](https://arxiv.org/abs/2601.04093v2)** | 2026-02-09 | <details><summary>Misus...</summary><p>Misusing LLM-driven search for harmful information-seeking poses serious risks. We characterize its usability and impact through a comprehensive red-teaming and evaluation</p></details> |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Vulnerabilities in Partial TEE-Shielded LLM Inference with Precomputed Noise](https://arxiv.org/abs/2602.11088v1)** | 2026-02-11 |  |
| **[LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks](https://arxiv.org/abs/2310.11409v7)** | 2026-02-11 |  |
| **[When Speculation Spills Secrets: Side Channels via Speculative Decoding In LLMs](https://arxiv.org/abs/2411.01076v4)** | 2026-02-11 |  |
| **[The Landscape of Prompt Injection Threats in LLM Agents: From Taxonomy to Analysis](https://arxiv.org/abs/2602.10453v1)** | 2026-02-11 |  |
| **[Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard](https://arxiv.org/abs/2511.10222v3)** | 2026-02-10 |  |
| **[CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment](https://arxiv.org/abs/2602.08023v2)** | 2026-02-10 |  |
| **[Stop Testing Attacks, Start Diagnosing Defenses: The Four-Checkpoint Framework Reveals Where LLM Safety Breaks](https://arxiv.org/abs/2602.09629v1)** | 2026-02-10 | 17 pages, pre-print |
| **[The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097v2)** | 2026-02-10 | <details><summary>Accep...</summary><p>Accepted by ICLR 2026</p></details> |
| **[Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs](https://arxiv.org/abs/2501.16534v4)** | 2026-02-09 | <details><summary>Accep...</summary><p>Accepted to the 4th IEEE Conference on Secure and Trustworthy Machine Learning (SaTML'26)</p></details> |
| **[SearchAttack: Red-Teaming LLMs against Knowledge-to-Action Threats under Online Web Search](https://arxiv.org/abs/2601.04093v2)** | 2026-02-09 | <details><summary>Misus...</summary><p>Misusing LLM-driven search for harmful information-seeking poses serious risks. We characterize its usability and impact through a comprehensive red-teaming and evaluation</p></details> |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away](https://arxiv.org/abs/2602.11096v1)** | 2026-02-11 |  |
| **[Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility](https://arxiv.org/abs/2602.03402v2)** | 2026-02-11 |  |
| **[Agentic Knowledge Distillation: Autonomous Training of Small Language Models for SMS Threat Detection](https://arxiv.org/abs/2602.10869v1)** | 2026-02-11 |  |
| **[from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors](https://arxiv.org/abs/2503.00038v5)** | 2026-02-11 | <details><summary>arXiv...</summary><p>arXiv admin note: substantial text overlap with arXiv:2412.12145</p></details> |
| **[Generative clinical time series models trained on moderate amounts of patient data are privacy preserving](https://arxiv.org/abs/2602.10631v1)** | 2026-02-11 |  |
| **[Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models](https://arxiv.org/abs/2602.10382v1)** | 2026-02-11 | 13 pages, 35 figures |
| **[A Low-Rank Defense Method for Adversarial Attack on Diffusion Models](https://arxiv.org/abs/2602.10319v1)** | 2026-02-10 | Accepted by ICME2025 |
| **[SecureCode: A Production-Grade Multi-Turn Dataset for Training Security-Aware Code Generation Models](https://arxiv.org/abs/2512.18542v2)** | 2026-02-10 | <details><summary>27 pa...</summary><p>27 pages, 12 figures, 10 tables. Dataset available at https://huggingface.co/datasets/scthornton/securecode. Code and validation tools at https://github.com/scthornton/securecode</p></details> |
| **[Learning-Based Automated Adversarial Red-Teaming for Robustness Evaluation of Large Language Models](https://arxiv.org/abs/2512.20677v2)** | 2026-02-10 | accepted by EACL |
| **[When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models](https://arxiv.org/abs/2602.10179v1)** | 2026-02-10 | <details><summary>Proje...</summary><p>Project homepage: https://csu-jpg.github.io/vja.github.io/</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away](https://arxiv.org/abs/2602.11096v1)** | 2026-02-11 |  |
| **[Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility](https://arxiv.org/abs/2602.03402v2)** | 2026-02-11 |  |
| **[Agentic Knowledge Distillation: Autonomous Training of Small Language Models for SMS Threat Detection](https://arxiv.org/abs/2602.10869v1)** | 2026-02-11 |  |
| **[from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors](https://arxiv.org/abs/2503.00038v5)** | 2026-02-11 | <details><summary>arXiv...</summary><p>arXiv admin note: substantial text overlap with arXiv:2412.12145</p></details> |
| **[Generative clinical time series models trained on moderate amounts of patient data are privacy preserving](https://arxiv.org/abs/2602.10631v1)** | 2026-02-11 |  |
| **[Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models](https://arxiv.org/abs/2602.10382v1)** | 2026-02-11 | 13 pages, 35 figures |
| **[A Low-Rank Defense Method for Adversarial Attack on Diffusion Models](https://arxiv.org/abs/2602.10319v1)** | 2026-02-10 | Accepted by ICME2025 |
| **[SecureCode: A Production-Grade Multi-Turn Dataset for Training Security-Aware Code Generation Models](https://arxiv.org/abs/2512.18542v2)** | 2026-02-10 | <details><summary>27 pa...</summary><p>27 pages, 12 figures, 10 tables. Dataset available at https://huggingface.co/datasets/scthornton/securecode. Code and validation tools at https://github.com/scthornton/securecode</p></details> |
| **[Learning-Based Automated Adversarial Red-Teaming for Robustness Evaluation of Large Language Models](https://arxiv.org/abs/2512.20677v2)** | 2026-02-10 | accepted by EACL |
| **[When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models](https://arxiv.org/abs/2602.10179v1)** | 2026-02-10 | <details><summary>Proje...</summary><p>Project homepage: https://csu-jpg.github.io/vja.github.io/</p></details> |

