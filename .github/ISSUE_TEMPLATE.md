---
title: Latest 15 Papers - February 12, 2026
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment](https://arxiv.org/abs/2602.08023v2)** | 2026-02-10 |  |
| **[Stop Testing Attacks, Start Diagnosing Defenses: The Four-Checkpoint Framework Reveals Where LLM Safety Breaks](https://arxiv.org/abs/2602.09629v1)** | 2026-02-10 | 17 pages, pre-print |
| **[The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097v2)** | 2026-02-10 | <details><summary>Accep...</summary><p>Accepted by ICLR 2026</p></details> |
| **[Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs](https://arxiv.org/abs/2501.16534v4)** | 2026-02-09 | <details><summary>Accep...</summary><p>Accepted to the 4th IEEE Conference on Secure and Trustworthy Machine Learning (SaTML'26)</p></details> |
| **[SearchAttack: Red-Teaming LLMs against Knowledge-to-Action Threats under Online Web Search](https://arxiv.org/abs/2601.04093v2)** | 2026-02-09 | <details><summary>Misus...</summary><p>Misusing LLM-driven search for harmful information-seeking poses serious risks. We characterize its usability and impact through a comprehensive red-teaming and evaluation</p></details> |
| **[Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.08621v1)** | 2026-02-09 |  |
| **[LLMs + Security = Trouble](https://arxiv.org/abs/2602.08422v1)** | 2026-02-09 |  |
| **[SoK: Trust-Authorization Mismatch in LLM Agent Interactions](https://arxiv.org/abs/2512.06914v2)** | 2026-02-09 |  |
| **[Can We Infer Confidential Properties of Training Data from LLMs?](https://arxiv.org/abs/2506.10364v4)** | 2026-02-09 |  |
| **[Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation](https://arxiv.org/abs/2602.08062v1)** | 2026-02-08 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment](https://arxiv.org/abs/2602.08023v2)** | 2026-02-10 |  |
| **[Stop Testing Attacks, Start Diagnosing Defenses: The Four-Checkpoint Framework Reveals Where LLM Safety Breaks](https://arxiv.org/abs/2602.09629v1)** | 2026-02-10 | 17 pages, pre-print |
| **[The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097v2)** | 2026-02-10 | <details><summary>Accep...</summary><p>Accepted by ICLR 2026</p></details> |
| **[Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs](https://arxiv.org/abs/2501.16534v4)** | 2026-02-09 | <details><summary>Accep...</summary><p>Accepted to the 4th IEEE Conference on Secure and Trustworthy Machine Learning (SaTML'26)</p></details> |
| **[SearchAttack: Red-Teaming LLMs against Knowledge-to-Action Threats under Online Web Search](https://arxiv.org/abs/2601.04093v2)** | 2026-02-09 | <details><summary>Misus...</summary><p>Misusing LLM-driven search for harmful information-seeking poses serious risks. We characterize its usability and impact through a comprehensive red-teaming and evaluation</p></details> |
| **[Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.08621v1)** | 2026-02-09 |  |
| **[Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs](https://arxiv.org/abs/2602.08563v1)** | 2026-02-09 | <details><summary>Accep...</summary><p>Accepted at IEEE SaTML 2026</p></details> |
| **[LLMs + Security = Trouble](https://arxiv.org/abs/2602.08422v1)** | 2026-02-09 |  |
| **[SoK: Trust-Authorization Mismatch in LLM Agent Interactions](https://arxiv.org/abs/2512.06914v2)** | 2026-02-09 |  |
| **[Can We Infer Confidential Properties of Training Data from LLMs?](https://arxiv.org/abs/2506.10364v4)** | 2026-02-09 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Towards Poisoning Robustness Certification for Natural Language Generation](https://arxiv.org/abs/2602.09757v1)** | 2026-02-10 |  |
| **[Linear Model Extraction via Factual and Counterfactual Queries](https://arxiv.org/abs/2602.09748v1)** | 2026-02-10 |  |
| **[AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models](https://arxiv.org/abs/2602.09611v1)** | 2026-02-10 | preprint |
| **[A Behavioral Fingerprint for Large Language Models: Provenance Tracking via Refusal Vectors](https://arxiv.org/abs/2602.09434v1)** | 2026-02-10 |  |
| **[Understanding and Enhancing Encoder-based Adversarial Transferability against Large Vision-Language Models](https://arxiv.org/abs/2602.09431v1)** | 2026-02-10 | <details><summary>Under...</summary><p>Under review; 21 pages</p></details> |
| **[A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?](https://arxiv.org/abs/2511.05476v3)** | 2026-02-09 | <details><summary>This ...</summary><p>This paper is a revised version of a manuscript currently under revision at the Journal of Systems and Software</p></details> |
| **[Is Reasoning Capability Enough for Safety in Long-Context Language Models?](https://arxiv.org/abs/2602.08874v1)** | 2026-02-09 | 25 pages, 7 figures |
| **[ASIDE: Architectural Separation of Instructions and Data in Language Models](https://arxiv.org/abs/2503.10566v4)** | 2026-02-09 | ICLR 2026 paper |
| **[Large Language Lobotomy: Jailbreaking Mixture-of-Experts via Expert Silencing](https://arxiv.org/abs/2602.08741v1)** | 2026-02-09 |  |
| **[Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.08621v1)** | 2026-02-09 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Towards Poisoning Robustness Certification for Natural Language Generation](https://arxiv.org/abs/2602.09757v1)** | 2026-02-10 |  |
| **[Linear Model Extraction via Factual and Counterfactual Queries](https://arxiv.org/abs/2602.09748v1)** | 2026-02-10 |  |
| **[AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models](https://arxiv.org/abs/2602.09611v1)** | 2026-02-10 | preprint |
| **[A Behavioral Fingerprint for Large Language Models: Provenance Tracking via Refusal Vectors](https://arxiv.org/abs/2602.09434v1)** | 2026-02-10 |  |
| **[Understanding and Enhancing Encoder-based Adversarial Transferability against Large Vision-Language Models](https://arxiv.org/abs/2602.09431v1)** | 2026-02-10 | <details><summary>Under...</summary><p>Under review; 21 pages</p></details> |
| **[A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?](https://arxiv.org/abs/2511.05476v3)** | 2026-02-09 | <details><summary>This ...</summary><p>This paper is a revised version of a manuscript currently under revision at the Journal of Systems and Software</p></details> |
| **[Is Reasoning Capability Enough for Safety in Long-Context Language Models?](https://arxiv.org/abs/2602.08874v1)** | 2026-02-09 | 25 pages, 7 figures |
| **[ASIDE: Architectural Separation of Instructions and Data in Language Models](https://arxiv.org/abs/2503.10566v4)** | 2026-02-09 | ICLR 2026 paper |
| **[Large Language Lobotomy: Jailbreaking Mixture-of-Experts via Expert Silencing](https://arxiv.org/abs/2602.08741v1)** | 2026-02-09 |  |
| **[Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.08621v1)** | 2026-02-09 |  |

