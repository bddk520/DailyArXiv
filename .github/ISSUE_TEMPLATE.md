---
title: Latest 15 Papers - October 14, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](http://arxiv.org/abs/2507.01752v2)** | 2025-10-10 |  |
| **[Fewer Weights, More Problems: A Practical Attack on LLM Pruning](http://arxiv.org/abs/2510.07985v2)** | 2025-10-10 |  |
| **[P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs](http://arxiv.org/abs/2510.04503v2)** | 2025-10-10 |  |
| **[The Model's Language Matters: A Comparative Privacy Analysis of LLMs](http://arxiv.org/abs/2510.08813v1)** | 2025-10-09 |  |
| **[Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?](http://arxiv.org/abs/2510.06594v2)** | 2025-10-09 |  |
| **[Watch your steps: Dormant Adversarial Behaviors that Activate upon LLM Finetuning](http://arxiv.org/abs/2505.16567v3)** | 2025-10-09 |  |
| **[Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment](http://arxiv.org/abs/2509.22745v2)** | 2025-10-09 | Under review |
| **[Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](http://arxiv.org/abs/2507.11112v2)** | 2025-10-09 |  |
| **[Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations](http://arxiv.org/abs/2510.08120v1)** | 2025-10-09 | <details><summary>12 pa...</summary><p>12 pages, 2 figures, 3 tables</p></details> |
| **[(Token-Level) InfoRMIA: Stronger Membership Inference and Memorization Assessment for LLMs](http://arxiv.org/abs/2510.05582v2)** | 2025-10-09 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](http://arxiv.org/abs/2507.01752v2)** | 2025-10-10 |  |
| **[Fewer Weights, More Problems: A Practical Attack on LLM Pruning](http://arxiv.org/abs/2510.07985v2)** | 2025-10-10 |  |
| **[P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs](http://arxiv.org/abs/2510.04503v2)** | 2025-10-10 |  |
| **[The Model's Language Matters: A Comparative Privacy Analysis of LLMs](http://arxiv.org/abs/2510.08813v1)** | 2025-10-09 |  |
| **[Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?](http://arxiv.org/abs/2510.06594v2)** | 2025-10-09 |  |
| **[The Shape of Adversarial Influence: Characterizing LLM Latent Spaces with Persistent Homology](http://arxiv.org/abs/2505.20435v2)** | 2025-10-09 |  |
| **[Watch your steps: Dormant Adversarial Behaviors that Activate upon LLM Finetuning](http://arxiv.org/abs/2505.16567v3)** | 2025-10-09 |  |
| **[Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment](http://arxiv.org/abs/2509.22745v2)** | 2025-10-09 | Under review |
| **[Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](http://arxiv.org/abs/2507.11112v2)** | 2025-10-09 |  |
| **[Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations](http://arxiv.org/abs/2510.08120v1)** | 2025-10-09 | <details><summary>12 pa...</summary><p>12 pages, 2 figures, 3 tables</p></details> |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects](http://arxiv.org/abs/2510.09269v1)** | 2025-10-10 |  |
| **[SegTrans: Transferable Adversarial Examples for Segmentation Models](http://arxiv.org/abs/2510.08922v1)** | 2025-10-10 | Accepted by TMM 2025 |
| **[Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models](http://arxiv.org/abs/2510.08859v1)** | 2025-10-09 |  |
| **[The Model's Language Matters: A Comparative Privacy Analysis of LLMs](http://arxiv.org/abs/2510.08813v1)** | 2025-10-09 |  |
| **[MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols](http://arxiv.org/abs/2508.13220v2)** | 2025-10-09 | <details><summary>This ...</summary><p>This is a technical report from Lingnan University, Hong Kong. Code is available at https://github.com/AIS2Lab/MCPSecBench</p></details> |
| **[Provably Robust Adaptation for Language-Empowered Foundation Models](http://arxiv.org/abs/2510.08659v1)** | 2025-10-09 | 19 pages |
| **[Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks](http://arxiv.org/abs/2506.11113v3)** | 2025-10-09 | <details><summary>Minor...</summary><p>Minor correction: Fixed sign errors in the results table. The update does not affect the main findings or conclusions</p></details> |
| **[Rule Encoding and Compliance in Large Language Models: An Information-Theoretic Analysis](http://arxiv.org/abs/2510.05106v2)** | 2025-10-09 |  |
| **[Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models](http://arxiv.org/abs/2508.21099v2)** | 2025-10-09 |  |
| **[PiCo: Jailbreaking Multimodal Large Language Models via Pictorial Code Contextualization](http://arxiv.org/abs/2504.01444v4)** | 2025-10-09 | <details><summary>Accep...</summary><p>Accepted to IEEE International Conference on Multimedia and Expo (ICME) 2025</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Causal Additive Models with Unobserved Causal Paths and Backdoor Paths](http://arxiv.org/abs/2502.07646v2)** | 2025-10-10 | 23 pages |
| **[Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects](http://arxiv.org/abs/2510.09269v1)** | 2025-10-10 |  |
| **[SegTrans: Transferable Adversarial Examples for Segmentation Models](http://arxiv.org/abs/2510.08922v1)** | 2025-10-10 | Accepted by TMM 2025 |
| **[Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models](http://arxiv.org/abs/2510.08859v1)** | 2025-10-09 |  |
| **[The Model's Language Matters: A Comparative Privacy Analysis of LLMs](http://arxiv.org/abs/2510.08813v1)** | 2025-10-09 |  |
| **[MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols](http://arxiv.org/abs/2508.13220v2)** | 2025-10-09 | <details><summary>This ...</summary><p>This is a technical report from Lingnan University, Hong Kong. Code is available at https://github.com/AIS2Lab/MCPSecBench</p></details> |
| **[Provably Robust Adaptation for Language-Empowered Foundation Models](http://arxiv.org/abs/2510.08659v1)** | 2025-10-09 | 19 pages |
| **[Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks](http://arxiv.org/abs/2506.11113v3)** | 2025-10-09 | <details><summary>Minor...</summary><p>Minor correction: Fixed sign errors in the results table. The update does not affect the main findings or conclusions</p></details> |
| **[Rule Encoding and Compliance in Large Language Models: An Information-Theoretic Analysis](http://arxiv.org/abs/2510.05106v2)** | 2025-10-09 |  |
| **[Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models](http://arxiv.org/abs/2508.21099v2)** | 2025-10-09 |  |

