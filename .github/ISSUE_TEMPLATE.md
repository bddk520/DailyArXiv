---
title: Latest 15 Papers - January 12, 2026
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Knowledge-to-Data: LLM-Driven Synthesis of Structured Network Traffic for Testbed-Free IDS Evaluation](https://arxiv.org/abs/2601.05022v1)** | 2026-01-08 |  |
| **[Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029v2)** | 2026-01-08 |  |
| **[ResMAS: Resilience Optimization in LLM-based Multi-agent Systems](https://arxiv.org/abs/2601.04694v1)** | 2026-01-08 |  |
| **[Know Thy Enemy: Securing LLMs Against Prompt Injection via Diverse Data Synthesis and Instruction-Level Chain-of-Thought Learning](https://arxiv.org/abs/2601.04666v1)** | 2026-01-08 | 19 pages, 6 figures |
| **[MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Domain Risks in LLMs](https://arxiv.org/abs/2511.07107v2)** | 2026-01-08 |  |
| **[BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents](https://arxiv.org/abs/2601.04566v1)** | 2026-01-08 |  |
| **[SearchAttack: Red-Teaming LLMs against Real-World Threats via Framing Unsafe Web Information-Seeking Tasks](https://arxiv.org/abs/2601.04093v1)** | 2026-01-07 | <details><summary>We fi...</summary><p>We find that the key to jailbreak the LLM is objectifying its safety responsibility, thus we delegate the open-web to inject harmful semantics and get the huge gain from unmoderated web resources</p></details> |
| **[Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390v3)** | 2026-01-07 |  |
| **[Shadow Unlearning: A Neuro-Semantic Approach to Fidelity-Preserving Faceless Forgetting in LLMs](https://arxiv.org/abs/2601.04275v1)** | 2026-01-07 |  |
| **[Web Fraud Attacks Against LLM-Driven Multi-Agent Systems](https://arxiv.org/abs/2509.01211v2)** | 2026-01-07 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Knowledge-to-Data: LLM-Driven Synthesis of Structured Network Traffic for Testbed-Free IDS Evaluation](https://arxiv.org/abs/2601.05022v1)** | 2026-01-08 |  |
| **[Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029v2)** | 2026-01-08 |  |
| **[ResMAS: Resilience Optimization in LLM-based Multi-agent Systems](https://arxiv.org/abs/2601.04694v1)** | 2026-01-08 |  |
| **[Know Thy Enemy: Securing LLMs Against Prompt Injection via Diverse Data Synthesis and Instruction-Level Chain-of-Thought Learning](https://arxiv.org/abs/2601.04666v1)** | 2026-01-08 | 19 pages, 6 figures |
| **[MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Domain Risks in LLMs](https://arxiv.org/abs/2511.07107v2)** | 2026-01-08 |  |
| **[BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents](https://arxiv.org/abs/2601.04566v1)** | 2026-01-08 |  |
| **[SearchAttack: Red-Teaming LLMs against Real-World Threats via Framing Unsafe Web Information-Seeking Tasks](https://arxiv.org/abs/2601.04093v1)** | 2026-01-07 | <details><summary>We fi...</summary><p>We find that the key to jailbreak the LLM is objectifying its safety responsibility, thus we delegate the open-web to inject harmful semantics and get the huge gain from unmoderated web resources</p></details> |
| **[Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390v3)** | 2026-01-07 |  |
| **[Shadow Unlearning: A Neuro-Semantic Approach to Fidelity-Preserving Faceless Forgetting in LLMs](https://arxiv.org/abs/2601.04275v1)** | 2026-01-07 |  |
| **[Web Fraud Attacks Against LLM-Driven Multi-Agent Systems](https://arxiv.org/abs/2509.01211v2)** | 2026-01-07 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[$PC^2$: Politically Controversial Content Generation via Jailbreaking Attacks on GPT-based Text-to-Image Models](https://arxiv.org/abs/2601.05150v1)** | 2026-01-08 |  |
| **[Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models](https://arxiv.org/abs/2601.05144v1)** | 2026-01-08 |  |
| **[Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization](https://arxiv.org/abs/2601.01747v2)** | 2026-01-08 | EACL |
| **[Measuring the Impact of Student Gaming Behaviors on Learner Modeling](https://arxiv.org/abs/2512.18659v2)** | 2026-01-08 |  |
| **[Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition](https://arxiv.org/abs/2601.04752v1)** | 2026-01-08 | <details><summary>accep...</summary><p>accepted to ITC-CSCC 2025</p></details> |
| **[MCP-Guard: A Multi-Stage Defense-in-Depth Framework for Securing Model Context Protocol in Agentic AI](https://arxiv.org/abs/2508.10991v4)** | 2026-01-08 |  |
| **[Deep Dive into the Abuse of DL APIs To Create Malicious AI Models and How to Detect Them](https://arxiv.org/abs/2601.04553v1)** | 2026-01-08 | virusbulletin 2025 |
| **[BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation](https://arxiv.org/abs/2601.04534v1)** | 2026-01-08 | <details><summary>Under...</summary><p>Under review, 12 pages, 7 figures, 5 tables</p></details> |
| **[Exploring the limits of strong membership inference attacks on large language models](https://arxiv.org/abs/2505.18773v3)** | 2026-01-08 | NeurIPS 2025 |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[$PC^2$: Politically Controversial Content Generation via Jailbreaking Attacks on GPT-based Text-to-Image Models](https://arxiv.org/abs/2601.05150v1)** | 2026-01-08 |  |
| **[Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models](https://arxiv.org/abs/2601.05144v1)** | 2026-01-08 |  |
| **[Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization](https://arxiv.org/abs/2601.01747v2)** | 2026-01-08 | EACL |
| **[Measuring the Impact of Student Gaming Behaviors on Learner Modeling](https://arxiv.org/abs/2512.18659v2)** | 2026-01-08 |  |
| **[Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition](https://arxiv.org/abs/2601.04752v1)** | 2026-01-08 | <details><summary>accep...</summary><p>accepted to ITC-CSCC 2025</p></details> |
| **[MCP-Guard: A Multi-Stage Defense-in-Depth Framework for Securing Model Context Protocol in Agentic AI](https://arxiv.org/abs/2508.10991v4)** | 2026-01-08 |  |
| **[Deep Dive into the Abuse of DL APIs To Create Malicious AI Models and How to Detect Them](https://arxiv.org/abs/2601.04553v1)** | 2026-01-08 | virusbulletin 2025 |
| **[BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation](https://arxiv.org/abs/2601.04534v1)** | 2026-01-08 | <details><summary>Under...</summary><p>Under review, 12 pages, 7 figures, 5 tables</p></details> |
| **[Exploring the limits of strong membership inference attacks on large language models](https://arxiv.org/abs/2505.18773v3)** | 2026-01-08 | NeurIPS 2025 |

