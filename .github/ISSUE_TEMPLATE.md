---
title: Latest 15 Papers - March 12, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Stepwise Reasoning Error Disruption Attack of LLMs](http://arxiv.org/abs/2412.11934v3)** | 2025-03-10 |  |
| **[Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions](http://arxiv.org/abs/2410.18966v2)** | 2025-03-09 | <details><summary>3 tab...</summary><p>3 tables and 1 figures in the main text. This paper is accepted by NAACL 2025 findings</p></details> |
| **[MAD-MAX: Modular And Diverse Malicious Attack MiXtures for Automated LLM Red Teaming](http://arxiv.org/abs/2503.06253v1)** | 2025-03-08 |  |
| **[Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents](http://arxiv.org/abs/2410.02644v2)** | 2025-03-08 |  |
| **[SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How to Fix It)](http://arxiv.org/abs/2406.17975v3)** | 2025-03-07 | <details><summary>IEEE ...</summary><p>IEEE Conference on Secure and Trustworthy Machine Learning (SaTML 2025)</p></details> |
| **[Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection via Backdoor Attacks](http://arxiv.org/abs/2503.05445v1)** | 2025-03-07 |  |
| **[DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios](http://arxiv.org/abs/2410.23746v2)** | 2025-03-07 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Camera-Ready)</p></details> |
| **[A Practical Memory Injection Attack against LLM Agents](http://arxiv.org/abs/2503.03704v2)** | 2025-03-07 |  |
| **[Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety](http://arxiv.org/abs/2503.05021v1)** | 2025-03-06 |  |
| **[Get my drift? Catching LLM Task Drift with Activation Deltas](http://arxiv.org/abs/2406.00799v6)** | 2025-03-06 | SaTML 2025 |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Stepwise Reasoning Error Disruption Attack of LLMs](http://arxiv.org/abs/2412.11934v3)** | 2025-03-10 |  |
| **[Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions](http://arxiv.org/abs/2410.18966v2)** | 2025-03-09 | <details><summary>3 tab...</summary><p>3 tables and 1 figures in the main text. This paper is accepted by NAACL 2025 findings</p></details> |
| **[MAD-MAX: Modular And Diverse Malicious Attack MiXtures for Automated LLM Red Teaming](http://arxiv.org/abs/2503.06253v1)** | 2025-03-08 |  |
| **[Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents](http://arxiv.org/abs/2410.02644v2)** | 2025-03-08 |  |
| **[SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How to Fix It)](http://arxiv.org/abs/2406.17975v3)** | 2025-03-07 | <details><summary>IEEE ...</summary><p>IEEE Conference on Secure and Trustworthy Machine Learning (SaTML 2025)</p></details> |
| **[Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection via Backdoor Attacks](http://arxiv.org/abs/2503.05445v1)** | 2025-03-07 |  |
| **[DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios](http://arxiv.org/abs/2410.23746v2)** | 2025-03-07 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Camera-Ready)</p></details> |
| **[A Practical Memory Injection Attack against LLM Agents](http://arxiv.org/abs/2503.03704v2)** | 2025-03-07 |  |
| **[Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety](http://arxiv.org/abs/2503.05021v1)** | 2025-03-06 |  |
| **[Get my drift? Catching LLM Task Drift with Activation Deltas](http://arxiv.org/abs/2406.00799v6)** | 2025-03-06 | SaTML 2025 |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MIBench: A Comprehensive Framework for Benchmarking Model Inversion Attack and Defense](http://arxiv.org/abs/2410.05159v3)** | 2025-03-10 | 20 pages |
| **[Stochastic Tube-based Model Predictive Control for Cyber-Physical Systems under False Data Injection Attacks with Bounded Probability](http://arxiv.org/abs/2503.07385v1)** | 2025-03-10 |  |
| **[Extracting Training Data from Unconditional Diffusion Models](http://arxiv.org/abs/2410.02467v6)** | 2025-03-10 |  |
| **[Privacy and Accuracy Implications of Model Complexity and Integration in Heterogeneous Federated Learning](http://arxiv.org/abs/2311.17750v3)** | 2025-03-10 | <details><summary>Code:...</summary><p>Code: https://github.com/ellisalicante/ma-fl-mia</p></details> |
| **[Robust Diffusion Models for Adversarial Purification](http://arxiv.org/abs/2403.16067v4)** | 2025-03-10 |  |
| **[ConcreTizer: Model Inversion Attack via Occupancy Classification and Dispersion Control for 3D Point Cloud Restoration](http://arxiv.org/abs/2503.06986v1)** | 2025-03-10 |  |
| **[InferDPT: Privacy-Preserving Inference for Black-box Large Language Model](http://arxiv.org/abs/2310.12214v7)** | 2025-03-10 |  |
| **[MIGA: Mutual Information-Guided Attack on Denoising Models for Semantic Manipulation](http://arxiv.org/abs/2503.06966v1)** | 2025-03-10 |  |
| **[Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?](http://arxiv.org/abs/2407.17417v2)** | 2025-03-10 | <details><summary>19 pa...</summary><p>19 pages, 7 figures. Published at AAAI 2025. Code will be available at https://github.com/michael-panaitescu/watermark_copyright_aaai25</p></details> |
| **[CtrlRAG: Black-box Adversarial Attacks Based on Masked Language Models in Retrieval-Augmented Language Generation](http://arxiv.org/abs/2503.06950v1)** | 2025-03-10 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MIBench: A Comprehensive Framework for Benchmarking Model Inversion Attack and Defense](http://arxiv.org/abs/2410.05159v3)** | 2025-03-10 | 20 pages |
| **[Stochastic Tube-based Model Predictive Control for Cyber-Physical Systems under False Data Injection Attacks with Bounded Probability](http://arxiv.org/abs/2503.07385v1)** | 2025-03-10 |  |
| **[Extracting Training Data from Unconditional Diffusion Models](http://arxiv.org/abs/2410.02467v6)** | 2025-03-10 |  |
| **[Privacy and Accuracy Implications of Model Complexity and Integration in Heterogeneous Federated Learning](http://arxiv.org/abs/2311.17750v3)** | 2025-03-10 | <details><summary>Code:...</summary><p>Code: https://github.com/ellisalicante/ma-fl-mia</p></details> |
| **[Robust Diffusion Models for Adversarial Purification](http://arxiv.org/abs/2403.16067v4)** | 2025-03-10 |  |
| **[ConcreTizer: Model Inversion Attack via Occupancy Classification and Dispersion Control for 3D Point Cloud Restoration](http://arxiv.org/abs/2503.06986v1)** | 2025-03-10 |  |
| **[InferDPT: Privacy-Preserving Inference for Black-box Large Language Model](http://arxiv.org/abs/2310.12214v7)** | 2025-03-10 |  |
| **[MIGA: Mutual Information-Guided Attack on Denoising Models for Semantic Manipulation](http://arxiv.org/abs/2503.06966v1)** | 2025-03-10 |  |
| **[Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?](http://arxiv.org/abs/2407.17417v2)** | 2025-03-10 | <details><summary>19 pa...</summary><p>19 pages, 7 figures. Published at AAAI 2025. Code will be available at https://github.com/michael-panaitescu/watermark_copyright_aaai25</p></details> |
| **[CtrlRAG: Black-box Adversarial Attacks Based on Masked Language Models in Retrieval-Augmented Language Generation](http://arxiv.org/abs/2503.06950v1)** | 2025-03-10 |  |

