---
title: Latest 15 Papers - January 19, 2026
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[NATLM: Detecting Defects in NFT Smart Contracts Leveraging LLM](https://arxiv.org/abs/2508.01351v2)** | 2026-01-15 |  |
| **[Reasoning Hijacking: Subverting LLM Classification via Decision-Criteria Injection](https://arxiv.org/abs/2601.10294v1)** | 2026-01-15 |  |
| **[ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback](https://arxiv.org/abs/2601.10156v1)** | 2026-01-15 | <details><summary>Work ...</summary><p>Work in Progress. Code available: https://github.com/MurrayTom/ToolSafe</p></details> |
| **[STELP: Secure Transpilation and Execution of LLM-Generated Programs](https://arxiv.org/abs/2601.05467v3)** | 2026-01-15 |  |
| **[Understanding and Preserving Safety in Fine-Tuned LLMs](https://arxiv.org/abs/2601.10141v1)** | 2026-01-15 |  |
| **[MindGuard: Intrinsic Decision Inspection for Securing LLM Agents Against Metadata Poisoning](https://arxiv.org/abs/2508.20412v3)** | 2026-01-15 |  |
| **[SoK: Privacy-aware LLM in Healthcare: Threat Model, Privacy Techniques, Challenges and Recommendations](https://arxiv.org/abs/2601.10004v1)** | 2026-01-15 |  |
| **[VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit](https://arxiv.org/abs/2601.05755v2)** | 2026-01-14 |  |
| **[Burn-After-Use for Preventing Data Leakage through a Secure Multi-Tenant Architecture in Enterprise LLM](https://arxiv.org/abs/2601.06627v2)** | 2026-01-14 | 16 pages, 5 figures |
| **[SpatialJB: How Text Distribution Art Becomes the "Jailbreak Key" for LLM Guardrails](https://arxiv.org/abs/2601.09321v1)** | 2026-01-14 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[NATLM: Detecting Defects in NFT Smart Contracts Leveraging LLM](https://arxiv.org/abs/2508.01351v2)** | 2026-01-15 |  |
| **[Reasoning Hijacking: Subverting LLM Classification via Decision-Criteria Injection](https://arxiv.org/abs/2601.10294v1)** | 2026-01-15 |  |
| **[ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback](https://arxiv.org/abs/2601.10156v1)** | 2026-01-15 | <details><summary>Work ...</summary><p>Work in Progress. Code available: https://github.com/MurrayTom/ToolSafe</p></details> |
| **[STELP: Secure Transpilation and Execution of LLM-Generated Programs](https://arxiv.org/abs/2601.05467v3)** | 2026-01-15 |  |
| **[Understanding and Preserving Safety in Fine-Tuned LLMs](https://arxiv.org/abs/2601.10141v1)** | 2026-01-15 |  |
| **[MindGuard: Intrinsic Decision Inspection for Securing LLM Agents Against Metadata Poisoning](https://arxiv.org/abs/2508.20412v3)** | 2026-01-15 |  |
| **[SoK: Privacy-aware LLM in Healthcare: Threat Model, Privacy Techniques, Challenges and Recommendations](https://arxiv.org/abs/2601.10004v1)** | 2026-01-15 |  |
| **[VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit](https://arxiv.org/abs/2601.05755v2)** | 2026-01-14 |  |
| **[Burn-After-Use for Preventing Data Leakage through a Secure Multi-Tenant Architecture in Enterprise LLM](https://arxiv.org/abs/2601.06627v2)** | 2026-01-14 | 16 pages, 5 figures |
| **[SpatialJB: How Text Distribution Art Becomes the "Jailbreak Key" for LLM Guardrails](https://arxiv.org/abs/2601.09321v1)** | 2026-01-14 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Detecting Winning Arguments with Large Language Models and Persuasion Strategies](https://arxiv.org/abs/2601.10660v1)** | 2026-01-15 |  |
| **[Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543v1)** | 2026-01-15 |  |
| **[$PC^2$: Politically Controversial Content Generation via Jailbreaking Attacks on GPT-based Text-to-Image Models](https://arxiv.org/abs/2601.05150v2)** | 2026-01-15 | <details><summary>19 pa...</summary><p>19 pages, 15 figures, 9 tables</p></details> |
| **[Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models](https://arxiv.org/abs/2601.10313v1)** | 2026-01-15 | 15 pages, 7 figures |
| **[MLAAD: The Multi-Language Audio Anti-Spoofing Dataset](https://arxiv.org/abs/2401.09512v9)** | 2026-01-15 | IJCNN 2024 |
| **[SoK: Privacy-aware LLM in Healthcare: Threat Model, Privacy Techniques, Challenges and Recommendations](https://arxiv.org/abs/2601.10004v1)** | 2026-01-15 |  |
| **[Exploring the Secondary Risks of Large Language Models](https://arxiv.org/abs/2506.12382v4)** | 2026-01-14 | 18 pages, 5 figures |
| **[STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models](https://arxiv.org/abs/2601.09281v1)** | 2026-01-14 |  |
| **[Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models](https://arxiv.org/abs/2508.01741v3)** | 2026-01-14 |  |
| **[Enhanced Web Payload Classification Using WAMM: An AI-Based Framework for Dataset Refinement and Model Evaluation](https://arxiv.org/abs/2512.23610v2)** | 2026-01-13 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Detecting Winning Arguments with Large Language Models and Persuasion Strategies](https://arxiv.org/abs/2601.10660v1)** | 2026-01-15 |  |
| **[Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543v1)** | 2026-01-15 |  |
| **[$PC^2$: Politically Controversial Content Generation via Jailbreaking Attacks on GPT-based Text-to-Image Models](https://arxiv.org/abs/2601.05150v2)** | 2026-01-15 | <details><summary>19 pa...</summary><p>19 pages, 15 figures, 9 tables</p></details> |
| **[Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models](https://arxiv.org/abs/2601.10313v1)** | 2026-01-15 | 15 pages, 7 figures |
| **[MLAAD: The Multi-Language Audio Anti-Spoofing Dataset](https://arxiv.org/abs/2401.09512v9)** | 2026-01-15 | IJCNN 2024 |
| **[SoK: Privacy-aware LLM in Healthcare: Threat Model, Privacy Techniques, Challenges and Recommendations](https://arxiv.org/abs/2601.10004v1)** | 2026-01-15 |  |
| **[Exploring the Secondary Risks of Large Language Models](https://arxiv.org/abs/2506.12382v4)** | 2026-01-14 | 18 pages, 5 figures |
| **[STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models](https://arxiv.org/abs/2601.09281v1)** | 2026-01-14 |  |
| **[Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models](https://arxiv.org/abs/2508.01741v3)** | 2026-01-14 |  |
| **[Enhanced Web Payload Classification Using WAMM: An AI-Based Framework for Dataset Refinement and Model Evaluation](https://arxiv.org/abs/2512.23610v2)** | 2026-01-13 |  |

