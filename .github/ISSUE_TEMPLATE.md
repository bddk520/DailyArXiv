---
title: Latest 15 Papers - March 18, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions](http://arxiv.org/abs/2406.12480v2)** | 2025-03-12 | ICLR 2025 Spotlight |
| **[CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data](http://arxiv.org/abs/2503.09334v1)** | 2025-03-12 | <details><summary>The p...</summary><p>The paper is submitted to "The 48th International ACM SIGIR Conference on Research and Development in Information Retrieval" and is currently under review</p></details> |
| **[DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios](http://arxiv.org/abs/2410.23746v3)** | 2025-03-12 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Camera-Ready)</p></details> |
| **[A Survey on Trustworthy LLM Agents: Threats and Countermeasures](http://arxiv.org/abs/2503.09648v1)** | 2025-03-12 |  |
| **[Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States](http://arxiv.org/abs/2503.09066v1)** | 2025-03-12 | 4 figures |
| **[JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing](http://arxiv.org/abs/2503.08990v1)** | 2025-03-12 |  |
| **[Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities](http://arxiv.org/abs/2410.18469v3)** | 2025-03-11 | <details><summary>Accep...</summary><p>Accepted to NAACL 2025 Main (oral)</p></details> |
| **[Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context](http://arxiv.org/abs/2412.16359v2)** | 2025-03-11 | <details><summary>arXiv...</summary><p>arXiv admin note: text overlap with arXiv:2407.14644</p></details> |
| **[Dialogue Injection Attack: Jailbreaking LLMs through Context Manipulation](http://arxiv.org/abs/2503.08195v1)** | 2025-03-11 | 17 pages, 10 figures |
| **[MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents](http://arxiv.org/abs/2412.08014v2)** | 2025-03-11 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions](http://arxiv.org/abs/2406.12480v2)** | 2025-03-12 | ICLR 2025 Spotlight |
| **[CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data](http://arxiv.org/abs/2503.09334v1)** | 2025-03-12 | <details><summary>The p...</summary><p>The paper is submitted to "The 48th International ACM SIGIR Conference on Research and Development in Information Retrieval" and is currently under review</p></details> |
| **[DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios](http://arxiv.org/abs/2410.23746v3)** | 2025-03-12 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Camera-Ready)</p></details> |
| **[A Survey on Trustworthy LLM Agents: Threats and Countermeasures](http://arxiv.org/abs/2503.09648v1)** | 2025-03-12 |  |
| **[Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States](http://arxiv.org/abs/2503.09066v1)** | 2025-03-12 | 4 figures |
| **[JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing](http://arxiv.org/abs/2503.08990v1)** | 2025-03-12 |  |
| **[Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities](http://arxiv.org/abs/2410.18469v3)** | 2025-03-11 | <details><summary>Accep...</summary><p>Accepted to NAACL 2025 Main (oral)</p></details> |
| **[Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context](http://arxiv.org/abs/2412.16359v2)** | 2025-03-11 | <details><summary>arXiv...</summary><p>arXiv admin note: text overlap with arXiv:2407.14644</p></details> |
| **[Dialogue Injection Attack: Jailbreaking LLMs through Context Manipulation](http://arxiv.org/abs/2503.08195v1)** | 2025-03-11 | 17 pages, 10 figures |
| **[MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents](http://arxiv.org/abs/2412.08014v2)** | 2025-03-11 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Are Deep Speech Denoising Models Robust to Adversarial Noise?](http://arxiv.org/abs/2503.11627v1)** | 2025-03-14 | 13 pages, 5 figures |
| **[Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense](http://arxiv.org/abs/2503.11619v1)** | 2025-03-14 |  |
| **[Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation](http://arxiv.org/abs/2310.00096v2)** | 2025-03-14 | <details><summary>Accep...</summary><p>Accepted in Artificial Intelligence Review</p></details> |
| **[Auditing language models for hidden objectives](http://arxiv.org/abs/2503.10965v1)** | 2025-03-14 |  |
| **[Towards a Systematic Evaluation of Hallucinations in Large-Vision Language Models](http://arxiv.org/abs/2412.20622v2)** | 2025-03-13 |  |
| **[ChatGPT Encounters Morphing Attack Detection: Zero-Shot MAD with Multi-Modal Large Language Models and General Vision Models](http://arxiv.org/abs/2503.10937v1)** | 2025-03-13 |  |
| **[TAIJI: Textual Anchoring for Immunizing Jailbreak Images in Vision Language Models](http://arxiv.org/abs/2503.10872v1)** | 2025-03-13 | <details><summary>Under...</summary><p>Under review of IJCAI-25</p></details> |
| **[A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1](http://arxiv.org/abs/2503.10635v1)** | 2025-03-13 | <details><summary>Code ...</summary><p>Code at: https://github.com/VILA-Lab/M-Attack</p></details> |
| **[Hierarchical Self-Supervised Adversarial Training for Robust Vision Models in Histopathology](http://arxiv.org/abs/2503.10629v1)** | 2025-03-13 |  |
| **[ASIDE: Architectural Separation of Instructions and Data in Language Models](http://arxiv.org/abs/2503.10566v1)** | 2025-03-13 | <details><summary>ICLR ...</summary><p>ICLR 2025 Workshop on Building Trust in Language Models and Applications</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Are Deep Speech Denoising Models Robust to Adversarial Noise?](http://arxiv.org/abs/2503.11627v1)** | 2025-03-14 | 13 pages, 5 figures |
| **[Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense](http://arxiv.org/abs/2503.11619v1)** | 2025-03-14 |  |
| **[Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation](http://arxiv.org/abs/2310.00096v2)** | 2025-03-14 | <details><summary>Accep...</summary><p>Accepted in Artificial Intelligence Review</p></details> |
| **[Auditing language models for hidden objectives](http://arxiv.org/abs/2503.10965v1)** | 2025-03-14 |  |
| **[Towards a Systematic Evaluation of Hallucinations in Large-Vision Language Models](http://arxiv.org/abs/2412.20622v2)** | 2025-03-13 |  |
| **[ChatGPT Encounters Morphing Attack Detection: Zero-Shot MAD with Multi-Modal Large Language Models and General Vision Models](http://arxiv.org/abs/2503.10937v1)** | 2025-03-13 |  |
| **[TAIJI: Textual Anchoring for Immunizing Jailbreak Images in Vision Language Models](http://arxiv.org/abs/2503.10872v1)** | 2025-03-13 | <details><summary>Under...</summary><p>Under review of IJCAI-25</p></details> |
| **[A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1](http://arxiv.org/abs/2503.10635v1)** | 2025-03-13 | <details><summary>Code ...</summary><p>Code at: https://github.com/VILA-Lab/M-Attack</p></details> |
| **[Hierarchical Self-Supervised Adversarial Training for Robust Vision Models in Histopathology](http://arxiv.org/abs/2503.10629v1)** | 2025-03-13 |  |
| **[ASIDE: Architectural Separation of Instructions and Data in Language Models](http://arxiv.org/abs/2503.10566v1)** | 2025-03-13 | <details><summary>ICLR ...</summary><p>ICLR 2025 Workshop on Building Trust in Language Models and Applications</p></details> |

