---
title: Latest 15 Papers - February 21, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Theoretically Grounded Framework for LLM Watermarking: A Distribution-Adaptive Approach](http://arxiv.org/abs/2410.02890v4)** | 2025-02-19 |  |
| **[Safety Layers in Aligned Large Language Models: The Key to LLM Security](http://arxiv.org/abs/2408.17003v4)** | 2025-02-19 |  |
| **[Efficient Safety Retrofitting Against Jailbreaking for LLMs](http://arxiv.org/abs/2502.13603v1)** | 2025-02-19 |  |
| **[RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation through LLM Activation Analysis](http://arxiv.org/abs/2411.18948v2)** | 2025-02-19 |  |
| **[Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection](http://arxiv.org/abs/2411.01077v2)** | 2025-02-18 |  |
| **[LAMD: Context-driven Android Malware Detection and Classification with LLMs](http://arxiv.org/abs/2502.13055v1)** | 2025-02-18 |  |
| **[LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks](http://arxiv.org/abs/2310.11409v5)** | 2025-02-18 |  |
| **[R.R.: Unveiling LLM Training Privacy through Recollection and Ranking](http://arxiv.org/abs/2502.12658v1)** | 2025-02-18 | 13 pages, 9 figures |
| **[DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent](http://arxiv.org/abs/2502.12575v1)** | 2025-02-18 |  |
| **[Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings](http://arxiv.org/abs/2412.13879v3)** | 2025-02-18 | <details><summary>22 pa...</summary><p>22 pages, 8 figures, 11 tables</p></details> |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Theoretically Grounded Framework for LLM Watermarking: A Distribution-Adaptive Approach](http://arxiv.org/abs/2410.02890v4)** | 2025-02-19 |  |
| **[Safety Layers in Aligned Large Language Models: The Key to LLM Security](http://arxiv.org/abs/2408.17003v4)** | 2025-02-19 |  |
| **[Efficient Safety Retrofitting Against Jailbreaking for LLMs](http://arxiv.org/abs/2502.13603v1)** | 2025-02-19 |  |
| **[RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation through LLM Activation Analysis](http://arxiv.org/abs/2411.18948v2)** | 2025-02-19 |  |
| **[Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection](http://arxiv.org/abs/2411.01077v2)** | 2025-02-18 |  |
| **[LAMD: Context-driven Android Malware Detection and Classification with LLMs](http://arxiv.org/abs/2502.13055v1)** | 2025-02-18 |  |
| **[LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks](http://arxiv.org/abs/2310.11409v5)** | 2025-02-18 |  |
| **[R.R.: Unveiling LLM Training Privacy through Recollection and Ranking](http://arxiv.org/abs/2502.12658v1)** | 2025-02-18 | 13 pages, 9 figures |
| **[DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent](http://arxiv.org/abs/2502.12575v1)** | 2025-02-18 |  |
| **[Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings](http://arxiv.org/abs/2412.13879v3)** | 2025-02-18 | <details><summary>22 pa...</summary><p>22 pages, 8 figures, 11 tables</p></details> |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region](http://arxiv.org/abs/2502.13946v1)** | 2025-02-19 |  |
| **[FedCC: Robust Federated Learning against Model Poisoning Attacks](http://arxiv.org/abs/2212.01976v3)** | 2025-02-19 |  |
| **[Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models](http://arxiv.org/abs/2502.11054v3)** | 2025-02-19 |  |
| **[Safety Layers in Aligned Large Language Models: The Key to LLM Security](http://arxiv.org/abs/2408.17003v4)** | 2025-02-19 |  |
| **[Na'vi or Knave: Jailbreaking Language Models via Metaphorical Avatars](http://arxiv.org/abs/2412.12145v3)** | 2025-02-19 | <details><summary>We st...</summary><p>We still need to polish our paper</p></details> |
| **[Poisoned Source Code Detection in Code Models](http://arxiv.org/abs/2502.13459v1)** | 2025-02-19 | <details><summary>Accep...</summary><p>Accepted for Publication in the Journal of Systems and Software (JSS)</p></details> |
| **[Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](http://arxiv.org/abs/2406.14023v2)** | 2025-02-19 | <details><summary>Code,...</summary><p>Code, data and benchmarks are available at https://github.com/yuchenwen1/ImplicitBiasPsychometricEvaluation and https://github.com/yuchenwen1/BUMBLE</p></details> |
| **[Secure and Efficient Watermarking for Latent Diffusion Models in Model Distribution Scenarios](http://arxiv.org/abs/2502.13345v1)** | 2025-02-18 |  |
| **[Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers and Large Language Models](http://arxiv.org/abs/2502.09782v3)** | 2025-02-18 | <details><summary>We wo...</summary><p>We would like to withdraw our paper due to a significant error in the experimental methodology, which impacts the validity of our results. The error specifically affects the analysis presented in Section 4, where an incorrect dataset preprocessing step led to misleading conclusions</p></details> |
| **[UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models](http://arxiv.org/abs/2502.13141v1)** | 2025-02-18 | <details><summary>18 Pa...</summary><p>18 Pages, 8 Figures, 5 Tables, Keywords: Attack Defending, Security, Prompt Injection, Backdoor Attacks, Adversarial Attacks, Prompt Trigger Attacks</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region](http://arxiv.org/abs/2502.13946v1)** | 2025-02-19 |  |
| **[FedCC: Robust Federated Learning against Model Poisoning Attacks](http://arxiv.org/abs/2212.01976v3)** | 2025-02-19 |  |
| **[Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models](http://arxiv.org/abs/2502.11054v3)** | 2025-02-19 |  |
| **[Safety Layers in Aligned Large Language Models: The Key to LLM Security](http://arxiv.org/abs/2408.17003v4)** | 2025-02-19 |  |
| **[Na'vi or Knave: Jailbreaking Language Models via Metaphorical Avatars](http://arxiv.org/abs/2412.12145v3)** | 2025-02-19 | <details><summary>We st...</summary><p>We still need to polish our paper</p></details> |
| **[Poisoned Source Code Detection in Code Models](http://arxiv.org/abs/2502.13459v1)** | 2025-02-19 | <details><summary>Accep...</summary><p>Accepted for Publication in the Journal of Systems and Software (JSS)</p></details> |
| **[Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](http://arxiv.org/abs/2406.14023v2)** | 2025-02-19 | <details><summary>Code,...</summary><p>Code, data and benchmarks are available at https://github.com/yuchenwen1/ImplicitBiasPsychometricEvaluation and https://github.com/yuchenwen1/BUMBLE</p></details> |
| **[Secure and Efficient Watermarking for Latent Diffusion Models in Model Distribution Scenarios](http://arxiv.org/abs/2502.13345v1)** | 2025-02-18 |  |
| **[Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers and Large Language Models](http://arxiv.org/abs/2502.09782v3)** | 2025-02-18 | <details><summary>We wo...</summary><p>We would like to withdraw our paper due to a significant error in the experimental methodology, which impacts the validity of our results. The error specifically affects the analysis presented in Section 4, where an incorrect dataset preprocessing step led to misleading conclusions</p></details> |
| **[UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models](http://arxiv.org/abs/2502.13141v1)** | 2025-02-18 | <details><summary>18 Pa...</summary><p>18 Pages, 8 Figures, 5 Tables, Keywords: Attack Defending, Security, Prompt Injection, Backdoor Attacks, Adversarial Attacks, Prompt Trigger Attacks</p></details> |

