---
title: Latest 15 Papers - February 20, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection](http://arxiv.org/abs/2411.01077v2)** | 2025-02-18 |  |
| **[LAMD: Context-driven Android Malware Detection and Classification with LLMs](http://arxiv.org/abs/2502.13055v1)** | 2025-02-18 |  |
| **[LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks](http://arxiv.org/abs/2310.11409v5)** | 2025-02-18 |  |
| **[R.R.: Unveiling LLM Training Privacy through Recollection and Ranking](http://arxiv.org/abs/2502.12658v1)** | 2025-02-18 | 13 pages, 9 figures |
| **[DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent](http://arxiv.org/abs/2502.12575v1)** | 2025-02-18 |  |
| **[Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings](http://arxiv.org/abs/2412.13879v3)** | 2025-02-18 | <details><summary>22 pa...</summary><p>22 pages, 8 figures, 11 tables</p></details> |
| **[Data to Defense: The Role of Curation in Customizing LLMs Against Jailbreaking Attacks](http://arxiv.org/abs/2410.02220v4)** | 2025-02-18 |  |
| **[FedEAT: A Robustness Optimization Framework for Federated LLMs](http://arxiv.org/abs/2502.11863v1)** | 2025-02-17 |  |
| **[BaxBench: Can LLMs Generate Correct and Secure Backends?](http://arxiv.org/abs/2502.11844v1)** | 2025-02-17 |  |
| **[Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?](http://arxiv.org/abs/2502.11598v1)** | 2025-02-17 | <details><summary>22 pa...</summary><p>22 pages, 12 figures, 13 tables</p></details> |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection](http://arxiv.org/abs/2411.01077v2)** | 2025-02-18 |  |
| **[LAMD: Context-driven Android Malware Detection and Classification with LLMs](http://arxiv.org/abs/2502.13055v1)** | 2025-02-18 |  |
| **[LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks](http://arxiv.org/abs/2310.11409v5)** | 2025-02-18 |  |
| **[R.R.: Unveiling LLM Training Privacy through Recollection and Ranking](http://arxiv.org/abs/2502.12658v1)** | 2025-02-18 | 13 pages, 9 figures |
| **[DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent](http://arxiv.org/abs/2502.12575v1)** | 2025-02-18 |  |
| **[Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings](http://arxiv.org/abs/2412.13879v3)** | 2025-02-18 | <details><summary>22 pa...</summary><p>22 pages, 8 figures, 11 tables</p></details> |
| **[Data to Defense: The Role of Curation in Customizing LLMs Against Jailbreaking Attacks](http://arxiv.org/abs/2410.02220v4)** | 2025-02-18 |  |
| **[FedEAT: A Robustness Optimization Framework for Federated LLMs](http://arxiv.org/abs/2502.11863v1)** | 2025-02-17 |  |
| **[BaxBench: Can LLMs Generate Correct and Secure Backends?](http://arxiv.org/abs/2502.11844v1)** | 2025-02-17 |  |
| **[Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?](http://arxiv.org/abs/2502.11598v1)** | 2025-02-17 | <details><summary>22 pa...</summary><p>22 pages, 12 figures, 13 tables</p></details> |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models](http://arxiv.org/abs/2502.13141v1)** | 2025-02-18 | <details><summary>18 Pa...</summary><p>18 Pages, 8 Figures, 5 Tables, Keywords: Attack Defending, Security, Prompt Injection, Backdoor Attacks, Adversarial Attacks, Prompt Trigger Attacks</p></details> |
| **[Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking](http://arxiv.org/abs/2502.12970v1)** | 2025-02-18 | 18 pages |
| **[H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking](http://arxiv.org/abs/2502.12893v1)** | 2025-02-18 |  |
| **[The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1](http://arxiv.org/abs/2502.12659v1)** | 2025-02-18 |  |
| **[Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach](http://arxiv.org/abs/2502.12630v1)** | 2025-02-18 |  |
| **[Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement](http://arxiv.org/abs/2407.01461v2)** | 2025-02-18 |  |
| **[SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings](http://arxiv.org/abs/2502.12562v1)** | 2025-02-18 |  |
| **[Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models](http://arxiv.org/abs/2502.11054v2)** | 2025-02-18 |  |
| **[SoK: Understanding Vulnerabilities in the Large Language Model Supply Chain](http://arxiv.org/abs/2502.12497v1)** | 2025-02-18 |  |
| **[SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks](http://arxiv.org/abs/2502.11090v2)** | 2025-02-18 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models](http://arxiv.org/abs/2502.13141v1)** | 2025-02-18 | <details><summary>18 Pa...</summary><p>18 Pages, 8 Figures, 5 Tables, Keywords: Attack Defending, Security, Prompt Injection, Backdoor Attacks, Adversarial Attacks, Prompt Trigger Attacks</p></details> |
| **[Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking](http://arxiv.org/abs/2502.12970v1)** | 2025-02-18 | 18 pages |
| **[H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking](http://arxiv.org/abs/2502.12893v1)** | 2025-02-18 |  |
| **[Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality](http://arxiv.org/abs/2410.04780v2)** | 2025-02-18 | <details><summary>Accep...</summary><p>Accepted by The Thirteenth International Conference on Learning Representations (ICLR 2025)</p></details> |
| **[The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1](http://arxiv.org/abs/2502.12659v1)** | 2025-02-18 |  |
| **[Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach](http://arxiv.org/abs/2502.12630v1)** | 2025-02-18 |  |
| **[Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement](http://arxiv.org/abs/2407.01461v2)** | 2025-02-18 |  |
| **[SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings](http://arxiv.org/abs/2502.12562v1)** | 2025-02-18 |  |
| **[Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models](http://arxiv.org/abs/2502.11054v2)** | 2025-02-18 |  |
| **[SoK: Understanding Vulnerabilities in the Large Language Model Supply Chain](http://arxiv.org/abs/2502.12497v1)** | 2025-02-18 |  |

