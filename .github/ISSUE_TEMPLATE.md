---
title: Latest 15 Papers - February 24, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Red-Teaming LLM Multi-Agent Systems via Communication Attacks](http://arxiv.org/abs/2502.14847v1)** | 2025-02-20 |  |
| **[Fundamental Limitations in Defending LLM Finetuning APIs](http://arxiv.org/abs/2502.14828v1)** | 2025-02-20 |  |
| **[PEARL: Towards Permutation-Resilient LLMs](http://arxiv.org/abs/2502.14628v1)** | 2025-02-20 | ICLR 2025 |
| **[BaxBench: Can LLMs Generate Correct and Secure Backends?](http://arxiv.org/abs/2502.11844v2)** | 2025-02-20 |  |
| **[Towards Secure Program Partitioning for Smart Contracts with LLM's In-Context Learning](http://arxiv.org/abs/2502.14215v1)** | 2025-02-20 |  |
| **[Multi-Faceted Studies on Data Poisoning can Advance LLM Development](http://arxiv.org/abs/2502.14182v1)** | 2025-02-20 |  |
| **[Theoretically Grounded Framework for LLM Watermarking: A Distribution-Adaptive Approach](http://arxiv.org/abs/2410.02890v4)** | 2025-02-19 |  |
| **[Safety Layers in Aligned Large Language Models: The Key to LLM Security](http://arxiv.org/abs/2408.17003v4)** | 2025-02-19 |  |
| **[Efficient Safety Retrofitting Against Jailbreaking for LLMs](http://arxiv.org/abs/2502.13603v1)** | 2025-02-19 |  |
| **[RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation through LLM Activation Analysis](http://arxiv.org/abs/2411.18948v2)** | 2025-02-19 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Red-Teaming LLM Multi-Agent Systems via Communication Attacks](http://arxiv.org/abs/2502.14847v1)** | 2025-02-20 |  |
| **[Fundamental Limitations in Defending LLM Finetuning APIs](http://arxiv.org/abs/2502.14828v1)** | 2025-02-20 |  |
| **[PEARL: Towards Permutation-Resilient LLMs](http://arxiv.org/abs/2502.14628v1)** | 2025-02-20 | ICLR 2025 |
| **[BaxBench: Can LLMs Generate Correct and Secure Backends?](http://arxiv.org/abs/2502.11844v2)** | 2025-02-20 |  |
| **[Towards Secure Program Partitioning for Smart Contracts with LLM's In-Context Learning](http://arxiv.org/abs/2502.14215v1)** | 2025-02-20 |  |
| **[Multi-Faceted Studies on Data Poisoning can Advance LLM Development](http://arxiv.org/abs/2502.14182v1)** | 2025-02-20 |  |
| **[Theoretically Grounded Framework for LLM Watermarking: A Distribution-Adaptive Approach](http://arxiv.org/abs/2410.02890v4)** | 2025-02-19 |  |
| **[Safety Layers in Aligned Large Language Models: The Key to LLM Security](http://arxiv.org/abs/2408.17003v4)** | 2025-02-19 |  |
| **[Efficient Safety Retrofitting Against Jailbreaking for LLMs](http://arxiv.org/abs/2502.13603v1)** | 2025-02-19 |  |
| **[RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation through LLM Activation Analysis](http://arxiv.org/abs/2411.18948v2)** | 2025-02-19 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Safety Evaluation of DeepSeek Models in Chinese Contexts](http://arxiv.org/abs/2502.11137v2)** | 2025-02-20 | <details><summary>12 pa...</summary><p>12 pages, 2 tables, 7 figures</p></details> |
| **[HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](http://arxiv.org/abs/2502.14744v1)** | 2025-02-20 |  |
| **[Moshi Moshi? A Model Selection Hijacking Adversarial Attack](http://arxiv.org/abs/2502.14586v1)** | 2025-02-20 |  |
| **[FUIA: Model Inversion Attack against Federated Unlearning](http://arxiv.org/abs/2502.14558v1)** | 2025-02-20 | Initial manuscript |
| **[CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models](http://arxiv.org/abs/2502.14529v1)** | 2025-02-20 |  |
| **[PPO-MI: Efficient Black-Box Model Inversion via Proximal Policy Optimization](http://arxiv.org/abs/2502.14370v1)** | 2025-02-20 | <details><summary>6 pag...</summary><p>6 pages, submitting to ICML 2025</p></details> |
| **[Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models](http://arxiv.org/abs/2502.09723v2)** | 2025-02-20 | 15 pages, 11 figures |
| **[Eliminating Backdoors in Neural Code Models for Secure Code Understanding](http://arxiv.org/abs/2408.04683v2)** | 2025-02-20 | <details><summary>Accep...</summary><p>Accepted to the 33rd ACM International Conference on the Foundations of Software Engineering (FSE 2025)</p></details> |
| **[Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach](http://arxiv.org/abs/2502.14285v1)** | 2025-02-20 | <details><summary>14 pa...</summary><p>14 pages,8 figures,4 tables</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Safety Evaluation of DeepSeek Models in Chinese Contexts](http://arxiv.org/abs/2502.11137v2)** | 2025-02-20 | <details><summary>12 pa...</summary><p>12 pages, 2 tables, 7 figures</p></details> |
| **[HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](http://arxiv.org/abs/2502.14744v1)** | 2025-02-20 |  |
| **[Moshi Moshi? A Model Selection Hijacking Adversarial Attack](http://arxiv.org/abs/2502.14586v1)** | 2025-02-20 |  |
| **[FUIA: Model Inversion Attack against Federated Unlearning](http://arxiv.org/abs/2502.14558v1)** | 2025-02-20 | Initial manuscript |
| **[CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models](http://arxiv.org/abs/2502.14529v1)** | 2025-02-20 |  |
| **[PPO-MI: Efficient Black-Box Model Inversion via Proximal Policy Optimization](http://arxiv.org/abs/2502.14370v1)** | 2025-02-20 | <details><summary>6 pag...</summary><p>6 pages, submitting to ICML 2025</p></details> |
| **[Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models](http://arxiv.org/abs/2502.09723v2)** | 2025-02-20 | 15 pages, 11 figures |
| **[Eliminating Backdoors in Neural Code Models for Secure Code Understanding](http://arxiv.org/abs/2408.04683v2)** | 2025-02-20 | <details><summary>Accep...</summary><p>Accepted to the 33rd ACM International Conference on the Foundations of Software Engineering (FSE 2025)</p></details> |
| **[Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach](http://arxiv.org/abs/2502.14285v1)** | 2025-02-20 | <details><summary>14 pa...</summary><p>14 pages,8 figures,4 tables</p></details> |

