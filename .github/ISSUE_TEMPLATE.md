---
title: Latest 15 Papers - October 08, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Proactive defense against LLM Jailbreak](http://arxiv.org/abs/2510.05052v1)** | 2025-10-06 |  |
| **[SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests](http://arxiv.org/abs/2510.04891v1)** | 2025-10-06 |  |
| **[RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection](http://arxiv.org/abs/2510.04885v1)** | 2025-10-06 |  |
| **[Can We Infer Confidential Properties of Training Data from LLMs?](http://arxiv.org/abs/2506.10364v3)** | 2025-10-06 |  |
| **[P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs](http://arxiv.org/abs/2510.04503v1)** | 2025-10-06 |  |
| **[Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents in LLM-Based Multi-Agent Systems](http://arxiv.org/abs/2507.04724v2)** | 2025-10-06 |  |
| **[SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](http://arxiv.org/abs/2510.04398v1)** | 2025-10-05 | <details><summary>Accep...</summary><p>Accepted at NeurIPS 2025. Code is available at https://github.com/Buyun-Liang/SECA</p></details> |
| **[VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy](http://arxiv.org/abs/2510.04261v1)** | 2025-10-05 |  |
| **[When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers](http://arxiv.org/abs/2402.10601v4)** | 2025-10-04 | <details><summary>Publi...</summary><p>Published in Reliable ML from Unreliable Data workshop @ NeurIPS 2025</p></details> |
| **[Revisiting Backdoor Attacks on LLMs: A Stealthy and Practical Poisoning Framework via Harmless Inputs](http://arxiv.org/abs/2505.17601v5)** | 2025-10-04 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Proactive defense against LLM Jailbreak](http://arxiv.org/abs/2510.05052v1)** | 2025-10-06 |  |
| **[SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests](http://arxiv.org/abs/2510.04891v1)** | 2025-10-06 |  |
| **[RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection](http://arxiv.org/abs/2510.04885v1)** | 2025-10-06 |  |
| **[Can We Infer Confidential Properties of Training Data from LLMs?](http://arxiv.org/abs/2506.10364v3)** | 2025-10-06 |  |
| **[P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs](http://arxiv.org/abs/2510.04503v1)** | 2025-10-06 |  |
| **[Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents in LLM-Based Multi-Agent Systems](http://arxiv.org/abs/2507.04724v2)** | 2025-10-06 |  |
| **[SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](http://arxiv.org/abs/2510.04398v1)** | 2025-10-05 | <details><summary>Accep...</summary><p>Accepted at NeurIPS 2025. Code is available at https://github.com/Buyun-Liang/SECA</p></details> |
| **[VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy](http://arxiv.org/abs/2510.04261v1)** | 2025-10-05 |  |
| **[When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers](http://arxiv.org/abs/2402.10601v4)** | 2025-10-04 | <details><summary>Publi...</summary><p>Published in Reliable ML from Unreliable Data workshop @ NeurIPS 2025</p></details> |
| **[Revisiting Backdoor Attacks on LLMs: A Stealthy and Practical Poisoning Framework via Harmless Inputs](http://arxiv.org/abs/2505.17601v5)** | 2025-10-04 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Rethinking Exact Unlearning under Exposure: Extracting Forgotten Data under Exact Unlearning in Large Language Model](http://arxiv.org/abs/2505.24379v2)** | 2025-10-06 | <details><summary>Accep...</summary><p>Accepted by Neurips 2025</p></details> |
| **[Sampling-aware Adversarial Attacks Against Large Language Models](http://arxiv.org/abs/2507.04446v3)** | 2025-10-06 |  |
| **[Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents in LLM-Based Multi-Agent Systems](http://arxiv.org/abs/2507.04724v2)** | 2025-10-06 |  |
| **[Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](http://arxiv.org/abs/2506.07468v3)** | 2025-10-06 |  |
| **[Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models](http://arxiv.org/abs/2510.04347v1)** | 2025-10-05 | <details><summary>15 pa...</summary><p>15 pages total (9 pages main text + 4 pages appendix + references), 12 figures, preprint version. The final version may differ</p></details> |
| **[FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields](http://arxiv.org/abs/2510.04325v1)** | 2025-10-05 |  |
| **[AttackSeqBench: Benchmarking Large Language Models in Analyzing Attack Sequences within Cyber Threat Intelligence](http://arxiv.org/abs/2503.03170v2)** | 2025-10-05 | 36 pages, 9 figures |
| **[Cascading Adversarial Bias from Injection to Distillation in Language Models](http://arxiv.org/abs/2505.24842v2)** | 2025-10-05 |  |
| **[Quantifying Risks in Multi-turn Conversation with Large Language Models](http://arxiv.org/abs/2510.03969v1)** | 2025-10-04 |  |
| **[Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal](http://arxiv.org/abs/2507.21750v2)** | 2025-10-04 | <details><summary>This ...</summary><p>This paper was accepted with an A-decision to Transactions of the Association for Computational Linguistics. This version is the pre-publication version prior to MIT Press production</p></details> |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Tokens, the oft-overlooked appetizer: Large language models, the distributional hypothesis, and meaning](http://arxiv.org/abs/2412.10924v6)** | 2025-10-06 |  |
| **[Rethinking Exact Unlearning under Exposure: Extracting Forgotten Data under Exact Unlearning in Large Language Model](http://arxiv.org/abs/2505.24379v2)** | 2025-10-06 | <details><summary>Accep...</summary><p>Accepted by Neurips 2025</p></details> |
| **[Sampling-aware Adversarial Attacks Against Large Language Models](http://arxiv.org/abs/2507.04446v3)** | 2025-10-06 |  |
| **[Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents in LLM-Based Multi-Agent Systems](http://arxiv.org/abs/2507.04724v2)** | 2025-10-06 |  |
| **[Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](http://arxiv.org/abs/2506.07468v3)** | 2025-10-06 |  |
| **[Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models](http://arxiv.org/abs/2510.04347v1)** | 2025-10-05 | <details><summary>15 pa...</summary><p>15 pages total (9 pages main text + 4 pages appendix + references), 12 figures, preprint version. The final version may differ</p></details> |
| **[FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields](http://arxiv.org/abs/2510.04325v1)** | 2025-10-05 |  |
| **[AttackSeqBench: Benchmarking Large Language Models in Analyzing Attack Sequences within Cyber Threat Intelligence](http://arxiv.org/abs/2503.03170v2)** | 2025-10-05 | 36 pages, 9 figures |
| **[Cascading Adversarial Bias from Injection to Distillation in Language Models](http://arxiv.org/abs/2505.24842v2)** | 2025-10-05 |  |
| **[Quantifying Risks in Multi-turn Conversation with Large Language Models](http://arxiv.org/abs/2510.03969v1)** | 2025-10-04 |  |

