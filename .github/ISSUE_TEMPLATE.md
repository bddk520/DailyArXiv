---
title: Latest 15 Papers - April 11, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## LLM AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[LLM Safeguard is a Double-Edged Sword: Exploiting False Positives for Denial-of-Service Attacks](http://arxiv.org/abs/2410.02916v3)** | 2025-04-09 |  |
| **[Defending LLM Watermarking Against Spoofing Attacks with Contrastive Representation Learning](http://arxiv.org/abs/2504.06575v1)** | 2025-04-09 |  |
| **[Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups](http://arxiv.org/abs/2504.06160v2)** | 2025-04-09 |  |
| **[StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization](http://arxiv.org/abs/2504.05804v1)** | 2025-04-08 |  |
| **[Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking](http://arxiv.org/abs/2504.05652v1)** | 2025-04-08 |  |
| **[ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs](http://arxiv.org/abs/2504.05605v1)** | 2025-04-08 | <details><summary>Zhao ...</summary><p>Zhao et al., 16 pages, 2025, uploaded by Hanzhou Wu, Shanghai University</p></details> |
| **[How to evaluate control measures for LLM agents? A trajectory from today to superintelligence](http://arxiv.org/abs/2504.05259v1)** | 2025-04-07 |  |
| **[Safety Layers in Aligned Large Language Models: The Key to LLM Security](http://arxiv.org/abs/2408.17003v5)** | 2025-04-07 | <details><summary>Accep...</summary><p>Accepted by ICLR 2025. The code is available at https://github.com/listen0425/Safety-Layers</p></details> |
| **[Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs](http://arxiv.org/abs/2504.04715v1)** | 2025-04-07 |  |
| **[CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data](http://arxiv.org/abs/2503.09334v2)** | 2025-04-05 |  |

## LLM AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor Generator Against LLM Alignment](http://arxiv.org/abs/2410.11283v2)** | 2025-04-09 | <details><summary>Publi...</summary><p>Published at the Neurips Safe Generative AI Workshop 2024</p></details> |
| **[LLM Safeguard is a Double-Edged Sword: Exploiting False Positives for Denial-of-Service Attacks](http://arxiv.org/abs/2410.02916v3)** | 2025-04-09 |  |
| **[Defending LLM Watermarking Against Spoofing Attacks with Contrastive Representation Learning](http://arxiv.org/abs/2504.06575v1)** | 2025-04-09 |  |
| **[Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups](http://arxiv.org/abs/2504.06160v2)** | 2025-04-09 |  |
| **[StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization](http://arxiv.org/abs/2504.05804v1)** | 2025-04-08 |  |
| **[Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking](http://arxiv.org/abs/2504.05652v1)** | 2025-04-08 |  |
| **[ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs](http://arxiv.org/abs/2504.05605v1)** | 2025-04-08 | <details><summary>Zhao ...</summary><p>Zhao et al., 16 pages, 2025, uploaded by Hanzhou Wu, Shanghai University</p></details> |
| **[How to evaluate control measures for LLM agents? A trajectory from today to superintelligence](http://arxiv.org/abs/2504.05259v1)** | 2025-04-07 |  |
| **[Safety Layers in Aligned Large Language Models: The Key to LLM Security](http://arxiv.org/abs/2408.17003v5)** | 2025-04-07 | <details><summary>Accep...</summary><p>Accepted by ICLR 2025. The code is available at https://github.com/listen0425/Safety-Layers</p></details> |
| **[Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs](http://arxiv.org/abs/2504.04715v1)** | 2025-04-07 |  |

## large language model AND attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Privacy Attacks on Image AutoRegressive Models](http://arxiv.org/abs/2502.02514v3)** | 2025-04-09 | <details><summary>Code:...</summary><p>Code: https://github.com/sprintml/privacy_attacks_against_iars</p></details> |
| **[JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model](http://arxiv.org/abs/2504.03770v2)** | 2025-04-08 |  |
| **[Model Inversion Attack against Federated Unlearning](http://arxiv.org/abs/2502.14558v3)** | 2025-04-08 |  |
| **[Parasite: A Steganography-based Backdoor Attack Framework for Diffusion Models](http://arxiv.org/abs/2504.05815v1)** | 2025-04-08 |  |
| **[Separator Injection Attack: Uncovering Dialogue Biases in Large Language Models Caused by Role Separators](http://arxiv.org/abs/2504.05689v1)** | 2025-04-08 |  |
| **[Nes2Net: A Lightweight Nested Architecture for Foundation Model Driven Speech Anti-spoofing](http://arxiv.org/abs/2504.05657v1)** | 2025-04-08 | <details><summary>This ...</summary><p>This manuscript has been submitted for peer review</p></details> |
| **[SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments](http://arxiv.org/abs/2412.00114v2)** | 2025-04-08 |  |
| **[Proactive Adversarial Defense: Harnessing Prompt Tuning in Vision-Language Models to Detect Unseen Backdoored Images](http://arxiv.org/abs/2412.08755v4)** | 2025-04-07 |  |
| **[DiffPatch: Generating Customizable Adversarial Patches using Diffusion Models](http://arxiv.org/abs/2412.01440v3)** | 2025-04-07 |  |
| **[Adversarial Robustness for Deep Learning-based Wildfire Prediction Models](http://arxiv.org/abs/2412.20006v3)** | 2025-04-07 |  |

## large language model AND Backdoor Attack
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Privacy Attacks on Image AutoRegressive Models](http://arxiv.org/abs/2502.02514v3)** | 2025-04-09 | <details><summary>Code:...</summary><p>Code: https://github.com/sprintml/privacy_attacks_against_iars</p></details> |
| **[JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model](http://arxiv.org/abs/2504.03770v2)** | 2025-04-08 |  |
| **[Model Inversion Attack against Federated Unlearning](http://arxiv.org/abs/2502.14558v3)** | 2025-04-08 |  |
| **[Parasite: A Steganography-based Backdoor Attack Framework for Diffusion Models](http://arxiv.org/abs/2504.05815v1)** | 2025-04-08 |  |
| **[Separator Injection Attack: Uncovering Dialogue Biases in Large Language Models Caused by Role Separators](http://arxiv.org/abs/2504.05689v1)** | 2025-04-08 |  |
| **[Nes2Net: A Lightweight Nested Architecture for Foundation Model Driven Speech Anti-spoofing](http://arxiv.org/abs/2504.05657v1)** | 2025-04-08 | <details><summary>This ...</summary><p>This manuscript has been submitted for peer review</p></details> |
| **[SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments](http://arxiv.org/abs/2412.00114v2)** | 2025-04-08 |  |
| **[Proactive Adversarial Defense: Harnessing Prompt Tuning in Vision-Language Models to Detect Unseen Backdoored Images](http://arxiv.org/abs/2412.08755v4)** | 2025-04-07 |  |
| **[DiffPatch: Generating Customizable Adversarial Patches using Diffusion Models](http://arxiv.org/abs/2412.01440v3)** | 2025-04-07 |  |
| **[Adversarial Robustness for Deep Learning-based Wildfire Prediction Models](http://arxiv.org/abs/2412.20006v3)** | 2025-04-07 |  |

